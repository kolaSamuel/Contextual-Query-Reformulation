{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open Dialogue Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "A-mSYXip6Lwi",
        "LhhEqjeJ5-Sm"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolaSamuel/Contextual-Query-Reformulation/blob/master/Open_Dialogue_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjlQ_wlDF7PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertForQuestionAnswering\n",
        "# !git clone https://github.com/OpenNMT/OpenNMT-py\n",
        "# !cd OpenNMT-py\n",
        "# !pip install -r OpenNMT-py/requirements.txt\n",
        "# !python OpenNMT-py/preprocess.py -train_src OpenNMT-py/data/src-train.txt -train_tgt OpenNMT-py/data/tgt-train.txt -valid_src OpenNMT-py/data/src-val.txt -valid_tgt OpenNMT-py/data/tgt-val.txt -save_data OpenNMT-py/data/demo\n",
        "!pip install OpenNMT-tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9sPBD5KrSkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # # !onmt-main -h\n",
        "# # from opennmt import models\n",
        "# # help(models.sequence_to_sequence)\n",
        "# from pytorch_pretrained_bert import \n",
        "# import goo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-mSYXip6Lwi",
        "colab_type": "text"
      },
      "source": [
        "## Corefrence Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7a8bqY8kHvq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Version fighting and pip installs\n",
        "!pip install --force-reinstall spacy==2.1.0\n",
        "!pip install neuralcoref\n",
        "# Load your usual SpaCy model (one of SpaCy English models)\n",
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy link en_core_web_md en --force\n",
        "!pip install allennlp\n",
        "# !git clone https://github.com/HLTCHKUST/Mem2Seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vymy0WgbGOfu",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "a219dfef-4257-4e7f-f022-34b5a8a38018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Imports\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)\n",
        "import spacy\n",
        "import neuralcoref\n",
        "from google.colab import files\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as stop\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from allennlp import pretrained\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9_C35zz5PsX",
        "colab_type": "code",
        "outputId": "080ec4a8-8055-40cd-b860-99fdba931375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Testing the neural coref model\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# load NeuralCoref and add it to the pipe of SpaCy's model\n",
        "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
        "nlp.add_pipe(coref, name='neuralcoref')\n",
        "\n",
        "# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations.\n",
        "doc = nlp(u'My sister has a dog. She loves him.')\n",
        "\n",
        "print(doc._.has_coref)\n",
        "doc._.coref_clusters\n",
        "doc._.coref_resolved"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My sister has a dog. My sister loves a dog.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxNOSP5-0GQ4",
        "colab_type": "code",
        "outputId": "e4e59cd5-ea3c-43e0-dc6a-48ec82f23c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from allennlp import pretrained\n",
        "model = pretrained.neural_coreference_resolution_lee_2017()\n",
        "\n",
        "results = model.predict(\"My sister has a dog. She loves him.\")\n",
        "print(model.coref_resolved(\"My sister has a dog. She loves him.\"), type(model.coref_resolved(\"My sister has a dog. She loves him.\")))\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 59248617/59248617 [00:04<00:00, 13135101.27B/s]\n",
            "WARNING:allennlp.models.model:Encountered the antecedent_indices key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "My sister has a dog. My sister loves My sister. <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clusters': [[[0, 1], [6, 6], [8, 8]]],\n",
              " 'document': ['My',\n",
              "  'sister',\n",
              "  'has',\n",
              "  'a',\n",
              "  'dog',\n",
              "  '.',\n",
              "  'She',\n",
              "  'loves',\n",
              "  'him',\n",
              "  '.'],\n",
              " 'predicted_antecedents': [-1, -1, 0, 1],\n",
              " 'top_spans': [[0, 0], [0, 1], [6, 6], [8, 8]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1tfq5hsL22",
        "colab_type": "code",
        "outputId": "fe58029c-ca48-4bc0-e7be-f75565321408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc.similarity(nlp(doc._.coref_resolved))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9740934608298534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60A9NkXJpYS2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Reading the Dialoogues\n",
        "with open('TREC++_Entity_index_TEST_input.txt') as file:\n",
        "  inputData = file.readlines()\n",
        "\n",
        "# with open('/content/gdrive/My Drive/Project/train-output.json') as file:\n",
        "#   goldData = json.load(file)\n",
        "\n",
        "# len(goldData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "usoVwgQEqoQB",
        "colab": {}
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRkOVxa848je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = stop.words('english')\n",
        "\n",
        "def spacy_tokenize(string, remove_stopwords=True):\n",
        "  tokens = list()\n",
        "  stopword_data = stopwords if remove_stopwords else set()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    if token.lemma_.strip() not in stopword_data:\n",
        "      tokens.append(token)\n",
        "  return \" \".join(token.text for token in tokens)\n",
        "\n",
        "def resolvedAbbs(doc):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24yoFPPvNAqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def coref_scorer(doc1, doc2, doc3):\n",
        "  results  = 0\n",
        "  count = 0\n",
        "  for x, y, z in zip(nlp(doc1).sents, nlp(doc2).sents, nlp(doc3).sents):\n",
        "    sent1 = nlp(spacy_tokenize(x.text))\n",
        "    sent2, sent3 = nlp(spacy_tokenize(y.text)), nlp(spacy_tokenize(z.text))\n",
        "    if sent2.text != sent3.text:\n",
        "#       print(sent1, sent2, sep='\\n')\n",
        "      results += sent1.similarity(sent2)\n",
        "      count += 1\n",
        "  \n",
        "  return results/count if count else np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwUOmEHdzxg5",
        "colab_type": "code",
        "outputId": "5cb53659-fcaa-4e1e-e2d9-a184cb25ce56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# A way we can control a parameter\n",
        "nlp.remove_pipe(\"neuralcoref\")  # This remove the current neuralcoref instance from SpaCy pipe\n",
        "coref = neuralcoref.NeuralCoref(nlp.vocab, greedyness=0.55)\n",
        "nlp.add_pipe(coref, name='neuralcoref')\n",
        "\n",
        "#allennlp pretrained end to end corefence resolution\n",
        "model = pretrained.neural_coreference_resolution_lee_2017()\n",
        "\n",
        "neural_resolvedData = []\n",
        "# neural_accuracy = []\n",
        "allen_resolvedData = []\n",
        "# allen_accuracy = []\n",
        "skips = 0\n",
        "\n",
        "for dialogue  in tqdm(inputData):\n",
        "  dialogue =dialogue.replace('<user>', '1@$%').replace('<system>', '1@$%')\n",
        "  neural_resolved = nlp(dialogue)._.coref_resolved\n",
        "  try:\n",
        "    allen_resolved = model.coref_resolved(dialogue)\n",
        "  except:\n",
        "    allen_resolved = dialogue\n",
        "    skips+=1\n",
        "  \n",
        "  \n",
        "  neural_resolvedData.append(neural_resolved)\n",
        "#   neural_accuracy.append(coref_scorer(neural_resolved, gold, dialogue))\n",
        "  allen_resolvedData.append(allen_resolved)\n",
        "#   allen_accuracy.append(coref_scorer(allen_resolved, gold, dialogue))\n",
        "\n",
        "results = pd.DataFrame.from_dict({'Dialogue':inputData,\n",
        "                                  'Neural Resolved Data': neural_resolvedData, \n",
        "                                  'Allen Resolved Data': allen_resolvedData,\n",
        "                                 })\n",
        "display(skips, results)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 66/66 [00:05<00:00, 12.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Neural Resolved Data</th>\n",
              "      <th>Allen Resolved Data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;user&gt; tell me about the types of irregular he...</td>\n",
              "      <td>1@$% tell me about the types of irregular hear...</td>\n",
              "      <td>1@$% tell me about the types of irregular hear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;user&gt; what are entity_1? &lt;system&gt; what are s...</td>\n",
              "      <td>1@$% what are entity_1? 1@$% what are some ex...</td>\n",
              "      <td>1@$% what are entity_1? 1@$% what are some ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;user&gt; what defines a entity_1? &lt;system&gt; how ...</td>\n",
              "      <td>1@$% what defines a entity_1? 1@$% how are a ...</td>\n",
              "      <td>1@$% what defines a entity_1? 1@$% how are a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;user&gt; what are the best ways to cook a entit...</td>\n",
              "      <td>1@$% what are the best ways to cook a entity_...</td>\n",
              "      <td>1@$% what are the best ways to cook a entity_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;user&gt; what is entity_1 exactly? &lt;system&gt; how...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>&lt;user&gt; tell me about entity_1.\\n</td>\n",
              "      <td>1@$% tell me about entity_1.\\n</td>\n",
              "      <td>1@$% tell me about entity_1.\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>&lt;user&gt; what is a normal entity_1? &lt;system&gt; wh...</td>\n",
              "      <td>1@$% what is a normal entity_1? 1@$% what doe...</td>\n",
              "      <td>1@$% what is a normal entity_1? 1@$% what doe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>&lt;user&gt; tell me about the international entity...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>&lt;user&gt; what is chemical energy? &lt;system&gt; desc...</td>\n",
              "      <td>1@$% what is chemical energy? 1@$% describe a...</td>\n",
              "      <td>1@$% what is chemical energy? 1@$% describe a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>&lt;user&gt; why is entity_2 important? &lt;system&gt; wh...</td>\n",
              "      <td>1@$% why is entity_2 important? 1@$% why shou...</td>\n",
              "      <td>1@$% why is entity_2 important? 1@$% why shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>&lt;user&gt; what comprises a entity_1? &lt;system&gt; is...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>&lt;user&gt; what is the entity_1? &lt;system&gt; what er...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>&lt;user&gt; what are the most important us supreme...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>&lt;user&gt; what are the different types of entity...</td>\n",
              "      <td>1@$% what are the different types of entity_1?\\n</td>\n",
              "      <td>1@$% what are the different types of entity_1?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>&lt;user&gt; tell me about entity_1. &lt;system&gt; are t...</td>\n",
              "      <td>1@$% tell me about entity_1. 1@$% are they re...</td>\n",
              "      <td>1@$% tell me about entity_1. 1@$% are they re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>&lt;user&gt; what are the main breeds of entity_1? ...</td>\n",
              "      <td>1@$% what are the main breeds of entity_1? 1@...</td>\n",
              "      <td>1@$% what are the main breeds of entity_1? 1@...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>&lt;user&gt; how does entity_3 entity_1? &lt;system&gt; w...</td>\n",
              "      <td>1@$% how does entity_3 entity_1? 1@$% what ha...</td>\n",
              "      <td>1@$% how does entity_3 entity_1? 1@$% what ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>&lt;user&gt; what are the different types of entity...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>&lt;user&gt; what comprises a entity_1? &lt;system&gt; is...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>&lt;user&gt; what is a good age to get entity_1? &lt;s...</td>\n",
              "      <td>1@$% what is a good age to get entity_1? 1@$%...</td>\n",
              "      <td>1@$% what is a good age to get entity_1? 1@$%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>&lt;user&gt; tell me about the international entity...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>&lt;user&gt; what are some advantages of using enti...</td>\n",
              "      <td>1@$% what are some advantages of using entity...</td>\n",
              "      <td>1@$% what are some advantages of using entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>&lt;user&gt; what are entity_1? &lt;system&gt; what are s...</td>\n",
              "      <td>1@$% what are entity_1? 1@$% what are some ex...</td>\n",
              "      <td>1@$% what are entity_1? 1@$% what are some ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>&lt;user&gt; what are the different forms of entity...</td>\n",
              "      <td>1@$% what are the different forms of entity_2?\\n</td>\n",
              "      <td>1@$% what are the different forms of entity_2?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>&lt;user&gt; what is a normal entity_1?\\n</td>\n",
              "      <td>1@$% what is a normal entity_1?\\n</td>\n",
              "      <td>1@$% what is a normal entity_1?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>&lt;user&gt; what is a entity_5? &lt;system&gt; what are ...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>&lt;user&gt; what is a entity_5? &lt;system&gt; what are ...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>&lt;user&gt; what was the entity_1 of 1933? &lt;system...</td>\n",
              "      <td>1@$% what was the entity_1 of 1933? 1@$% what...</td>\n",
              "      <td>1@$% what was the entity_1 of 1933? 1@$% what...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>&lt;user&gt; what are   entity_2? &lt;system&gt; how did ...</td>\n",
              "      <td>1@$% what are   entity_2? 1@$% how did this b...</td>\n",
              "      <td>1@$% what are   entity_2? 1@$% how did this b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>&lt;user&gt; what is entity_1 exactly?\\n</td>\n",
              "      <td>1@$% what is entity_1 exactly?\\n</td>\n",
              "      <td>1@$% what is entity_1 exactly?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>&lt;user&gt; what are the main types of entity_1 fa...</td>\n",
              "      <td>1@$% what are the main types of entity_1 farm...</td>\n",
              "      <td>1@$% what are the main types of entity_1 farm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>&lt;user&gt; tell me about the types of irregular h...</td>\n",
              "      <td>1@$% tell me about the types of irregular hea...</td>\n",
              "      <td>1@$% tell me about the types of irregular hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>&lt;user&gt; describe entity_1. &lt;system&gt; what are i...</td>\n",
              "      <td>1@$% describe entity_1. 1@$% what are its imp...</td>\n",
              "      <td>1@$% describe entity_1. 1@$% what are its imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>&lt;user&gt; what is a entity_5? &lt;system&gt; what are ...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>&lt;user&gt; what is a entity_5? &lt;system&gt; what are ...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "      <td>1@$% what is a entity_5? 1@$% what are entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>&lt;user&gt; what are the most important us supreme...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>&lt;user&gt; what is chemical energy?\\n</td>\n",
              "      <td>1@$% what is chemical energy?\\n</td>\n",
              "      <td>1@$% what is chemical energy?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>&lt;user&gt; describe entity_2. &lt;system&gt; what makes...</td>\n",
              "      <td>1@$% describe entity_2. 1@$% what makes it so...</td>\n",
              "      <td>1@$% describe entity_2. 1@$% what makes it so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>&lt;user&gt; what are the best ways to cook a entit...</td>\n",
              "      <td>1@$% what are the best ways to cook a entity_...</td>\n",
              "      <td>1@$% what are the best ways to cook a entity_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>&lt;user&gt; what is the entity_1? &lt;system&gt; what er...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>&lt;user&gt; what are the different types of entity...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>&lt;user&gt; what was the entity_1 of 1933? &lt;system...</td>\n",
              "      <td>1@$% what was the entity_1 of 1933? 1@$% what...</td>\n",
              "      <td>1@$% what was the entity_1 of 1933? 1@$% what...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>&lt;user&gt; what is a good age to get entity_1?\\n</td>\n",
              "      <td>1@$% what is a good age to get entity_1?\\n</td>\n",
              "      <td>1@$% what is a good age to get entity_1?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>&lt;user&gt; what are entity_1?\\n</td>\n",
              "      <td>1@$% what are entity_1?\\n</td>\n",
              "      <td>1@$% what are entity_1?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>&lt;user&gt; what are some advantages of using enti...</td>\n",
              "      <td>1@$% what are some advantages of using entity...</td>\n",
              "      <td>1@$% what are some advantages of using entity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>&lt;user&gt; tell me about the types of irregular h...</td>\n",
              "      <td>1@$% tell me about the types of irregular hea...</td>\n",
              "      <td>1@$% tell me about the types of irregular hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>&lt;user&gt; what is entity_1 exactly? &lt;system&gt; how...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>&lt;user&gt; what are the different types of entity...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>&lt;user&gt; what are the most important us supreme...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>&lt;user&gt; tell me about the international entity...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "      <td>1@$% tell me about the international entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>&lt;user&gt; what is entity_3?\\n</td>\n",
              "      <td>1@$% what is entity_3?\\n</td>\n",
              "      <td>1@$% what is entity_3?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>&lt;user&gt; what is the entity_1? &lt;system&gt; what er...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "      <td>1@$% what is the entity_1? 1@$% what era did ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>&lt;user&gt; what are the different types of entity...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "      <td>1@$% what are the different types of entity_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>&lt;user&gt; describe entity_2. &lt;system&gt; what are i...</td>\n",
              "      <td>1@$% describe entity_2. 1@$% what are entity_...</td>\n",
              "      <td>1@$% describe entity_2. 1@$% what are entity_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>&lt;user&gt; how can i help my friend stop entity_2?\\n</td>\n",
              "      <td>1@$% how can i help my friend stop entity_2?\\n</td>\n",
              "      <td>1@$% how can i help i's friend stop entity_2?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>&lt;user&gt; what is entity_1 exactly? &lt;system&gt; how...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "      <td>1@$% what is entity_1 exactly? 1@$% how is it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>&lt;user&gt; what are the most important us supreme...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "      <td>1@$% what are the most important us supreme c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>&lt;user&gt; what was the entity_1 revolution?\\n</td>\n",
              "      <td>1@$% what was the entity_1 revolution?\\n</td>\n",
              "      <td>1@$% what was the entity_1 revolution?\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>&lt;user&gt; what comprises a entity_1? &lt;system&gt; is...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "      <td>1@$% what comprises a entity_1? 1@$% is it th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>\\n</td>\n",
              "      <td>\\n</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Dialogue  ...                                Allen Resolved Data\n",
              "0   <user> tell me about the types of irregular he...  ...  1@$% tell me about the types of irregular hear...\n",
              "1    <user> what are entity_1? <system> what are s...  ...   1@$% what are entity_1? 1@$% what are some ex...\n",
              "2    <user> what defines a entity_1? <system> how ...  ...   1@$% what defines a entity_1? 1@$% how are a ...\n",
              "3    <user> what are the best ways to cook a entit...  ...   1@$% what are the best ways to cook a entity_...\n",
              "4    <user> what is entity_1 exactly? <system> how...  ...   1@$% what is entity_1 exactly? 1@$% how is it...\n",
              "5                    <user> tell me about entity_1.\\n  ...                     1@$% tell me about entity_1.\\n\n",
              "6    <user> what is a normal entity_1? <system> wh...  ...   1@$% what is a normal entity_1? 1@$% what doe...\n",
              "7    <user> tell me about the international entity...  ...   1@$% tell me about the international entity_1...\n",
              "8    <user> what is chemical energy? <system> desc...  ...   1@$% what is chemical energy? 1@$% describe a...\n",
              "9    <user> why is entity_2 important? <system> wh...  ...   1@$% why is entity_2 important? 1@$% why shou...\n",
              "10   <user> what comprises a entity_1? <system> is...  ...   1@$% what comprises a entity_1? 1@$% is it th...\n",
              "11   <user> what is the entity_1? <system> what er...  ...   1@$% what is the entity_1? 1@$% what era did ...\n",
              "12   <user> what are the most important us supreme...  ...   1@$% what are the most important us supreme c...\n",
              "13   <user> what are the different types of entity...  ...   1@$% what are the different types of entity_1?\\n\n",
              "14   <user> tell me about entity_1. <system> are t...  ...   1@$% tell me about entity_1. 1@$% are they re...\n",
              "15   <user> what are the main breeds of entity_1? ...  ...   1@$% what are the main breeds of entity_1? 1@...\n",
              "16   <user> how does entity_3 entity_1? <system> w...  ...   1@$% how does entity_3 entity_1? 1@$% what ha...\n",
              "17   <user> what are the different types of entity...  ...   1@$% what are the different types of entity_1...\n",
              "18   <user> what comprises a entity_1? <system> is...  ...   1@$% what comprises a entity_1? 1@$% is it th...\n",
              "19   <user> what is a good age to get entity_1? <s...  ...   1@$% what is a good age to get entity_1? 1@$%...\n",
              "20   <user> tell me about the international entity...  ...   1@$% tell me about the international entity_1...\n",
              "21   <user> what are some advantages of using enti...  ...   1@$% what are some advantages of using entity...\n",
              "22   <user> what are entity_1? <system> what are s...  ...   1@$% what are entity_1? 1@$% what are some ex...\n",
              "23   <user> what are the different forms of entity...  ...   1@$% what are the different forms of entity_2?\\n\n",
              "24                <user> what is a normal entity_1?\\n  ...                  1@$% what is a normal entity_1?\\n\n",
              "25   <user> what is a entity_5? <system> what are ...  ...   1@$% what is a entity_5? 1@$% what are entity...\n",
              "26   <user> what is a entity_5? <system> what are ...  ...   1@$% what is a entity_5? 1@$% what are entity...\n",
              "27   <user> what was the entity_1 of 1933? <system...  ...   1@$% what was the entity_1 of 1933? 1@$% what...\n",
              "28   <user> what are   entity_2? <system> how did ...  ...   1@$% what are   entity_2? 1@$% how did this b...\n",
              "29                 <user> what is entity_1 exactly?\\n  ...                   1@$% what is entity_1 exactly?\\n\n",
              "..                                                ...  ...                                                ...\n",
              "36   <user> what are the main types of entity_1 fa...  ...   1@$% what are the main types of entity_1 farm...\n",
              "37   <user> tell me about the types of irregular h...  ...   1@$% tell me about the types of irregular hea...\n",
              "38   <user> describe entity_1. <system> what are i...  ...   1@$% describe entity_1. 1@$% what are its imp...\n",
              "39   <user> what is a entity_5? <system> what are ...  ...   1@$% what is a entity_5? 1@$% what are entity...\n",
              "40   <user> what is a entity_5? <system> what are ...  ...   1@$% what is a entity_5? 1@$% what are entity...\n",
              "41   <user> what are the most important us supreme...  ...   1@$% what are the most important us supreme c...\n",
              "42                  <user> what is chemical energy?\\n  ...                    1@$% what is chemical energy?\\n\n",
              "43   <user> describe entity_2. <system> what makes...  ...   1@$% describe entity_2. 1@$% what makes it so...\n",
              "44   <user> what are the best ways to cook a entit...  ...   1@$% what are the best ways to cook a entity_...\n",
              "45   <user> what is the entity_1? <system> what er...  ...   1@$% what is the entity_1? 1@$% what era did ...\n",
              "46   <user> what are the different types of entity...  ...   1@$% what are the different types of entity_1...\n",
              "47   <user> what was the entity_1 of 1933? <system...  ...   1@$% what was the entity_1 of 1933? 1@$% what...\n",
              "48       <user> what is a good age to get entity_1?\\n  ...         1@$% what is a good age to get entity_1?\\n\n",
              "49                        <user> what are entity_1?\\n  ...                          1@$% what are entity_1?\\n\n",
              "50   <user> what are some advantages of using enti...  ...   1@$% what are some advantages of using entity...\n",
              "51   <user> tell me about the types of irregular h...  ...   1@$% tell me about the types of irregular hea...\n",
              "52   <user> what is entity_1 exactly? <system> how...  ...   1@$% what is entity_1 exactly? 1@$% how is it...\n",
              "53   <user> what are the different types of entity...  ...   1@$% what are the different types of entity_1...\n",
              "54   <user> what are the most important us supreme...  ...   1@$% what are the most important us supreme c...\n",
              "55   <user> tell me about the international entity...  ...   1@$% tell me about the international entity_1...\n",
              "56                         <user> what is entity_3?\\n  ...                           1@$% what is entity_3?\\n\n",
              "57   <user> what is the entity_1? <system> what er...  ...   1@$% what is the entity_1? 1@$% what era did ...\n",
              "58   <user> what are the different types of entity...  ...   1@$% what are the different types of entity_1...\n",
              "59   <user> describe entity_2. <system> what are i...  ...   1@$% describe entity_2. 1@$% what are entity_...\n",
              "60   <user> how can i help my friend stop entity_2?\\n  ...    1@$% how can i help i's friend stop entity_2?\\n\n",
              "61   <user> what is entity_1 exactly? <system> how...  ...   1@$% what is entity_1 exactly? 1@$% how is it...\n",
              "62   <user> what are the most important us supreme...  ...   1@$% what are the most important us supreme c...\n",
              "63         <user> what was the entity_1 revolution?\\n  ...           1@$% what was the entity_1 revolution?\\n\n",
              "64   <user> what comprises a entity_1? <system> is...  ...   1@$% what comprises a entity_1? 1@$% is it th...\n",
              "65                                                 \\n  ...                                                 \\n\n",
              "\n",
              "[66 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMqFfs8vfAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Pred-Neural.txt', 'w')as f:\n",
        "  print(*[x.split('1@$%')[-1].strip() for x in neural_resolvedData],\n",
        "        sep='\\n', file=f)\n",
        "with open('Pred-Allen.txt', 'w')as f:\n",
        "  print(*[x.split('1@$%')[-1].strip() for x in allen_resolvedData],\n",
        "        sep='\\n', file=f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaTljvbSwqLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(*nlp(d2).sents, sep='\\n')\n",
        "# print()\n",
        "# print(*nlp(d4).sents, sep='\\n')\n",
        "results[results['Allen Accuracy'] == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmqUjMZSF-HF",
        "colab_type": "code",
        "outputId": "d6c7cd6a-751d-4faa-f3c5-e04aa3315c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "print(results['Allen Accuracy'].describe())\n",
        "results['Neural Accuracy'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    1315.000000\n",
            "mean        0.804020\n",
            "std         0.113872\n",
            "min         0.000000\n",
            "25%         0.730045\n",
            "50%         0.814538\n",
            "75%         0.888122\n",
            "max         0.991586\n",
            "Name: Allen Accuracy, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1315.000000\n",
              "mean        0.812702\n",
              "std         0.106028\n",
              "min         0.451302\n",
              "25%         0.738119\n",
              "50%         0.825427\n",
              "75%         0.897927\n",
              "max         1.000000\n",
              "Name: Neural Accuracy, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM6_DZ5TFVPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.DataFrame.from_dict({'A':[1, 2, 3, 4, 5, 6] , 'B':[ 'I am a boy', 'is this hot', 'typing a word', 'fifty is a number', 'boy do you think', 'last sentence']})\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYRnwY3TF6zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2  = test[(test['A']%2 == 0) | test['B'].str.contains(r'boy', na=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhhEqjeJ5-Sm",
        "colab_type": "text"
      },
      "source": [
        "## Mem to Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9qg-ZjT5QHA",
        "colab_type": "code",
        "outputId": "7c365431-9787-4250-ce8d-7e2d9c56b725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "#Changing directory to Mem2Seq\n",
        "import os\n",
        "os.chdir('Mem2Seq')\n",
        "\n",
        "!git clone https://github.com/HLTCHKUST/Mem2Seq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mem2Seq'...\n",
            "remote: Enumerating objects: 266, done.\u001b[K\n",
            "remote: Total 266 (delta 0), reused 0 (delta 0), pack-reused 266\n",
            "Receiving objects: 100% (266/266), 8.04 MiB | 8.77 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZSRusKM6Up-",
        "colab_type": "code",
        "outputId": "293fccde-3d2f-45c3-fca3-575944aa89d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 main_train.py -lr=0.001 -layer=1 -hdd=128 -dr=0.2 -dec=Mem2Seq -bsz=8 -ds=kvr -t="
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': 'kvr', 'task': '', 'decoder': 'Mem2Seq', 'hidden': '128', 'batch': '8', 'learn': '0.001', 'drop': '0.2', 'unk_mask': 1, 'layer': '1', 'limit': -10000, 'path': None, 'test': None, 'sample': None, 'useKB': 1, 'entPtr': 0, 'evalp': 2, 'addName': ''}\n",
            "07-08 16:19 Reading lines from data/KVR/train.txt\n",
            "07-08 16:19 Pointer percentace= 0.4208753595747005 \n",
            "07-08 16:19 Max responce Len: 80\n",
            "07-08 16:19 Max Input Len: 249\n",
            "07-08 16:19 Avg. User Utterances: 2.593814432989691\n",
            "07-08 16:19 Avg. Bot Utterances: 2.593814432989691\n",
            "07-08 16:19 Avg. KB results: 64.69896907216494\n",
            "07-08 16:19 Avg. responce Len: 8.732273449920509\n",
            "Sample:  [['dish_parking', 'poi', 'parking_garage', 'road_block_nearby', '2_miles'], ['2_miles', 'distance', 'dish_parking', 'PAD', 'PAD'], ['road_block_nearby', 'traffic_info', 'dish_parking', 'PAD', 'PAD'], ['parking_garage', 'poi_type', 'dish_parking', 'PAD', 'PAD'], ['550_alester_ave', 'address', 'dish_parking', 'PAD', 'PAD'], ['stanford_oval_parking', 'poi', 'parking_garage', 'no_traffic', '6_miles'], ['6_miles', 'distance', 'stanford_oval_parking', 'PAD', 'PAD'], ['no_traffic', 'traffic_info', 'stanford_oval_parking', 'PAD', 'PAD'], ['parking_garage', 'poi_type', 'stanford_oval_parking', 'PAD', 'PAD'], ['610_amarillo_ave', 'address', 'stanford_oval_parking', 'PAD', 'PAD'], ['willows_market', 'poi', 'grocery_store', 'car_collision_nearby', '4_miles'], ['4_miles', 'distance', 'willows_market', 'PAD', 'PAD'], ['car_collision_nearby', 'traffic_info', 'willows_market', 'PAD', 'PAD'], ['grocery_store', 'poi_type', 'willows_market', 'PAD', 'PAD'], ['409_bollard_st', 'address', 'willows_market', 'PAD', 'PAD'], ['the_westin', 'poi', 'rest_stop', 'moderate_traffic', '2_miles'], ['2_miles', 'distance', 'the_westin', 'PAD', 'PAD'], ['moderate_traffic', 'traffic_info', 'the_westin', 'PAD', 'PAD'], ['rest_stop', 'poi_type', 'the_westin', 'PAD', 'PAD'], ['329_el_camino_real', 'address', 'the_westin', 'PAD', 'PAD'], ['toms_house', 'poi', 'friends_house', 'heavy_traffic', '1_miles'], ['1_miles', 'distance', 'toms_house', 'PAD', 'PAD'], ['heavy_traffic', 'traffic_info', 'toms_house', 'PAD', 'PAD'], ['friends_house', 'poi_type', 'toms_house', 'PAD', 'PAD'], ['580_van_ness_ave', 'address', 'toms_house', 'PAD', 'PAD'], ['pizza_chicago', 'poi', 'pizza_restaurant', 'heavy_traffic', '4_miles'], ['4_miles', 'distance', 'pizza_chicago', 'PAD', 'PAD'], ['heavy_traffic', 'traffic_info', 'pizza_chicago', 'PAD', 'PAD'], ['pizza_restaurant', 'poi_type', 'pizza_chicago', 'PAD', 'PAD'], ['915_arbol_dr', 'address', 'pizza_chicago', 'PAD', 'PAD'], ['valero', 'poi', 'gas_station', 'car_collision_nearby', '6_miles'], ['6_miles', 'distance', 'valero', 'PAD', 'PAD'], ['car_collision_nearby', 'traffic_info', 'valero', 'PAD', 'PAD'], ['gas_station', 'poi_type', 'valero', 'PAD', 'PAD'], ['200_alester_ave', 'address', 'valero', 'PAD', 'PAD'], ['mandarin_roots', 'poi', 'chinese_restaurant', 'no_traffic', '2_miles'], ['2_miles', 'distance', 'mandarin_roots', 'PAD', 'PAD'], ['no_traffic', 'traffic_info', 'mandarin_roots', 'PAD', 'PAD'], ['chinese_restaurant', 'poi_type', 'mandarin_roots', 'PAD', 'PAD'], ['271_springer_street', 'address', 'mandarin_roots', 'PAD', 'PAD'], ['where', '$u', 't1', 'PAD', 'PAD'], ['s', '$u', 't1', 'PAD', 'PAD'], ['the', '$u', 't1', 'PAD', 'PAD'], ['nearest', '$u', 't1', 'PAD', 'PAD'], ['parking_garage', '$u', 't1', 'PAD', 'PAD'], ['the', '$s', 't1', 'PAD', 'PAD'], ['nearest', '$s', 't1', 'PAD', 'PAD'], ['parking_garage', '$s', 't1', 'PAD', 'PAD'], ['is', '$s', 't1', 'PAD', 'PAD'], ['dish_parking', '$s', 't1', 'PAD', 'PAD'], ['at', '$s', 't1', 'PAD', 'PAD'], ['550_alester_ave', '$s', 't1', 'PAD', 'PAD'], ['would', '$s', 't1', 'PAD', 'PAD'], ['you', '$s', 't1', 'PAD', 'PAD'], ['like', '$s', 't1', 'PAD', 'PAD'], ['directions', '$s', 't1', 'PAD', 'PAD'], ['there', '$s', 't1', 'PAD', 'PAD'], ['yes', '$u', 't2', 'PAD', 'PAD'], ['please', '$u', 't2', 'PAD', 'PAD'], ['set', '$u', 't2', 'PAD', 'PAD'], ['directions', '$u', 't2', 'PAD', 'PAD'], ['via', '$u', 't2', 'PAD', 'PAD'], ['a', '$u', 't2', 'PAD', 'PAD'], ['route', '$u', 't2', 'PAD', 'PAD'], ['that', '$u', 't2', 'PAD', 'PAD'], ['avoids', '$u', 't2', 'PAD', 'PAD'], ['all', '$u', 't2', 'PAD', 'PAD'], ['heavy_traffic', '$u', 't2', 'PAD', 'PAD'], ['if', '$u', 't2', 'PAD', 'PAD'], ['possible', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] it looks like there is a road block being reported on the route but i will still find the quickest route to 550_alester_ave [70, 70, 54, 56, 48, 62, 70, 70, 70, 70, 70, 45, 63, 70, 70, 70, 70, 70, 45, 70, 63, 70, 51] [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1] ['550_alester_ave']\n",
            "07-08 16:19 Reading lines from data/KVR/dev.txt\n",
            "07-08 16:19 Pointer percentace= 0.4167286798630749 \n",
            "07-08 16:19 Max responce Len: 87\n",
            "07-08 16:19 Max Input Len: 264\n",
            "07-08 16:19 Avg. User Utterances: 2.5728476821192054\n",
            "07-08 16:19 Avg. Bot Utterances: 2.5728476821192054\n",
            "07-08 16:19 Avg. KB results: 63.847682119205295\n",
            "07-08 16:19 Avg. responce Len: 8.647361647361647\n",
            "Sample:  [['make', '$u', 't1', 'PAD', 'PAD'], ['an', '$u', 't1', 'PAD', 'PAD'], ['appointment', '$u', 't1', 'PAD', 'PAD'], ['to', '$u', 't1', 'PAD', 'PAD'], ['reserve', '$u', 't1', 'PAD', 'PAD'], ['conference_room_100', '$u', 't1', 'PAD', 'PAD'], ['later', '$u', 't1', 'PAD', 'PAD'], ['this', '$u', 't1', 'PAD', 'PAD'], ['week', '$u', 't1', 'PAD', 'PAD'], ['for', '$u', 't1', 'PAD', 'PAD'], ['a', '$u', 't1', 'PAD', 'PAD'], ['meeting', '$u', 't1', 'PAD', 'PAD'], ['what', '$s', 't1', 'PAD', 'PAD'], ['day', '$s', 't1', 'PAD', 'PAD'], ['and', '$s', 't1', 'PAD', 'PAD'], ['time', '$s', 't1', 'PAD', 'PAD'], ['should', '$s', 't1', 'PAD', 'PAD'], ['i', '$s', 't1', 'PAD', 'PAD'], ['set', '$s', 't1', 'PAD', 'PAD'], ['an', '$s', 't1', 'PAD', 'PAD'], ['appointment', '$s', 't1', 'PAD', 'PAD'], ['to', '$s', 't1', 'PAD', 'PAD'], ['reserve', '$s', 't1', 'PAD', 'PAD'], ['the', '$s', 't1', 'PAD', 'PAD'], ['conference', '$s', 't1', 'PAD', 'PAD'], ['room', '$s', 't1', 'PAD', 'PAD'], ['monday', '$u', 't2', 'PAD', 'PAD'], ['at', '$u', 't2', 'PAD', 'PAD'], ['3pm', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] i have made an appointment for monday at 3pm for the meeting [17, 29, 29, 19, 20, 9, 26, 27, 28, 9, 23, 11] [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1] ['meeting', 'monday', '3pm']\n",
            "07-08 16:19 Reading lines from data/KVR/test.txt\n",
            "07-08 16:19 Pointer percentace= 0.4224432239869378 \n",
            "07-08 16:19 Max responce Len: 36\n",
            "07-08 16:19 Max Input Len: 228\n",
            "07-08 16:19 Avg. User Utterances: 2.6546052631578947\n",
            "07-08 16:19 Avg. Bot Utterances: 2.6546052631578947\n",
            "07-08 16:19 Avg. KB results: 64.84539473684211\n",
            "07-08 16:19 Avg. responce Len: 8.34820322180917\n",
            "Sample:  [['remind', '$u', 't1', 'PAD', 'PAD'], ['me', '$u', 't1', 'PAD', 'PAD'], ['to', '$u', 't1', 'PAD', 'PAD'], ['take', '$u', 't1', 'PAD', 'PAD'], ['my', '$u', 't1', 'PAD', 'PAD'], ['pills', '$u', 't1', 'PAD', 'PAD'], ['what', '$s', 't1', 'PAD', 'PAD'], ['time', '$s', 't1', 'PAD', 'PAD'], ['do', '$s', 't1', 'PAD', 'PAD'], ['you', '$s', 't1', 'PAD', 'PAD'], ['need', '$s', 't1', 'PAD', 'PAD'], ['to', '$s', 't1', 'PAD', 'PAD'], ['take', '$s', 't1', 'PAD', 'PAD'], ['your', '$s', 't1', 'PAD', 'PAD'], ['pills', '$s', 't1', 'PAD', 'PAD'], ['i', '$u', 't2', 'PAD', 'PAD'], ['need', '$u', 't2', 'PAD', 'PAD'], ['to', '$u', 't2', 'PAD', 'PAD'], ['take', '$u', 't2', 'PAD', 'PAD'], ['my', '$u', 't2', 'PAD', 'PAD'], ['pills', '$u', 't2', 'PAD', 'PAD'], ['at', '$u', 't2', 'PAD', 'PAD'], ['7pm', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] ok setting your medicine appointment for 7pm [23, 23, 13, 23, 23, 23, 22] [0, 0, 1, 0, 0, 0, 1] ['7pm']\n",
            "07-08 16:19 Read 6290 sentence pairs train\n",
            "07-08 16:19 Read 777 sentence pairs dev\n",
            "07-08 16:19 Read 807 sentence pairs test\n",
            "07-08 16:19 Max len Input 265 \n",
            "07-08 16:19 Vocab_size 1554 \n",
            "07-08 16:19 USE_CUDA=False\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "07-08 16:19 Epoch:0\n",
            "  0% 0/787 [00:00<?, ?it/s]/content/Mem2Seq/models/Mem2Seq.py:155: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  ec = torch.nn.utils.clip_grad_norm(self.encoder.parameters(), clip)\n",
            "/content/Mem2Seq/models/Mem2Seq.py:156: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  dc = torch.nn.utils.clip_grad_norm(self.decoder.parameters(), clip)\n",
            "L:6.46, VL:4.60, PL:1.86: 100% 787/787 [02:11<00:00,  6.61it/s]\n",
            "07-08 16:21 Epoch:1\n",
            "L:5.44, VL:3.79, PL:1.65: 100% 787/787 [02:12<00:00,  5.93it/s]\n",
            "07-08 16:23 STARTING EVALUATION\n",
            "R:0.0906,W:74.4579: 100% 98/98 [00:22<00:00,  4.30it/s]\n",
            "07-08 16:24 F1 SCORE:\t0.09109028226675284\n",
            "07-08 16:24 \tCAL F1:\t0.1557692307692308\n",
            "07-08 16:24 \tWET F1:\t0.17182103610675037\n",
            "07-08 16:24 \tNAV F1:\t0.01023102310231023\n",
            "07-08 16:24 BLEU SCORE:5.39\n",
            "07-08 16:24 MODEL SAVED\n",
            "07-08 16:24 Epoch:2\n",
            "L:5.07, VL:3.50, PL:1.58: 100% 787/787 [02:14<00:00,  5.83it/s]\n",
            "07-08 16:26 Epoch:3\n",
            "L:4.80, VL:3.28, PL:1.52: 100% 787/787 [02:14<00:00,  5.85it/s]\n",
            "07-08 16:28 STARTING EVALUATION\n",
            "R:0.1008,W:69.1461: 100% 98/98 [00:24<00:00,  4.01it/s]\n",
            "07-08 16:28 F1 SCORE:\t0.18234025374855825\n",
            "07-08 16:28 \tCAL F1:\t0.2953067765567766\n",
            "07-08 16:28 \tWET F1:\t0.2976537673893086\n",
            "07-08 16:28 \tNAV F1:\t0.05624705327675625\n",
            "07-08 16:28 BLEU SCORE:7.5\n",
            "07-08 16:28 MODEL SAVED\n",
            "07-08 16:28 Epoch:4\n",
            "L:4.72, VL:3.22, PL:1.50: 100% 787/787 [02:15<00:00,  5.83it/s]\n",
            "07-08 16:31 Epoch:5\n",
            "L:4.60, VL:3.12, PL:1.47: 100% 787/787 [02:15<00:00,  5.81it/s]\n",
            "07-08 16:33 STARTING EVALUATION\n",
            "R:0.0931,W:67.2661: 100% 98/98 [00:24<00:00,  4.00it/s]\n",
            "07-08 16:33 F1 SCORE:\t0.25314949026367717\n",
            "07-08 16:33 \tCAL F1:\t0.4363324175824174\n",
            "07-08 16:33 \tWET F1:\t0.4059660666679944\n",
            "07-08 16:33 \tNAV F1:\t0.06881188118811882\n",
            "07-08 16:33 BLEU SCORE:8.83\n",
            "07-08 16:33 MODEL SAVED\n",
            "07-08 16:33 Epoch:6\n",
            "L:4.48, VL:3.03, PL:1.45: 100% 787/787 [02:15<00:00,  5.82it/s]\n",
            "07-08 16:36 Epoch:7\n",
            "L:4.47, VL:3.02, PL:1.46: 100% 787/787 [02:15<00:00,  5.80it/s]\n",
            "07-08 16:38 STARTING EVALUATION\n",
            "R:0.1071,W:69.7351: 100% 98/98 [00:26<00:00,  3.72it/s]\n",
            "07-08 16:38 F1 SCORE:\t0.2761470736107418\n",
            "07-08 16:38 \tCAL F1:\t0.46474358974358954\n",
            "07-08 16:38 \tWET F1:\t0.3991447106746164\n",
            "07-08 16:38 \tNAV F1:\t0.1065888731730316\n",
            "07-08 16:38 BLEU SCORE:9.45\n",
            "07-08 16:38 MODEL SAVED\n",
            "07-08 16:38 Epoch:8\n",
            "L:4.27, VL:2.86, PL:1.41: 100% 787/787 [02:15<00:00,  5.81it/s]\n",
            "07-08 16:41 Epoch:9\n",
            "L:4.32, VL:2.90, PL:1.42: 100% 787/787 [02:16<00:00,  5.75it/s]\n",
            "07-08 16:43 STARTING EVALUATION\n",
            "R:0.0982,W:69.2898: 100% 98/98 [00:26<00:00,  3.67it/s]\n",
            "07-08 16:43 F1 SCORE:\t0.3019941304370374\n",
            "07-08 16:43 \tCAL F1:\t0.4943320568320566\n",
            "07-08 16:43 \tWET F1:\t0.452024704026681\n",
            "07-08 16:43 \tNAV F1:\t0.11458431557441458\n",
            "07-08 16:43 BLEU SCORE:9.66\n",
            "07-08 16:43 MODEL SAVED\n",
            "07-08 16:43 Epoch:10\n",
            "L:4.33, VL:2.90, PL:1.43: 100% 787/787 [02:16<00:00,  6.21it/s]\n",
            "07-08 16:46 Epoch:11\n",
            "L:4.19, VL:2.80, PL:1.39: 100% 787/787 [02:18<00:00,  5.68it/s]\n",
            "07-08 16:48 STARTING EVALUATION\n",
            "R:0.0995,W:67.0939: 100% 98/98 [00:25<00:00,  3.79it/s]\n",
            "07-08 16:48 F1 SCORE:\t0.2693337327800998\n",
            "07-08 16:48 \tCAL F1:\t0.346268315018315\n",
            "07-08 16:48 \tWET F1:\t0.3633267567276464\n",
            "07-08 16:48 \tNAV F1:\t0.17435172088637438\n",
            "07-08 16:48 BLEU SCORE:9.41\n",
            "07-08 16:48 Epoch:12\n",
            "L:4.12, VL:2.73, PL:1.38: 100% 787/787 [02:17<00:00,  5.72it/s]\n",
            "07-08 16:51 Epoch:13\n",
            "L:4.06, VL:2.69, PL:1.37: 100% 787/787 [02:17<00:00,  5.73it/s]\n",
            "07-08 16:53 STARTING EVALUATION\n",
            "R:0.1122,W:66.5847: 100% 98/98 [00:25<00:00,  3.90it/s]\n",
            "07-08 16:53 F1 SCORE:\t0.2746224951519072\n",
            "07-08 16:53 \tCAL F1:\t0.3941315628815628\n",
            "07-08 16:53 \tWET F1:\t0.3237784857532756\n",
            "07-08 16:53 \tNAV F1:\t0.18413484205563418\n",
            "07-08 16:53 BLEU SCORE:10.05\n",
            "07-08 16:53 MODEL SAVED\n",
            "07-08 16:53 Epoch:14\n",
            "L:3.99, VL:2.64, PL:1.36: 100% 787/787 [02:18<00:00,  6.42it/s]\n",
            "07-08 16:56 Epoch:15\n",
            "L:3.95, VL:2.61, PL:1.34: 100% 787/787 [02:17<00:00,  5.72it/s]\n",
            "07-08 16:58 STARTING EVALUATION\n",
            "R:0.1033,W:66.9924: 100% 98/98 [00:26<00:00,  3.73it/s]\n",
            "07-08 16:58 F1 SCORE:\t0.36399292526409677\n",
            "07-08 16:58 \tCAL F1:\t0.47381715506715494\n",
            "07-08 16:58 \tWET F1:\t0.49790057008752503\n",
            "07-08 16:58 \tNAV F1:\t0.22856357064277844\n",
            "07-08 16:58 BLEU SCORE:11.5\n",
            "07-08 16:58 MODEL SAVED\n",
            "07-08 16:58 Epoch:16\n",
            "L:3.99, VL:2.63, PL:1.36: 100% 787/787 [02:18<00:00,  5.67it/s]\n",
            "07-08 17:01 Epoch:17\n",
            "L:3.80, VL:2.50, PL:1.30: 100% 787/787 [02:17<00:00,  5.72it/s]\n",
            "07-08 17:03 STARTING EVALUATION\n",
            "R:0.0995,W:69.0839: 100% 98/98 [00:27<00:00,  3.54it/s]\n",
            "07-08 17:03 F1 SCORE:\t0.362431858394343\n",
            "07-08 17:03 \tCAL F1:\t0.45041971916971896\n",
            "07-08 17:03 \tWET F1:\t0.47355782426417464\n",
            "07-08 17:03 \tNAV F1:\t0.25166588087380165\n",
            "07-08 17:03 BLEU SCORE:10.94\n",
            "07-08 17:03 Epoch:18\n",
            "L:3.86, VL:2.54, PL:1.32: 100% 787/787 [02:18<00:00,  5.69it/s]\n",
            "07-08 17:06 Epoch:19\n",
            "L:3.83, VL:2.51, PL:1.32: 100% 787/787 [02:18<00:00,  5.66it/s]\n",
            "07-08 17:08 STARTING EVALUATION\n",
            "R:0.1033,W:67.3939: 100% 98/98 [00:26<00:00,  3.76it/s]\n",
            "07-08 17:08 F1 SCORE:\t0.33341764310276445\n",
            "07-08 17:08 \tCAL F1:\t0.5122992392223159\n",
            "07-08 17:08 \tWET F1:\t0.40565196461143055\n",
            "07-08 17:08 \tNAV F1:\t0.19876630520194874\n",
            "07-08 17:08 BLEU SCORE:9.23\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n",
            "07-08 17:08 Epoch:20\n",
            "L:3.63, VL:2.37, PL:1.26: 100% 787/787 [02:18<00:00,  5.68it/s]\n",
            "07-08 17:11 Epoch:21\n",
            "L:3.69, VL:2.41, PL:1.28: 100% 787/787 [02:18<00:00,  5.67it/s]\n",
            "07-08 17:13 STARTING EVALUATION\n",
            "R:0.1084,W:67.1172: 100% 98/98 [00:26<00:00,  3.65it/s]\n",
            "07-08 17:14 F1 SCORE:\t0.3733980412836727\n",
            "07-08 17:14 \tCAL F1:\t0.43668300653594755\n",
            "07-08 17:14 \tWET F1:\t0.5244878613507794\n",
            "07-08 17:14 \tNAV F1:\t0.2518073235895017\n",
            "07-08 17:14 BLEU SCORE:11.12\n",
            "07-08 17:14 Epoch:22\n",
            "L:3.66, VL:2.38, PL:1.28: 100% 787/787 [02:17<00:00,  5.72it/s]\n",
            "07-08 17:16 Epoch:23\n",
            "L:3.55, VL:2.31, PL:1.24: 100% 787/787 [02:18<00:00,  5.68it/s]\n",
            "07-08 17:18 STARTING EVALUATION\n",
            "R:0.1110,W:69.9719: 100% 98/98 [00:27<00:00,  3.54it/s]\n",
            "07-08 17:19 F1 SCORE:\t0.3690150081004209\n",
            "07-08 17:19 \tCAL F1:\t0.5378052503052501\n",
            "07-08 17:19 \tWET F1:\t0.4602143313176949\n",
            "07-08 17:19 \tNAV F1:\t0.22838676724815332\n",
            "07-08 17:19 BLEU SCORE:10.45\n",
            "Epoch    11: reducing learning rate of group 0 to 2.5000e-04.\n",
            "07-08 17:19 Epoch:24\n",
            "L:3.65, VL:2.38, PL:1.27: 100% 787/787 [02:19<00:00,  5.64it/s]\n",
            "07-08 17:21 Epoch:25\n",
            "L:3.52, VL:2.28, PL:1.24: 100% 787/787 [02:17<00:00,  5.74it/s]\n",
            "07-08 17:23 STARTING EVALUATION\n",
            "R:0.1148,W:67.9605: 100% 98/98 [00:26<00:00,  3.64it/s]\n",
            "07-08 17:24 F1 SCORE:\t0.39429407962279955\n",
            "07-08 17:24 \tCAL F1:\t0.4987232318963085\n",
            "07-08 17:24 \tWET F1:\t0.5316779200367925\n",
            "07-08 17:24 \tNAV F1:\t0.25959453088165946\n",
            "07-08 17:24 BLEU SCORE:12.03\n",
            "07-08 17:24 MODEL SAVED\n",
            "07-08 17:24 Epoch:26\n",
            "L:3.53, VL:2.28, PL:1.24: 100% 787/787 [02:17<00:00,  6.10it/s]\n",
            "07-08 17:26 Epoch:27\n",
            "L:3.49, VL:2.25, PL:1.24: 100% 787/787 [02:16<00:00,  5.75it/s]\n",
            "07-08 17:28 STARTING EVALUATION\n",
            "R:0.1110,W:68.3821: 100% 98/98 [00:27<00:00,  3.60it/s]\n",
            "07-08 17:29 F1 SCORE:\t0.4016907783529531\n",
            "07-08 17:29 \tCAL F1:\t0.5124289706020473\n",
            "07-08 17:29 \tWET F1:\t0.552372412300346\n",
            "07-08 17:29 \tNAV F1:\t0.25590916234480576\n",
            "07-08 17:29 BLEU SCORE:13.03\n",
            "07-08 17:29 MODEL SAVED\n",
            "07-08 17:29 Epoch:28\n",
            "L:3.56, VL:2.30, PL:1.26: 100% 787/787 [02:18<00:00,  5.69it/s]\n",
            "07-08 17:31 Epoch:29\n",
            "L:3.45, VL:2.23, PL:1.22: 100% 787/787 [02:17<00:00,  5.72it/s]\n",
            "07-08 17:33 STARTING EVALUATION\n",
            "R:0.1110,W:68.3301: 100% 98/98 [00:27<00:00,  3.56it/s]\n",
            "07-08 17:34 F1 SCORE:\t0.40831750670261013\n",
            "07-08 17:34 \tCAL F1:\t0.5584821428571425\n",
            "07-08 17:34 \tWET F1:\t0.5037249889238855\n",
            "07-08 17:34 \tNAV F1:\t0.2747996228194247\n",
            "07-08 17:34 BLEU SCORE:11.23\n",
            "07-08 17:34 Epoch:30\n",
            "L:3.60, VL:2.34, PL:1.26: 100% 787/787 [02:18<00:00,  5.66it/s]\n",
            "07-08 17:36 Epoch:31\n",
            "L:3.52, VL:2.29, PL:1.24: 100% 787/787 [02:19<00:00,  5.65it/s]\n",
            "07-08 17:38 STARTING EVALUATION\n",
            "R:0.1097,W:68.4424: 100% 98/98 [00:27<00:00,  3.58it/s]\n",
            "07-08 17:39 F1 SCORE:\t0.3801307167306081\n",
            "07-08 17:39 \tCAL F1:\t0.4974869681600448\n",
            "07-08 17:39 \tWET F1:\t0.4594058056659194\n",
            "07-08 17:39 \tNAV F1:\t0.2730080150872228\n",
            "07-08 17:39 BLEU SCORE:11.14\n",
            "Epoch    15: reducing learning rate of group 0 to 1.2500e-04.\n",
            "07-08 17:39 Epoch:32\n",
            "L:3.39, VL:2.19, PL:1.20: 100% 787/787 [02:17<00:00,  5.71it/s]\n",
            "07-08 17:41 Epoch:33\n",
            "L:3.48, VL:2.25, PL:1.23: 100% 787/787 [02:19<00:00,  5.65it/s]\n",
            "07-08 17:43 STARTING EVALUATION\n",
            "R:0.1097,W:69.6294: 100% 98/98 [00:27<00:00,  3.54it/s]\n",
            "07-08 17:44 F1 SCORE:\t0.3997967475168355\n",
            "07-08 17:44 \tCAL F1:\t0.5329362411093178\n",
            "07-08 17:44 \tWET F1:\t0.4992209767927172\n",
            "07-08 17:44 \tNAV F1:\t0.2726779820839226\n",
            "07-08 17:44 BLEU SCORE:11.19\n",
            "07-08 17:44 Epoch:34\n",
            "L:3.47, VL:2.24, PL:1.23: 100% 787/787 [02:19<00:00,  5.63it/s]\n",
            "07-08 17:46 Epoch:35\n",
            "L:3.47, VL:2.24, PL:1.23: 100% 787/787 [02:20<00:00,  5.62it/s]\n",
            "07-08 17:49 STARTING EVALUATION\n",
            "R:0.1046,W:69.5423: 100% 98/98 [00:27<00:00,  3.57it/s]\n",
            "07-08 17:49 F1 SCORE:\t0.3924677699928113\n",
            "07-08 17:49 \tCAL F1:\t0.5413972245703013\n",
            "07-08 17:49 \tWET F1:\t0.4414018842434168\n",
            "07-08 17:49 \tNAV F1:\t0.2869636963696367\n",
            "07-08 17:49 BLEU SCORE:10.56\n",
            "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
            "07-08 17:49 Epoch:36\n",
            "L:3.42, VL:2.20, PL:1.22: 100% 787/787 [02:18<00:00,  7.12it/s]\n",
            "07-08 17:51 Epoch:37\n",
            "L:3.40, VL:2.19, PL:1.21: 100% 787/787 [02:20<00:00,  7.05it/s]\n",
            "07-08 17:54 STARTING EVALUATION\n",
            "R:0.1122,W:68.8014: 100% 98/98 [00:27<00:00,  3.52it/s]\n",
            "07-08 17:54 F1 SCORE:\t0.40037878776760616\n",
            "07-08 17:54 \tCAL F1:\t0.5555989950220717\n",
            "07-08 17:54 \tWET F1:\t0.47323028239202736\n",
            "07-08 17:54 \tNAV F1:\t0.27754596888260236\n",
            "07-08 17:54 BLEU SCORE:11.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guAz165zYj3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 main_test.py -path=save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq10.05 -dec=Mem2Seq -bsz=8 -ds=kvr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XiKfBQL8lE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path = 'save\\\\mem2seq-KVR'\n",
        "\n",
        "files = []\n",
        "# r=root, d=directories, f = files\n",
        "for r, d, f in os.walk('save/mem2seq-KVR'):\n",
        "    for file in f:\n",
        "        if '.txt' in file:\n",
        "            files.append(os.path.join(r, file))\n",
        "\n",
        "for f in files:\n",
        "    print(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1B5mcsA-M_f",
        "colab_type": "code",
        "outputId": "d789db53-512e-4d5e-f4a7-2bbd27fe7085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = 'save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq10.05'\n",
        "directory = a.split('/')\n",
        "directory[2].split('HDD')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '128BSZ8DR0.2L1lr0.001Mem2Seq10.05']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-90AHECG-sTw",
        "colab_type": "code",
        "outputId": "2f49a909-667a-4806-f5d1-c5f6293ff6c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# git clone https://github.com/kolaSamuel/Mem2Seq/tree/kolaSamuel-patch-1\n",
        "# import os\n",
        "os.chdir('../')\n",
        "!rm -rf Mem2Seq\n",
        "# os.mkdir('Samuel')\n",
        "# os.chdir('Samuel')\n",
        "!git clone --single-branch --branch kolaSamuel-patch-1 https://github.com/kolaSamuel/Mem2Seq\n",
        "os.chdir('Mem2Seq')\n",
        "!python3 main_test.py -path=save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq12.03 -dec=Mem2Seq -bsz=8 -ds=kvr\n",
        "# !rm -rf Mem2Seq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mem2Seq'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 297 (delta 13), reused 0 (delta 0), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (297/297), 21.27 MiB | 17.52 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "{'dataset': 'kvr', 'task': None, 'decoder': 'Mem2Seq', 'hidden': None, 'batch': '8', 'learn': None, 'drop': None, 'unk_mask': 1, 'layer': None, 'limit': -10000, 'path': 'save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq12.03', 'test': None, 'sample': None, 'useKB': 1, 'entPtr': 0, 'evalp': 2, 'addName': ''}\n",
            "07-08 20:46 Reading lines from data/KVR/train.txt\n",
            "07-08 20:46 Pointer percentace= 0.4208753595747005 \n",
            "07-08 20:46 Max responce Len: 80\n",
            "07-08 20:46 Max Input Len: 249\n",
            "07-08 20:46 Avg. User Utterances: 2.593814432989691\n",
            "07-08 20:46 Avg. Bot Utterances: 2.593814432989691\n",
            "07-08 20:46 Avg. KB results: 64.69896907216494\n",
            "07-08 20:46 Avg. responce Len: 8.732273449920509\n",
            "Sample:  [['dish_parking', 'poi', 'parking_garage', 'road_block_nearby', '2_miles'], ['2_miles', 'distance', 'dish_parking', 'PAD', 'PAD'], ['road_block_nearby', 'traffic_info', 'dish_parking', 'PAD', 'PAD'], ['parking_garage', 'poi_type', 'dish_parking', 'PAD', 'PAD'], ['550_alester_ave', 'address', 'dish_parking', 'PAD', 'PAD'], ['stanford_oval_parking', 'poi', 'parking_garage', 'no_traffic', '6_miles'], ['6_miles', 'distance', 'stanford_oval_parking', 'PAD', 'PAD'], ['no_traffic', 'traffic_info', 'stanford_oval_parking', 'PAD', 'PAD'], ['parking_garage', 'poi_type', 'stanford_oval_parking', 'PAD', 'PAD'], ['610_amarillo_ave', 'address', 'stanford_oval_parking', 'PAD', 'PAD'], ['willows_market', 'poi', 'grocery_store', 'car_collision_nearby', '4_miles'], ['4_miles', 'distance', 'willows_market', 'PAD', 'PAD'], ['car_collision_nearby', 'traffic_info', 'willows_market', 'PAD', 'PAD'], ['grocery_store', 'poi_type', 'willows_market', 'PAD', 'PAD'], ['409_bollard_st', 'address', 'willows_market', 'PAD', 'PAD'], ['the_westin', 'poi', 'rest_stop', 'moderate_traffic', '2_miles'], ['2_miles', 'distance', 'the_westin', 'PAD', 'PAD'], ['moderate_traffic', 'traffic_info', 'the_westin', 'PAD', 'PAD'], ['rest_stop', 'poi_type', 'the_westin', 'PAD', 'PAD'], ['329_el_camino_real', 'address', 'the_westin', 'PAD', 'PAD'], ['toms_house', 'poi', 'friends_house', 'heavy_traffic', '1_miles'], ['1_miles', 'distance', 'toms_house', 'PAD', 'PAD'], ['heavy_traffic', 'traffic_info', 'toms_house', 'PAD', 'PAD'], ['friends_house', 'poi_type', 'toms_house', 'PAD', 'PAD'], ['580_van_ness_ave', 'address', 'toms_house', 'PAD', 'PAD'], ['pizza_chicago', 'poi', 'pizza_restaurant', 'heavy_traffic', '4_miles'], ['4_miles', 'distance', 'pizza_chicago', 'PAD', 'PAD'], ['heavy_traffic', 'traffic_info', 'pizza_chicago', 'PAD', 'PAD'], ['pizza_restaurant', 'poi_type', 'pizza_chicago', 'PAD', 'PAD'], ['915_arbol_dr', 'address', 'pizza_chicago', 'PAD', 'PAD'], ['valero', 'poi', 'gas_station', 'car_collision_nearby', '6_miles'], ['6_miles', 'distance', 'valero', 'PAD', 'PAD'], ['car_collision_nearby', 'traffic_info', 'valero', 'PAD', 'PAD'], ['gas_station', 'poi_type', 'valero', 'PAD', 'PAD'], ['200_alester_ave', 'address', 'valero', 'PAD', 'PAD'], ['mandarin_roots', 'poi', 'chinese_restaurant', 'no_traffic', '2_miles'], ['2_miles', 'distance', 'mandarin_roots', 'PAD', 'PAD'], ['no_traffic', 'traffic_info', 'mandarin_roots', 'PAD', 'PAD'], ['chinese_restaurant', 'poi_type', 'mandarin_roots', 'PAD', 'PAD'], ['271_springer_street', 'address', 'mandarin_roots', 'PAD', 'PAD'], ['where', '$u', 't1', 'PAD', 'PAD'], ['s', '$u', 't1', 'PAD', 'PAD'], ['the', '$u', 't1', 'PAD', 'PAD'], ['nearest', '$u', 't1', 'PAD', 'PAD'], ['parking_garage', '$u', 't1', 'PAD', 'PAD'], ['the', '$s', 't1', 'PAD', 'PAD'], ['nearest', '$s', 't1', 'PAD', 'PAD'], ['parking_garage', '$s', 't1', 'PAD', 'PAD'], ['is', '$s', 't1', 'PAD', 'PAD'], ['dish_parking', '$s', 't1', 'PAD', 'PAD'], ['at', '$s', 't1', 'PAD', 'PAD'], ['550_alester_ave', '$s', 't1', 'PAD', 'PAD'], ['would', '$s', 't1', 'PAD', 'PAD'], ['you', '$s', 't1', 'PAD', 'PAD'], ['like', '$s', 't1', 'PAD', 'PAD'], ['directions', '$s', 't1', 'PAD', 'PAD'], ['there', '$s', 't1', 'PAD', 'PAD'], ['yes', '$u', 't2', 'PAD', 'PAD'], ['please', '$u', 't2', 'PAD', 'PAD'], ['set', '$u', 't2', 'PAD', 'PAD'], ['directions', '$u', 't2', 'PAD', 'PAD'], ['via', '$u', 't2', 'PAD', 'PAD'], ['a', '$u', 't2', 'PAD', 'PAD'], ['route', '$u', 't2', 'PAD', 'PAD'], ['that', '$u', 't2', 'PAD', 'PAD'], ['avoids', '$u', 't2', 'PAD', 'PAD'], ['all', '$u', 't2', 'PAD', 'PAD'], ['heavy_traffic', '$u', 't2', 'PAD', 'PAD'], ['if', '$u', 't2', 'PAD', 'PAD'], ['possible', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] it looks like there is a road block being reported on the route but i will still find the quickest route to 550_alester_ave [70, 70, 54, 56, 48, 62, 70, 70, 70, 70, 70, 45, 63, 70, 70, 70, 70, 70, 45, 70, 63, 70, 51] [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1] ['550_alester_ave']\n",
            "07-08 20:46 Reading lines from data/KVR/dev.txt\n",
            "07-08 20:46 Pointer percentace= 0.4167286798630749 \n",
            "07-08 20:46 Max responce Len: 87\n",
            "07-08 20:46 Max Input Len: 264\n",
            "07-08 20:46 Avg. User Utterances: 2.5728476821192054\n",
            "07-08 20:46 Avg. Bot Utterances: 2.5728476821192054\n",
            "07-08 20:46 Avg. KB results: 63.847682119205295\n",
            "07-08 20:46 Avg. responce Len: 8.647361647361647\n",
            "Sample:  [['make', '$u', 't1', 'PAD', 'PAD'], ['an', '$u', 't1', 'PAD', 'PAD'], ['appointment', '$u', 't1', 'PAD', 'PAD'], ['to', '$u', 't1', 'PAD', 'PAD'], ['reserve', '$u', 't1', 'PAD', 'PAD'], ['conference_room_100', '$u', 't1', 'PAD', 'PAD'], ['later', '$u', 't1', 'PAD', 'PAD'], ['this', '$u', 't1', 'PAD', 'PAD'], ['week', '$u', 't1', 'PAD', 'PAD'], ['for', '$u', 't1', 'PAD', 'PAD'], ['a', '$u', 't1', 'PAD', 'PAD'], ['meeting', '$u', 't1', 'PAD', 'PAD'], ['what', '$s', 't1', 'PAD', 'PAD'], ['day', '$s', 't1', 'PAD', 'PAD'], ['and', '$s', 't1', 'PAD', 'PAD'], ['time', '$s', 't1', 'PAD', 'PAD'], ['should', '$s', 't1', 'PAD', 'PAD'], ['i', '$s', 't1', 'PAD', 'PAD'], ['set', '$s', 't1', 'PAD', 'PAD'], ['an', '$s', 't1', 'PAD', 'PAD'], ['appointment', '$s', 't1', 'PAD', 'PAD'], ['to', '$s', 't1', 'PAD', 'PAD'], ['reserve', '$s', 't1', 'PAD', 'PAD'], ['the', '$s', 't1', 'PAD', 'PAD'], ['conference', '$s', 't1', 'PAD', 'PAD'], ['room', '$s', 't1', 'PAD', 'PAD'], ['monday', '$u', 't2', 'PAD', 'PAD'], ['at', '$u', 't2', 'PAD', 'PAD'], ['3pm', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] i have made an appointment for monday at 3pm for the meeting [17, 29, 29, 19, 20, 9, 26, 27, 28, 9, 23, 11] [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1] ['meeting', '3pm', 'monday']\n",
            "07-08 20:46 Reading lines from data/KVR/test.txt\n",
            "07-08 20:46 Pointer percentace= 0.4224432239869378 \n",
            "07-08 20:46 Max responce Len: 36\n",
            "07-08 20:46 Max Input Len: 228\n",
            "07-08 20:46 Avg. User Utterances: 2.6546052631578947\n",
            "07-08 20:46 Avg. Bot Utterances: 2.6546052631578947\n",
            "07-08 20:46 Avg. KB results: 64.84539473684211\n",
            "07-08 20:46 Avg. responce Len: 8.34820322180917\n",
            "Sample:  [['remind', '$u', 't1', 'PAD', 'PAD'], ['me', '$u', 't1', 'PAD', 'PAD'], ['to', '$u', 't1', 'PAD', 'PAD'], ['take', '$u', 't1', 'PAD', 'PAD'], ['my', '$u', 't1', 'PAD', 'PAD'], ['pills', '$u', 't1', 'PAD', 'PAD'], ['what', '$s', 't1', 'PAD', 'PAD'], ['time', '$s', 't1', 'PAD', 'PAD'], ['do', '$s', 't1', 'PAD', 'PAD'], ['you', '$s', 't1', 'PAD', 'PAD'], ['need', '$s', 't1', 'PAD', 'PAD'], ['to', '$s', 't1', 'PAD', 'PAD'], ['take', '$s', 't1', 'PAD', 'PAD'], ['your', '$s', 't1', 'PAD', 'PAD'], ['pills', '$s', 't1', 'PAD', 'PAD'], ['i', '$u', 't2', 'PAD', 'PAD'], ['need', '$u', 't2', 'PAD', 'PAD'], ['to', '$u', 't2', 'PAD', 'PAD'], ['take', '$u', 't2', 'PAD', 'PAD'], ['my', '$u', 't2', 'PAD', 'PAD'], ['pills', '$u', 't2', 'PAD', 'PAD'], ['at', '$u', 't2', 'PAD', 'PAD'], ['7pm', '$u', 't2', 'PAD', 'PAD'], ['$$$$', '$$$$', '$$$$', '$$$$', '$$$$']] ok setting your medicine appointment for 7pm [23, 23, 13, 23, 23, 23, 22] [0, 0, 1, 0, 0, 0, 1] ['7pm']\n",
            "07-08 20:46 Read 6290 sentence pairs train\n",
            "07-08 20:46 Read 777 sentence pairs dev\n",
            "07-08 20:46 Read 807 sentence pairs test\n",
            "07-08 20:46 Max len Input 265 \n",
            "07-08 20:46 Vocab_size 1554 \n",
            "07-08 20:46 USE_CUDA=False\n",
            "07-08 20:46 MODEL save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq12.03 LOADED\n",
            "07-08 20:46 STARTING EVALUATION\n",
            "  0% 0/101 [00:00<?, ?it/s]Correct:what city should i find friday s temperature for\n",
            "\tPredict:what city are you want the temperature for\n",
            "Correct:on friday there will be clear_skies in alhambra\n",
            "\tPredict:in alhambra it temperature is predicted to be low of high of\n",
            "Correct:there is a chevron\n",
            "\tPredict:i have a away\n",
            "Correct:taking you to chevron\n",
            "\tPredict:there is no_traffic on the route to to avoid all the quickest route\n",
            "Correct:783_arcadia_pl is the address for chevron gas_station\n",
            "\tPredict:chevron is located at 783_arcadia_pl\n",
            "Correct:you re welcome happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:what time do you need to take your pills\n",
            "\tPredict:what time and time should i to reminder\n",
            "Correct:ok setting your medicine appointment for 7pm\n",
            "\tPredict:you are welcome\n",
            "R:0.0000,W:0.9088:   1% 1/101 [00:00<00:22,  4.38it/s]Correct:it will be between 20f - 30f in alhambra on friday\n",
            "\tPredict:the temperature in alhambra will be a low of\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the weather in fresno over the next 48 hours will be cloudy with snow\n",
            "\tPredict:it will not be be fresno in in in\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:home is where is your heart is at 56_cadwell_street\n",
            "\tPredict:there is a away\n",
            "Correct:there are whole_foods 2_miles away and sigona_farmers_market 4_miles away where do we go\n",
            "\tPredict:the closest grocery_store is whole_foods at 2_miles away\n",
            "Correct:sigona_farmers_market is located 4_miles away with no_traffic it s located at 638_amherst_st\n",
            "\tPredict:the is is to sigona_farmers_market\n",
            "Correct:great glad i could help have a great day we will be there soon\n",
            "\tPredict:you re welcome\n",
            "R:0.0012,W:1.4854:   2% 2/101 [00:00<00:23,  4.17it/s]Correct:you are 3_miles away from home with heavy_traffic noted\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome driver\n",
            "\tPredict:you re welcome\n",
            "Correct:glad to help\n",
            "\tPredict:no problem\n",
            "Correct:we re 3_miles away from willows_market but there is a car_collision_nearby\n",
            "\tPredict:the closest grocery_store is 4_miles 4_miles away 4_miles away away away there is located at 394_van_ness_ave\n",
            "Correct:i m sorry but there are no other options nearby\n",
            "\tPredict:willows_market is located at 409_bollard_st\n",
            "Correct:we are 4_miles away from whole_foods and from safeway which one do you prefer\n",
            "\tPredict:the closest grocery_store is safeway 4_miles away would you like directions\n",
            "Correct:safeway is located at 452_arcadia_pl\n",
            "\tPredict:safeway is located safeway at 452_arcadia_pl\n",
            "Correct:you re all set drive carefully\n",
            "\tPredict:there is a to on and the on way\n",
            "R:0.0012,W:2.1897:   3% 3/101 [00:00<00:24,  3.99it/s]Correct:it will be 80f on tuesday in brentwood\n",
            "\tPredict:what will two_days the highest_temperature in brentwood brentwood\n",
            "Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:ok driver making a route to willows_market and avoiding the car accident\n",
            "\tPredict:willows_market is located at 409_bollard_st but i sent on your screen the best route to avoid the route to reach the garage fast enough\n",
            "Correct:you are welcome driver\n",
            "\tPredict:you re welcome\n",
            "Correct:jill s house is located 5_miles away at 347_alta_mesa_ave\n",
            "\tPredict:your home is at 56_cadwell_street\n",
            "Correct:the quickest route is 347_alta_mesa_ave at 5_miles we will be there shortly\n",
            "\tPredict:setting navigation to see now there is a car_collision_nearby\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:okay scheduling friday dinner with mom at 11am\n",
            "\tPredict:okay i a reminder for mom mom mom mom mom 11am mom\n",
            "R:0.0025,W:2.8520:   4% 4/101 [00:01<00:26,  3.72it/s]Correct:stanford_express_care is at 214_el_camino_real\n",
            "\tPredict:the hospital hospital is 5_miles away\n",
            "Correct:good day\n",
            "\tPredict:you re welcome\n",
            "Correct:the closest parking_garage is civic_center_garage located 4_miles away at 270_altaire_walk\n",
            "\tPredict:the closest parking_garage is 4_miles away at\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:i ve set the gps to stanford_shopping_center 773_alger_dr it s 2_miles away\n",
            "\tPredict:the nearest shopping_center is stanford_shopping_center stanford_shopping_center 2_miles away at there is\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your dinner is 5pm tonight\n",
            "\tPredict:your dinner is at at 5pm\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0050,W:3.3457:   5% 5/101 [00:01<00:24,  3.89it/s]Correct:where should i check for snow\n",
            "\tPredict:what city are you want to\n",
            "Correct:there will be no snow in corona this week\n",
            "\tPredict:it will not snow in corona corona corona\n",
            "Correct:no problem my driver\n",
            "\tPredict:you re welcome\n",
            "Correct:it will not be warm in compton on wednesday\n",
            "\tPredict:it will not be warm in compton on wednesday\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:what time shall i set a dinner reminder\n",
            "\tPredict:what time and time would you like reminder\n",
            "Correct:i will set a reminder for dinner at 7pm for the_6th of this month with marie\n",
            "\tPredict:okay i have dinner dinner dinner with marie on at with with\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0062,W:3.9589:   6% 6/101 [00:01<00:23,  4.01it/s]Correct:valero is 4_miles away\n",
            "\tPredict:there is a away from\n",
            "Correct:valero is at 200_alester_ave\n",
            "\tPredict:valero is the address\n",
            "Correct:i sent the best possible route on screen but there will be heavy_traffic anyway i m sorry\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:reminder for football on the_11th of this month at 1pm with marie is set\n",
            "\tPredict:okay setting a reminder for football_activity with marie on the_11th the_11th 1pm\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:what time would like me to set the appointment\n",
            "\tPredict:what day and time should i set the appointment\n",
            "Correct:okay i set your appointment for 3pm on saturday have a good time\n",
            "\tPredict:okay setting a reminder for your 3pm on saturday at 3pm\n",
            "R:0.0074,W:4.6044:   7% 7/101 [00:01<00:23,  4.00it/s]Correct:tell me what city you would like to know about snow today\n",
            "\tPredict:what city would you like to know the snow forecast for\n",
            "Correct:it will not snow in cleveland today\n",
            "\tPredict:in cleveland it will snow today and snow\n",
            "Correct:we re 3_miles away from midtown_shopping_center and there is no_traffic on our way\n",
            "\tPredict:i have a shopping gas_station listed at is 2_miles away\n",
            "Correct:you got it gps set\n",
            "\tPredict:amazing re welcome\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:your friends house is on 5677_southwest_4th_street there is moderate_traffic on our way but we re just 1_miles away\n",
            "\tPredict:i have a listing for friend s house is that\n",
            "Correct:it must have been a hard day for you if you don t remember the address of your home 56_cadwell_street i was kidding don t crash me please\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0087,W:5.2987:   8% 8/101 [00:02<00:24,  3.87it/s]Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:we re 5_miles away from starbucks would you like to go there\n",
            "\tPredict:there is cafe away from\n",
            "Correct:navigating you to starbucks\n",
            "\tPredict:there is no_traffic to starbucks\n",
            "Correct:the address is 792_bedoin_street i sent the info on your screen drive carefully\n",
            "\tPredict:you re welcome\n",
            "Correct:are you talking about the doctor_appointment on wednesday or the one on the_5th\n",
            "\tPredict:your doctor is scheduled for at on at\n",
            "Correct:your doctor_appointment on wednesday is at 7pm with your father\n",
            "\tPredict:you have a on on wednesday\n",
            "Correct:okay scheduling a conference with the sales_team at 9am on the_11th of this month\n",
            "\tPredict:okay setting a reminder for conference the_11th the_11th sales_team the_11th sales_team the_11th of sales_team\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0099,W:5.8487:   9% 9/101 [00:02<00:24,  3.79it/s]Correct:in what city are you interested\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:in los_angeles it will rain today and be 40f - 60f tomorrow it will be warm and 60f - 80f\n",
            "\tPredict:what city are you want los_angeles today and tomorrow s forecast for\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:hacienda_market is the closest grocery_store 1_miles away\n",
            "\tPredict:the quickest route is is\n",
            "Correct:there is the midtown_shopping_center 3_miles away and no_traffic on the route set this destination on your gps\n",
            "\tPredict:the nearest mall is midtown_shopping_center\n",
            "Correct:setting gps to midtown_shopping_center anything else\n",
            "\tPredict:you re welcome\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:your doctor_appointment on the_5th is at 6pm with alex\n",
            "\tPredict:you are welcome\n",
            "R:0.0099,W:6.6668:  10% 10/101 [00:02<00:23,  3.80it/s]Correct:for what city\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:in downtown_chicago it will be humid on monday snow on tueday dry on wednesday frost on thursday drizzle on friday clear_skies on saturday and sunday\n",
            "\tPredict:in downtown_chicago it will be overcast on monday and on tuesday and on sunday\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:setting navigation to 1313_chester_ave now there is a road_block_nearby so drive carefully\n",
            "\tPredict:the address is 578_arbol_dr\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:your tennis_activity is on the_4th at 5pm and your sister will be attending\n",
            "\tPredict:your tennis_activity is tennis_activity scheduled on on\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:schedule a meeting for what time and day\n",
            "\tPredict:what time and time should i schedule\n",
            "R:0.0099,W:7.4486:  11% 11/101 [00:02<00:25,  3.55it/s]Correct:what city would you like to know the weather about\n",
            "\tPredict:what city would you like to know the weather for\n",
            "Correct:it is not predicted to drizzle any today or tomorrow in durham\n",
            "\tPredict:it will not drizzle in durham today\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:what city do you want to hear the forecast for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:it will be overcast today and stormy tomorrow in mountain_view\n",
            "\tPredict:it will be raining today in mountain_view\n",
            "Correct:you re welcome\n",
            "\tPredict:anytime\n",
            "Correct:setting meeting for saturday at 1pm with sales_team to go_over_budget\n",
            "\tPredict:i will schedule a meeting with sales_team to go_over_budget at 1pm\n",
            "Correct:my pleasure\n",
            "\tPredict:you are welcome\n",
            "R:0.0099,W:8.1529:  12% 12/101 [00:03<00:24,  3.66it/s]Correct:what location should i look up the weekly forecast for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:it should not be hot in brentwood this week\n",
            "\tPredict:it will not be hot in brentwood brentwood this week\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there will be no drizzle in redwood_city this weekend\n",
            "\tPredict:there will not be drizzle in redwood_city this weekend\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest one is palo_alto_garage_r it s just 1_miles away\n",
            "\tPredict:the is is away away at\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:taking you home\n",
            "\tPredict:the is is away\n",
            "R:0.0124,W:8.5990:  13% 13/101 [00:03<00:22,  3.90it/s]Correct:what city are you interested in\n",
            "\tPredict:what city are you interested in\n",
            "Correct:the temperature in compton will be between 70f and 100f on monday and friday during the other days of the week it will remain between 30f and 70f\n",
            "\tPredict:it will be between a high of in in compton\n",
            "Correct:home is 5671_barringer_street anything else i can help you with\n",
            "\tPredict:the address is 56_cadwell_street\n",
            "Correct:got it will do\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome i am always happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:at what time should i set the appointment\n",
            "\tPredict:what time and time should i\n",
            "Correct:i have made an appointment for the doctor on the_12th at 7pm\n",
            "\tPredict:okay i ve set your for you on\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0149,W:9.1215:  14% 14/101 [00:03<00:21,  3.98it/s]Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:on friday the highest_temperature in mountain_view is predicted to be 50f\n",
            "\tPredict:what will highest_temperature be in mountain_view on on friday\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:stanford_express_care is on the way\n",
            "\tPredict:we re 5_miles away from cafe_venetia and there is no_traffic on our way\n",
            "Correct:your next tennis_activity is on the_4th at 5pm with your mother\n",
            "\tPredict:your next tennis_activity is on the_13th at 7pm with your brother\n",
            "Correct:glad i can help\n",
            "\tPredict:you are welcome\n",
            "Correct:okay i will remind you of your yoga_activity with your mother scheduled on the_15th of this month\n",
            "\tPredict:okay i a for your yoga_activity with at mother on the_15th of this month\n",
            "Correct:setting reminder for yoga_activity with your mother on the_15th at 3pm\n",
            "\tPredict:you are welcome\n",
            "R:0.0149,W:9.8791:  15% 15/101 [00:03<00:23,  3.70it/s]Correct:dew is predicted in danville on thursday\n",
            "\tPredict:there will be clear_skies in danville on thursday\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:durham does not have any snow predicted this week\n",
            "\tPredict:it is not snow in durham in\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:navigating to stanford_express_care at 214_el_camino_real there is heavy_traffic on the way there however\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:i will promptly schedule a swimming_activity on monday the_1st with your father at 4pm\n",
            "\tPredict:okay scheduling a swimming_activity with your father on monday the_1st at 4pm\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0149,W:10.4442:  16% 16/101 [00:04<00:23,  3.69it/s]Correct:what city do you want the weather for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:this week in cleveland the weather will be clear_skies on monday dew on tuesday stormy on wednesday drizzle on thrusday cloudy on friday overcast on saturday and raining on sunday\n",
            "\tPredict:it will be be on on on on\n",
            "Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:it will be dry wednesday in new_york\n",
            "\tPredict:it will not be in new_york on wednesday\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:for what day would you like to know\n",
            "\tPredict:what city would you like seattle weather for seattle\n",
            "Correct:this week in seattle the weather is forecasted to be overcast cloudy windy foggy drzzle warm and dry\n",
            "\tPredict:this week in seattle it will be overcast with a low of 80f and high of 70f\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0161,W:10.9804:  17% 17/101 [00:04<00:23,  3.55it/s]Correct:for what city would you like to know that\n",
            "\tPredict:what city would you like the weekend forecast for\n",
            "Correct:chef_chu_s is the only chinese_restaurant but here is heavy_traffic\n",
            "\tPredict:the nearest chinese_restaurant is away\n",
            "Correct:chef_chu_s is at 593_arrowhead_way\n",
            "\tPredict:chef_chu_s is located at 593_arrowhead_way\n",
            "Correct:there will be heavy_traffic sorry\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:chef_chu_s is 5_miles away\n",
            "\tPredict:the is chinese_restaurant away away away away is away\n",
            "Correct:there is currently no_traffic\n",
            "\tPredict:the is is away\n",
            "Correct:gps set happy to assist you\n",
            "\tPredict:gps set to chef_chu_s now\n",
            "R:0.0173,W:11.5917:  18% 18/101 [00:04<00:21,  3.79it/s]Correct:it will be rainy the first half of the week and then foggy the second and dry over the weekend in compton\n",
            "\tPredict:for what city would you like the weekend forecast for\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:it will not be humid in fresno next_week\n",
            "\tPredict:it will not be humid in fresno next_week\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:you have 4 doctor_appointment scheduled marie on the_15th your brother on wednesday your mother on the_20th and ana on the_12th\n",
            "\tPredict:you have two doctor_appointment scheduled one on the_12th at with and another on the_16th at with\n",
            "Correct:my pleasure goodbye\n",
            "\tPredict:you are welcome\n",
            "Correct:your tennis match is tuesday at 11am\n",
            "\tPredict:your tennis is is on at at\n",
            "Correct:you re very welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0198,W:12.1199:  19% 19/101 [00:05<00:23,  3.56it/s]Correct:what location do you want me to check if it will be rainy\n",
            "\tPredict:what city are you want to know if it will rain\n",
            "Correct:next_week it will rain on saturday in los_angeles\n",
            "\tPredict:it los_angeles not gonna be los_angeles on on\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:it will not be warm in grand_rapids on saturday\n",
            "\tPredict:it will not be warm grand_rapids grand_rapids on saturday\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:for which one i have two one at 7pm the_8th and one at 10am the_1st\n",
            "\tPredict:you have two dentist_appointment scheduled one on the_7th with your brother and one on at with with\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0198,W:12.7603:  20% 20/101 [00:05<00:22,  3.65it/s]Correct:what city do you want the weather for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:it will be dry other than sunday and the temperatures are expected to be between 70f - 100f monday and tuesday 30f - 40f wednesday 40f - 60f thursday and 80f - 100f friday to sunday\n",
            "\tPredict:the exeter is overcast in exeter on monday\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:it appears as if there will be a low of 40f and a high of 50f\n",
            "\tPredict:what city do you want the temperature for\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is a panda_express 3_miles from you as well as a pizza_hut 5_miles from you\n",
            "\tPredict:the nearest fast is restaurant food at away away away\n",
            "Correct:panda_express is at 842_arrowhead_way with no_traffic noted\n",
            "\tPredict:panda_express is whichever the route to avoid the heavy_traffic on our way\n",
            "Correct:i sent the info on your map drive carefully\n",
            "\tPredict:you re welcome\n",
            "R:0.0235,W:13.2348:  21% 21/101 [00:05<00:22,  3.49it/s]Correct:for what city would you like today and tomorrow s weather for\n",
            "\tPredict:what city would you like to know the weather for\n",
            "Correct:it will not be foggy in fresno in the coming days\n",
            "\tPredict:it is not predicted to be today and tomorrow in fresno\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:it will not drizzle in cleveland tomorrow\n",
            "\tPredict:what city are you in cleveland cleveland on\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:what days are you interested in\n",
            "\tPredict:what city are you want the weather report for\n",
            "Correct:today you have a 7pm meeting with hr to discuss_the_merger\n",
            "\tPredict:your meeting is scheduled for at\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0260,W:13.7473:  22% 22/101 [00:05<00:21,  3.69it/s]Correct:it will be stormy on monday the lowest_temperature on tuesday hot on wednesday the hottest temperatures on thursday and friday snow on saturday and clear_skies on sunday in durham\n",
            "\tPredict:it will be be on in on on\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you will find pizza_chicago at 915_arbol_dr it is 2_miles away without any traffic\n",
            "\tPredict:the is a away away away is away is\n",
            "Correct:fastest route being programmed for pizza_chicago\n",
            "\tPredict:setting navigation to the now\n",
            "Correct:you have a dentist_appointment on friday at 7pm\n",
            "\tPredict:your dentist_appointment is on friday at 7pm\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:conference scheduled for the_5th at 11am with the vice_president\n",
            "\tPredict:okay setting a reminder for the_5th the_5th the_5th at 11am\n",
            "Correct:have a good day\n",
            "\tPredict:you are welcome\n",
            "R:0.0260,W:14.2803:  23% 23/101 [00:06<00:21,  3.66it/s]Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:here are directions to 5672_barringer_street\n",
            "\tPredict:the nearest 5_miles is at\n",
            "Correct:the only route has heavy_traffic is that alright\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:directing to 5672_barringer_street now\n",
            "\tPredict:i sent the info on your screen you re welcome\n",
            "Correct:you re welcome and have a great day\n",
            "\tPredict:you re welcome\n",
            "Correct:taking you home\n",
            "\tPredict:there is no_traffic but i sent on your screen another route to reach the last is on your screen\n",
            "Correct:your address is 5671_barringer_street we re 3_miles away from home now\n",
            "\tPredict:home is at 5671_barringer_street\n",
            "Correct:your gps has been set for home\n",
            "\tPredict:gps set to home at 5671_barringer_street\n",
            "R:0.0272,W:15.5669:  24% 24/101 [00:06<00:19,  3.94it/s]Correct:what city do you want the weather for\n",
            "\tPredict:what city are you want to know if it will be cloudy in\n",
            "Correct:it won t be overcast or cloudy at all this week in carson\n",
            "\tPredict:it will be cloudy overcast this week in carson\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:palo_alto_cafe is 4_miles away and serves coffee and tea do you want the address\n",
            "\tPredict:there is a away from tea\n",
            "Correct:palo alto is located at 436_alger_dr\n",
            "\tPredict:palo_alto_cafe is 4_miles away\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:ok i ve set a reminder for your doctorappointment on the_12th at 4 50\n",
            "\tPredict:okay setting a reminder for your doctorappointment on the_12th at 4\n",
            "R:0.0285,W:15.9753:  25% 25/101 [00:06<00:19,  3.98it/s]Correct:what city would you like the forecast for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:it will be dry today snowy on tuesday dew on wednesday warm on thursday misty on friday snow on saturday and cloudy on sunday in grand_rapids\n",
            "\tPredict:the grand_rapids in grand_rapids is predicted to be on\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is a shopping_center 3_miles from you the address is 338_alester_ave would you like directions\n",
            "\tPredict:i have a away\n",
            "Correct:setting navigation to the midtown_shopping_center there is a car_collision_nearby so drive carefully\n",
            "\tPredict:the address is at\n",
            "Correct:i sent the info on your screen drive carefully\n",
            "\tPredict:you re welcome\n",
            "Correct:scheduled football_activity for today at 11am with ana\n",
            "\tPredict:okay scheduling a football_activity with with ana for ana today at 11am\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "R:0.0297,W:16.6480:  26% 26/101 [00:06<00:19,  3.84it/s]Correct:what city shall i check for you\n",
            "\tPredict:what city are you want the weather\n",
            "Correct:today is not humid in exeter it is stormy low of 40f high of 50f\n",
            "\tPredict:it is not in exeter right right now\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there isn t any rain predicted to fall in redwood_city this week\n",
            "\tPredict:there will be no raining this week in redwood_city this week\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you interested in\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:tomorrow in fresno it will rain\n",
            "\tPredict:there will be no hail in fresno tomorrow tomorrow\n",
            "Correct:making a note for yoga at 5pm on the_12th\n",
            "\tPredict:okay i note your yoga on on the_12th for the_12th\n",
            "R:0.0309,W:17.3210:  27% 27/101 [00:07<00:19,  3.87it/s]Correct:no hail in fresno tomorrow there will be rain\n",
            "\tPredict:no problem\n",
            "Correct:until next time\n",
            "\tPredict:no problem\n",
            "Correct:what city are you interested in\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:it will be cloudy with a low of 90f and a high of 100f in compton on sunday\n",
            "\tPredict:the weather forecast for be is - - - - - - -\n",
            "Correct:anytime\n",
            "\tPredict:you are welcome\n",
            "Correct:what city are you wanting to know the temperature for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:it will be between 60f - 70f today and 30f - 40f tomorrow in seattle\n",
            "\tPredict:it will be between 90f - 100f in in\n",
            "Correct:your doctorappointment is scheduled for friday at 11am\n",
            "\tPredict:your doctorappointment is at 11am\n",
            "R:0.0309,W:18.0647:  28% 28/101 [00:07<00:18,  3.93it/s]Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest starbucks is 3_miles away there is a road block on its way so i already sent on your screen another route to reach faster\n",
            "\tPredict:there is a starbucks\n",
            "Correct:yes it is hope you can have your coffee soon\n",
            "\tPredict:you re welcome\n",
            "Correct:the starbucks is located at 792_bedoin_street setting navigation now\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome drive safely and enjoy\n",
            "\tPredict:you re welcome\n",
            "Correct:yes i have one listed that is 1_miles away\n",
            "\tPredict:there is a parking_garage 1_miles away\n",
            "Correct:then let s go at palo_alto_garage_r i sent the info on your screen\n",
            "\tPredict:setting navigation to quickest route to your screen\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0309,W:18.7378:  29% 29/101 [00:07<00:17,  4.07it/s]Correct:it will be cloudy but not foggy in redwood_city on tuesday\n",
            "\tPredict:it will not be foggy in redwood_city on tuesday\n",
            "Correct:glad i could help out\n",
            "\tPredict:you re welcome\n",
            "Correct:navigating you to town_and_country\n",
            "\tPredict:the nearest mall is town_and_country\n",
            "Correct:setting gps for fastest route with no_traffic\n",
            "\tPredict:you re welcome\n",
            "Correct:the address is 383_university_ave i sent it on your screen\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome and we should be there soon\n",
            "\tPredict:you re welcome\n",
            "Correct:yoga is with alex at 11am\n",
            "\tPredict:your have is on scheduled at\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0322,W:19.3150:  30% 30/101 [00:07<00:16,  4.30it/s]Correct:there is valero 7_miles away with moderate_traffic on our way\n",
            "\tPredict:the nearest gas_station valero is 7_miles valero\n",
            "Correct:valero is at 200_alester_ave\n",
            "\tPredict:valero is the only one i can find it is the only option\n",
            "Correct:there is moderate_traffic on our way i sent the route on your screen\n",
            "\tPredict:there is a road_block_nearby but i sent on your screen another route to reach there\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you have two meeting scheduled one at 4pm in conference_room_102 to discuss_the_company_picnic and one at 6pm in 100_conference_room to go over the quarterly report\n",
            "\tPredict:your have two meeting scheduled one on the in conference_room_100 with the the one on at with the the the the the the the in the the the the the is the the the the the is the the the the is at\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:your appointment is for tuesday at 11am\n",
            "\tPredict:your doctorappointment is at 11am\n",
            "Correct:your next meeting is on monday at 7pm with the infrastructure_team to go_over_budget\n",
            "\tPredict:invited next meeting is is at at 1pm\n",
            "R:0.0334,W:19.9086:  31% 31/101 [00:08<00:21,  3.26it/s]Correct:what location do you want to know if it will snow\n",
            "\tPredict:what city are you want to know if it s going to snow\n",
            "Correct:it will snow on tuesday in mountain_view\n",
            "\tPredict:it will not snow in mountain_view mountain_view\n",
            "Correct:the nearest parking_garage is palo_alto_garage_r it s 1_miles away navigate you there\n",
            "\tPredict:the closest parking_garage is palo_alto_garage_r it s 2_miles away\n",
            "Correct:i sent the info and the quickest info on your screen you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your boss will be at the meeting on monday at 1pm\n",
            "\tPredict:your meeting is scheduled for at at\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "Correct:your swimming_activity with jon has been scheduled for dec 10th at 3pm\n",
            "\tPredict:okay scheduling a swimming_activity with jon for december jon at 3pm\n",
            "Correct:it is my job and pleasure\n",
            "\tPredict:you are welcome\n",
            "R:0.0334,W:20.4991:  32% 32/101 [00:08<00:20,  3.34it/s]Correct:no problem\n",
            "\tPredict:anytime\n",
            "Correct:we re 3_miles away from topanga_mall and there is heavy_traffic on its way\n",
            "\tPredict:the nearest mall is topanga_mall\n",
            "Correct:you will find topanga_mall at 171_oak_rd\n",
            "\tPredict:topanga_mall is at 610_amarillo_ave\n",
            "Correct:setting navigation now\n",
            "\tPredict:topanga_mall is at\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:bye\n",
            "\tPredict:you re welcome\n",
            "Correct:when\n",
            "\tPredict:what time and time should i schedule\n",
            "Correct:okay we set up your doctorappointment today at 1pm\n",
            "\tPredict:okay setting a reminder for for today at 1pm 1pm 1pm\n",
            "R:0.0334,W:22.5684:  33% 33/101 [00:08<00:18,  3.69it/s]Correct:it will be stormy and hailing in san_francisco in the next two_days\n",
            "\tPredict:it will not be warm in san_francisco on\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:the_westin is 4_miles away with moderate_traffic will that help\n",
            "\tPredict:the is is is\n",
            "Correct:the_westin is at 329_el_camino_real do you need me to set the navigation\n",
            "\tPredict:navigating you to the_westin\n",
            "Correct:navigation set have a great day\n",
            "\tPredict:you re welcome\n",
            "Correct:for which one i have two one on the_8th at 11am and one on wednesday at 11am\n",
            "\tPredict:your lab_appointment is on on on on with at\n",
            "Correct:your dinner is on tuesday with your sister\n",
            "\tPredict:your dinner is at 5pm on\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0334,W:23.2146:  34% 34/101 [00:09<00:17,  3.91it/s]Correct:where are you wanting to know if it s going to rain or not\n",
            "\tPredict:what city are you want the know the\n",
            "Correct:it is not going to rain in san_francisco this week\n",
            "\tPredict:no it s not gonna rain in san_francisco\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you wanting to know the weather for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:tomorrow in seattle is looking warm with a low of 70f and a high of 90f no sign of fog\n",
            "\tPredict:it will not be foggy today and tomorrow in compton\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:what city would you like the 7 day forecast for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0334,W:23.7854:  35% 35/101 [00:09<00:16,  3.90it/s]Correct:it should not be humid in compton next_week\n",
            "\tPredict:it will not be humid in compton in\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:tom lives at 580_van_ness_ave\n",
            "\tPredict:you re two grocery_stores listed at tom s house\n",
            "Correct:i did you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your doctor_appointment is on monday at 2pm\n",
            "\tPredict:your doctorappointment is at 3pm\n",
            "Correct:what time on the_1st should i set a reminder to watch the football game\n",
            "\tPredict:what time and time should i the the football game\n",
            "Correct:i have set a reminder for the football game at 2pm on the_1st\n",
            "\tPredict:okay i ve set a reminder for for at\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0347,W:24.3996:  36% 36/101 [00:09<00:16,  4.01it/s]Correct:what city do you want the weather forecast for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:today in downtown_chicago there should be hail with a high of 70f\n",
            "\tPredict:it downtown_chicago not be today in downtown_chicago today\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:on thursday the temperature in alhambra will be low of 20f and high of 30f\n",
            "\tPredict:there will be be in alhambra on thursday\n",
            "Correct:happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you inquiring about\n",
            "\tPredict:what city would you like the weather the weather for\n",
            "Correct:what would you like to know about the weather in durham\n",
            "\tPredict:it will not be in in in in\n",
            "Correct:the_4th at 1pm with tom the_10th at 9am with ana the_13th at 9am with your father the_5th at 2pm with alex and the_10th at 4pm with your sister\n",
            "\tPredict:your have dental appointment on on on\n",
            "R:0.0359,W:25.0219:  37% 37/101 [00:09<00:16,  3.87it/s]Correct:the weekly forecast does not show that there will be a blizzard in durham this week\n",
            "\tPredict:there will be a blizzard in durham durham durham\n",
            "Correct:not a problem\n",
            "\tPredict:you re welcome\n",
            "Correct:is there a specific day you would like to know the weather for in new_york\n",
            "\tPredict:what days are you like the weather for\n",
            "Correct:it will be dry and between 30f - 50f friday in new_york\n",
            "\tPredict:it is not predicted to be in in\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:it is cloudy right now in cleveland\n",
            "\tPredict:what will be you in in\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:taking_medicine with your brother at 6pm on the_13th has been scheduled\n",
            "\tPredict:okay scheduling taking_medicine with brother for next the_13th at 6pm\n",
            "R:0.0359,W:25.6672:  38% 38/101 [00:10<00:16,  3.83it/s]Correct:the palo_alto_medical_foundation is 4_miles away at 611_ames_ave would you like directions there\n",
            "\tPredict:the closest hospital is standford childrens health\n",
            "Correct:there is moderate_traffic on our way and you should be able to reach there fast i sent all the info on your screen drive carefully\n",
            "\tPredict:there is moderate_traffic on our way i sent the best route on your screen\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is a safeway 2_miles away would you like to get its address\n",
            "\tPredict:the nearest grocery_store is safeway\n",
            "Correct:safeway is at 452_arcadia_pl\n",
            "\tPredict:there is heavy_traffic to safeway\n",
            "Correct:the only route has moderate_traffic is that going to be okay\n",
            "\tPredict:you re welcome\n",
            "Correct:your football_activity is on sunday at 9am with your brother\n",
            "\tPredict:you have two football scheduled one on the_5th with with brother and another on on with at\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "R:0.0371,W:26.3785:  39% 39/101 [00:10<00:17,  3.60it/s]Correct:it is not hailing in manhattan the forecast says it is dry today\n",
            "\tPredict:it will not be in manhattan manhattan in manhattan\n",
            "Correct:any time\n",
            "\tPredict:you re welcome\n",
            "Correct:could you please let me know what place i am looking up the data for\n",
            "\tPredict:what city would you like to know if it will be cloudy in\n",
            "Correct:this week boston does not have any weather predictions of it being cloudy\n",
            "\tPredict:it will not be cloudy boston boston this week\n",
            "Correct:no problem my driver\n",
            "\tPredict:you re welcome\n",
            "Correct:safeway is the only grocery_store listed it has moderate_traffic will that be ok\n",
            "\tPredict:you re welcome\n",
            "Correct:setting destination now\n",
            "\tPredict:gps set for safeway\n",
            "Correct:you re welcome\n",
            "\tPredict:gps re welcome\n",
            "R:0.0371,W:27.1394:  40% 40/101 [00:10<00:16,  3.69it/s]Correct:for which time period did you want weather information\n",
            "\tPredict:what city are you interested in weather for\n",
            "Correct:it is between 50f - 70f in new_york right now\n",
            "\tPredict:current new_york is the only low of 90f and a high of 100f\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you in\n",
            "\tPredict:what city are you want the\n",
            "Correct:this week in manhattan the temperature will range from 20f to 100f\n",
            "\tPredict:it will be between it will be between 30f in manhattan this week\n",
            "Correct:i m here to assist you\n",
            "\tPredict:you re welcome\n",
            "Correct:okay setting a reminder to take medicine with your brother frank at 9am on sunday\n",
            "\tPredict:okay setting a reminder medicine medicine sunday sunday 9am sunday with your brother\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "R:0.0371,W:27.9056:  41% 41/101 [00:10<00:16,  3.66it/s]Correct:there is no starbucks locally\n",
            "\tPredict:the nearest starbucks is 1_miles away at away away away away\n",
            "Correct:teavana is 5_miles away at 145_amherst_st with moderate_traffic\n",
            "\tPredict:starbucks is coffee shop is away\n",
            "Correct:jack lives at 864_almanor_ln\n",
            "\tPredict:jill lives at\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your conference is set for monday with your boss to go_over_budget\n",
            "\tPredict:your conference is conference with your the boss is at at\n",
            "Correct:the conference is with your boss\n",
            "\tPredict:you are welcome\n",
            "Correct:you re welcome don t forget it s held in conference_room_102\n",
            "\tPredict:you re welcome\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "R:0.0384,W:28.6424:  42% 42/101 [00:11<00:14,  3.94it/s]Correct:the forecast says there will be snow in fresno there is no mention of clear_skies on wednesday\n",
            "\tPredict:there will be clear_skies in fresno on wednesday\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the current route is the fastest one i can find\n",
            "\tPredict:there is moderate_traffic on our way but i sent on your screen the best route on your screen\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:palo_alto_garage_r is at 481_amaranta_ave\n",
            "\tPredict:the nearest parking_garage is at 481_amaranta_ave at\n",
            "Correct:i will send on your screen the quickest route sure there is heavy_traffic now and we will have to make the route slightly longer\n",
            "\tPredict:the quickest route to palo_alto_garage_r is that is\n",
            "Correct:you re welcome we should arrive shortly\n",
            "\tPredict:you re welcome\n",
            "Correct:which appointment would you like the date for\n",
            "\tPredict:your appointment is on for thursday at\n",
            "R:0.0396,W:29.1998:  43% 43/101 [00:11<00:15,  3.74it/s]Correct:what city would you like to hear this weather information for\n",
            "\tPredict:there is no chance chance chance snow this week in\n",
            "Correct:it will not snow in san_mateo this week\n",
            "\tPredict:there is no chance chance of snow this week in san_mateo\n",
            "Correct:whole_foods located at 819_alma_st is 1_miles away although there is a car_collision_nearby\n",
            "\tPredict:the is grocery_store is\n",
            "Correct:the current route is the quickest\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the one at 6pm with jon is on the_11th\n",
            "\tPredict:you have dinner event on the_2nd with jon and one on on with\n",
            "Correct:glad i can help\n",
            "\tPredict:you are welcome\n",
            "Correct:what time do you want to go swimming\n",
            "\tPredict:what time and time should i schedule\n",
            "R:0.0408,W:29.9348:  44% 44/101 [00:11<00:14,  3.85it/s]Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:which location do you want to know the current temperature of\n",
            "\tPredict:what city are you want the temperature for\n",
            "Correct:the temperature in alameda now is low 60f and high 80f\n",
            "\tPredict:the current alameda is in the low of\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:on friday on san_francisco the weather will be dry with a high of 80f\n",
            "\tPredict:there will be clear_skies in san_francisco on friday\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:the_clement_hotel is 8_miles away at 657_ames_ave\n",
            "\tPredict:the nearest hotel is the_clement_hotel 8_miles away\n",
            "Correct:there is currently moderate_traffic on the way to the hotel setting directions now\n",
            "\tPredict:the_clement_hotel is at 657_ames_ave\n",
            "R:0.0421,W:30.5661:  45% 45/101 [00:11<00:14,  3.88it/s]Correct:what city are you wanting to know the weather for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:in los_angeles it is going to be hot today with a low of 40f and a high of 60f tomorrow will be rainy with a low of 40f and a high of 60f\n",
            "\tPredict:what will be be in in in\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you inquiring about\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:in seattle there is snow right now\n",
            "\tPredict:there is no hail in seattle right now\n",
            "Correct:you are most welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:what city can i check for snow for you\n",
            "\tPredict:what city are you want to know if it will snow\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0433,W:31.1860:  46% 46/101 [00:12<00:13,  3.95it/s]Correct:snow is predicted to fall on saturday in alameda\n",
            "\tPredict:it will not snow in alameda alameda alameda\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:peets_coffee also serves tea they are 2_miles away at 9981_archuleta_ave\n",
            "\tPredict:the closest is away 4_miles away at away is away\n",
            "Correct:what time would like the reminder\n",
            "\tPredict:what time and time should i the reminder\n",
            "Correct:i have set a reminder for sunday at 2pm\n",
            "\tPredict:okay setting a reminder for for sunday for sunday at 2pm\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:i ve scheduled a dentist_appointment for 3pm on the_12th anything else\n",
            "\tPredict:okay i a dentist_appointment for your on on the_12th\n",
            "Correct:i have scheduled the appointment\n",
            "\tPredict:you are welcome\n",
            "R:0.0458,W:31.6261:  47% 47/101 [00:12<00:13,  3.97it/s]Correct:moderate_traffic is listed to peets_coffee\n",
            "\tPredict:the is is at\n",
            "Correct:displaying directions now\n",
            "\tPredict:setting navigation to peets_coffee now\n",
            "Correct:you re welcome drive safely and enjoy\n",
            "\tPredict:you re welcome\n",
            "Correct:the_westin is located 7_miles away with no_traffic setting directions now to 329_el_camino_real\n",
            "\tPredict:the is 7_miles the_westin 7_miles away away is away\n",
            "Correct:setting navigation now\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:jon and jeff will be attending your tennis_activity\n",
            "\tPredict:your have two tennis scheduled one on with with with with on on\n",
            "Correct:they will be there at 1pm\n",
            "\tPredict:your tennis_activity be tennis_activity on tuesday your your your your your your your mother with your mother\n",
            "R:0.0470,W:32.7228:  48% 48/101 [00:12<00:13,  4.08it/s]Correct:what city do you want to hear the forecast report from\n",
            "\tPredict:what city would you like the weather for\n",
            "Correct:it will be stormy in camarillon on friday\n",
            "\tPredict:it will not be foggy in camarillo on camarillo\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:saturday it will snow in fresno\n",
            "\tPredict:it will not snow in fresno this week\n",
            "Correct:glad to help\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:the football game is on thursday\n",
            "\tPredict:your football game is at at\n",
            "Correct:you have an optometrist_appointment today at 1pm\n",
            "\tPredict:your conference is at 3pm\n",
            "R:0.0495,W:33.1710:  49% 49/101 [00:12<00:12,  4.21it/s]Correct:there aren t any cloudy days in the forecast for inglewood\n",
            "\tPredict:it will not be cloudy in inglewood next_week\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:home is 5_miles away at 5671 barringer street\n",
            "\tPredict:your home is at at\n",
            "Correct:you have two swimming_activity scheduled one on wednesday at 5pm and the other on the_5th at 4pm\n",
            "\tPredict:you have two swimming_activity scheduled one on with with with on on on\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:the agenda for your meeting is to discuss_the_merger\n",
            "\tPredict:your meeting is at 3pm\n",
            "Correct:your meeting is on wednesday at 3pm in conference_room_100\n",
            "\tPredict:you have meeting is on wednesday at\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0507,W:33.6760:  50% 50/101 [00:13<00:12,  4.10it/s]Correct:no the only route available has heavy_traffic i m sorry\n",
            "\tPredict:you re welcome\n",
            "Correct:sorry about it enjoy some music in the meantime\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest hotel is hotel_keen it s 3_miles away do you want its address\n",
            "\tPredict:the nearest hotel is hotel_keen 3_miles away\n",
            "Correct:578_arbol_dr there is no_traffic and you should arrive in a couple minutes\n",
            "\tPredict:hotel_keen is at 578_arbol_dr\n",
            "Correct:i picked the best route for you drive carefully\n",
            "\tPredict:gps set for hotel_keen at 578_arbol_dr\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:okay i will set a schedule for a swimming_activity with marie on sunday at 6pm\n",
            "\tPredict:okay scheduling a swimming_activity with marie on marie on sunday at 6pm\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0532,W:34.1657:  50% 51/101 [00:13<00:12,  4.04it/s]Correct:okay what city should i look in\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:during the week it will be cloudy in grand_rapids only today\n",
            "\tPredict:it will not be cloudy grand_rapids grand_rapids on\n",
            "Correct:glad i could help out\n",
            "\tPredict:you re welcome\n",
            "Correct:there is heavy_traffic in this moment but we re just 1_miles away it shouldn t take too long to reach there\n",
            "\tPredict:your home is at 10_ames_street\n",
            "Correct:i sent all the info on your screen drive carefully\n",
            "\tPredict:setting navigation to quickest route now\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:i have located a parking_garage 2_miles away at 880_ames_ct called webster_garage\n",
            "\tPredict:there is a parking_garage 1_miles away\n",
            "Correct:it is located 2_miles away with moderate_traffic en route\n",
            "\tPredict:there is moderate_traffic on our way\n",
            "R:0.0532,W:34.7865:  51% 52/101 [00:13<00:12,  3.98it/s]Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:your doctorappointment at 2pm your aunt will be going\n",
            "\tPredict:your doctorappointment is on at with with your with at\n",
            "Correct:happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:i am setting an appointment at the dentist on the_12th of current month at 9am with martha\n",
            "\tPredict:okay setting for a reminder for at at at with martha on the_12th of month\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:swimming plans\n",
            "\tPredict:okay setting a reminder for swimming_activity swimming swimming with your\n",
            "Correct:swimming plans on friday at 11am have been added to the calendar\n",
            "\tPredict:okay setting a reminder for your friday at 11am on friday\n",
            "Correct:ok\n",
            "\tPredict:you re welcome\n",
            "R:0.0532,W:36.6263:  52% 53/101 [00:13<00:11,  4.05it/s]Correct:there is not gonna be any blizzard this week in alhambra on tuesday it will rain\n",
            "\tPredict:it is not blizzard in alhambra alhambra on tuesday\n",
            "Correct:you are very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is 2 tea shops near you teavana is 4_miles away and cafe_venetia is 5_miles away which one do you want\n",
            "\tPredict:i have a away\n",
            "Correct:teavana is the fastest journey because there is no_traffic on the way there cafe_venetia has moderate_traffic should we go to teavana\n",
            "\tPredict:cafe_venetia is located at 269_alger_drive\n",
            "Correct:i sent on your screen its position and the fastest route to get there\n",
            "\tPredict:145_amherst_st is the teavana\n",
            "Correct:teavana is located at 145_amherst_st\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome glad i could be of assistance\n",
            "\tPredict:you re welcome\n",
            "Correct:your agenda for your meeting today is go_over_budget\n",
            "\tPredict:your meeting is at at\n",
            "R:0.0532,W:37.3125:  53% 54/101 [00:14<00:11,  3.96it/s]Correct:there is no fog in the forecast for exeter next_week\n",
            "\tPredict:it will not be foggy in exeter on next_week\n",
            "Correct:you re welcome\n",
            "\tPredict:no problem\n",
            "Correct:this week the highest_temperature in cleveland will be 100f on saturday\n",
            "\tPredict:what will highest_temperature in cleveland cleveland cleveland this week\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:stanford_shopping_center is the only nearby shopping_center\n",
            "\tPredict:the shopping_center is is at\n",
            "Correct:i do not have info on the local starbucks but i do see another coffee place would you like to hear info on it\n",
            "\tPredict:the nearest starbucks is at\n",
            "Correct:there is coupa located at 394_van_ness_ave there is moderate_traffic but you re just 6_miles away\n",
            "\tPredict:the address is starbucks is\n",
            "Correct:you are welcome\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "R:0.0545,W:38.0992:  54% 55/101 [00:14<00:11,  3.84it/s]Correct:there is a car_collision_nearby stanford_shopping_center\n",
            "\tPredict:stanford_shopping_center is the only shopping_center\n",
            "Correct:the route to 773_alger_dr set to the quickest possible\n",
            "\tPredict:i sent the info on your screen you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:philz serves coffee and is 6_miles away at 583_alester_ave\n",
            "\tPredict:the nearest coffee shop is philz which is 2_miles away\n",
            "Correct:there is no_traffic on the 6_miles route to philz enjoy\n",
            "\tPredict:setting navigation to philz now\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you have 3 optometrist_appointment scheduled one with marie on the_2nd martha on saturday and your brother on the_16th\n",
            "\tPredict:you have two optometrist_appointment scheduled one on the_2nd with marie and one on on on with with at\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0569,W:38.4801:  55% 56/101 [00:14<00:12,  3.66it/s]Correct:in new_york it will be raining monday and tuesday snowing wednesday windy on thursday frost on friday dew on saturday and overcast on sunday\n",
            "\tPredict:what city would you like the weather for new_york\n",
            "Correct:you re welcome patience\n",
            "\tPredict:no problem\n",
            "Correct:there isn t any traffic to pizza_my_heart at 528_anton_ct it will be a 6_miles drive\n",
            "\tPredict:the quickest route is is\n",
            "Correct:quickest route with no_traffic set for pizza_my_heart have a great day\n",
            "\tPredict:setting navigation to pizza_my_heart now\n",
            "Correct:there is a chef_chu_s and a tai_pan\n",
            "\tPredict:i have a chinese_restaurant listed at would you like to\n",
            "Correct:that would be chef_chu_s\n",
            "\tPredict:tai_pan is whichever the fastest route to tai_pan\n",
            "Correct:chef_chu_s is at 593_arrowhead_way\n",
            "\tPredict:chef_chu_s is whichever the way to chef_chu_s at chef_chu_s is that is set\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0582,W:39.3925:  56% 57/101 [00:15<00:12,  3.50it/s]Correct:what city do you want the weather for\n",
            "\tPredict:what city would you like the weather forecast for\n",
            "Correct:in manhattan it will be stormy on wednesday and thursday\n",
            "\tPredict:it will not be stormy in manhattan in this week\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:hotel_keen is a rest_stop located 3_miles away there is a car_collision_nearby but should delay you to long\n",
            "\tPredict:the quickest route is is\n",
            "Correct:the address for hotel_keen is 578_arbol_dr 3_miles away with a car_collision_nearby\n",
            "\tPredict:hotel_keen is at 578_arbol_dr\n",
            "Correct:i sent the quickest route on your screen\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:if you drive carefully it will be for sure\n",
            "\tPredict:you re welcome\n",
            "R:0.0582,W:40.1844:  57% 58/101 [00:15<00:11,  3.62it/s]Correct:in los_angeles it is gonna be warm on tuesday\n",
            "\tPredict:it will not be warm in los_angeles this week\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest pizza_restaurant with no_traffic is 3_miles away at 113_anton_ct do you want to go to the round_table\n",
            "\tPredict:the nearest pizza_restaurant is is at it is away\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:palo_alto_medical_foundation at 611_ames_ave is 4_miles away but there is no_traffic on the way setting navigation now\n",
            "\tPredict:the is located 214_el_camino_real\n",
            "Correct:your home is at 56_cadwell_street\n",
            "\tPredict:we re 6_miles away from teavana and there is moderate_traffic on our way\n",
            "Correct:no problem\n",
            "\tPredict:setting navigation to 56_cadwell_street now\n",
            "Correct:your dinner reservation is at 5pm\n",
            "\tPredict:your dinner is at 5pm\n",
            "R:0.0594,W:41.4051:  58% 59/101 [00:15<00:11,  3.71it/s]Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your lab_appointment is on the_5th and your aunt will be attending\n",
            "\tPredict:you are welcome\n",
            "Correct:the lab_appointment is schedule for 2pm on the_5th and your aunt will be attending\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:ok dentist_appointment with your dad on the_1st at 2pm is set\n",
            "\tPredict:okay setting a reminder for your dentist_appointment with at 2pm on\n",
            "Correct:what date and time is your swim appointment\n",
            "\tPredict:okay setting a reminder for swim swim swim swim swim swim\n",
            "Correct:i have added swim appointment to your calendar at 10am on thursday\n",
            "\tPredict:okay setting a reminder for your swim at 10am thursday at 10am\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0606,W:41.9198:  59% 60/101 [00:15<00:10,  3.94it/s]Correct:the weather in cleveland on thursday will have dew and a high of 30f\n",
            "\tPredict:what days are you interested in\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest gas_station is located 5_miles away need more info\n",
            "\tPredict:the nearest gas_station is located at 5_miles away would you like directions\n",
            "Correct:setting gps for quickest route to valero gas_station now\n",
            "\tPredict:you re welcome\n",
            "Correct:valero is located at 200_alester_ave\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome glad i could help\n",
            "\tPredict:you re welcome\n",
            "Correct:your home is 4_miles away but there is heavy_traffic on our way i sent the best available route on your screen\n",
            "\tPredict:the quickest route home is 4_miles away at 9981_archuleta_ave\n",
            "Correct:sure it s 56_cadwell_street you can see it on your screen\n",
            "\tPredict:you re welcome\n",
            "R:0.0606,W:42.7508:  60% 61/101 [00:16<00:10,  3.86it/s]Correct:the temperatures in seattle will be high this week and the hottest days will be monday and tuesday with 100f ( but pay attention it s gonna rain too )\n",
            "\tPredict:it is not hot in boston boston on saturday\n",
            "Correct:the highs for boston this weekend are in the 90s while the lows are supposed to be 70s\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome glad i could help\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:stanford_shopping_center is 4_miles away\n",
            "\tPredict:the shopping_center is is at at\n",
            "Correct:ok you will headed toward stanford_shopping_center at 773_alger_dr located 4_miles away\n",
            "\tPredict:the quickest route to stanford_shopping_center is that is\n",
            "Correct:okay scheduling a swimming_activity with your brother for the_7th at 11am\n",
            "\tPredict:okay scheduling a swimming_activity with brother for the_7th at 11am\n",
            "Correct:glad to help\n",
            "\tPredict:you are welcome\n",
            "R:0.0606,W:43.3421:  61% 62/101 [00:16<00:10,  3.61it/s]Correct:the weather forecast predicts that it will be stormy in los_angeles on sunday\n",
            "\tPredict:it will not be windy in los_angeles on sunday\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:for which one i have two one at 6pm and one at 3pm\n",
            "\tPredict:you have two conference scheduled one on the_1st with the and one on with at with the the the and the\n",
            "Correct:the event is on sunday at 6pm in conference_room_102 and management will be attending\n",
            "\tPredict:you have two conference scheduled one at with the at with at with the the on and with the the the the and the the the the the the other is at at\n",
            "Correct:no problem goodbye\n",
            "\tPredict:you re welcome\n",
            "Correct:currently you are taking_medicine on two occasions one is on the_13th at 4pm with your mother the other is on monday at 11am with marie\n",
            "\tPredict:your have is take medicine medicine at at at with and with and on\n",
            "Correct:hr will be at the friday meeting\n",
            "\tPredict:your meeting is at at 7pm\n",
            "R:0.0606,W:44.1558:  62% 63/101 [00:16<00:11,  3.30it/s]Correct:there is a travelers_lodge one miles away\n",
            "\tPredict:there is a within 1_miles away\n",
            "Correct:travelers_lodge 333_arbol_dr\n",
            "\tPredict:travelers_lodge is located at\n",
            "Correct:gps set for shortest route\n",
            "\tPredict:gps set for travelers_lodge route to travelers_lodge\n",
            "Correct:have a good day\n",
            "\tPredict:thanks re welcome\n",
            "Correct:you are playing tennis on 11am friday\n",
            "\tPredict:your tennis playing tennis scheduled at\n",
            "Correct:your next lab_appointment is next monday at 2pm\n",
            "\tPredict:your lab_appointment is on your lab_appointment\n",
            "Correct:okay scheduling a dentist_appointment with ana on the_8th at 1pm\n",
            "\tPredict:okay scheduling a dentist_appointment for the_8th at 1pm 1pm the_8th\n",
            "Correct:glad i can assist you\n",
            "\tPredict:you re welcome\n",
            "R:0.0606,W:44.8135:  63% 64/101 [00:16<00:10,  3.59it/s]Correct:what city do you want the weather for\n",
            "\tPredict:what city do you want the now\n",
            "Correct:there are no clouds in fresno right now\n",
            "\tPredict:in fresno it will be a low of 80f and a high of 100f\n",
            "Correct:you have four yoga_activity scheduled one on monday with alex two on friday with sister and one with marie and one on the_17th with martha\n",
            "\tPredict:your yoga_activity is yoga_activity scheduled one at 4pm with martha on at at\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:you have two dentist_appointment one with your aunt and one with your sister both on monday\n",
            "\tPredict:your dentist_appointment is on monday at at\n",
            "Correct:glad i can assist you\n",
            "\tPredict:you are welcome\n",
            "Correct:okay scheduling taking_medicine with martha at 5pm next wednesday\n",
            "\tPredict:okay scheduling taking_medicine with martha for next wednesday at 5pm\n",
            "Correct:taking_medicine scheduled for next wednesday at 5pm\n",
            "\tPredict:you are welcome\n",
            "R:0.0619,W:45.3688:  64% 65/101 [00:17<00:10,  3.42it/s]Correct:anytime\n",
            "\tPredict:you are welcome\n",
            "Correct:there is no starbucks listed locally but i have another coffee shop listed would you like the address to it\n",
            "\tPredict:the nearest starbucks is 4_miles away at away\n",
            "Correct:then let s go at cafe_venetia 269_alger_dr\n",
            "\tPredict:the address is starbucks\n",
            "Correct:the destination is 4_miles away in no_traffic\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is no starbucks close but there is two others one is coupa that is 6_miles away and the other is cafe_venetia which is 3_miles away which one do you want\n",
            "\tPredict:the nearest starbucks is at away\n",
            "Correct:cafe_venetia is located at 269_alger_drive with heavy_traffic is that going to be okay\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:i sent the info on your screen drive carefully\n",
            "\tPredict:you re welcome\n",
            "R:0.0619,W:46.1845:  65% 66/101 [00:17<00:10,  3.48it/s]Correct:where would you like to know if it will snow\n",
            "\tPredict:what city are you want to know if it s snow\n",
            "Correct:no it s not forecast to snow in corona today the forecast is humid with a low of 70f and a high of 90f\n",
            "\tPredict:it will not snow in corona corona corona today\n",
            "Correct:you re welcome have a great day\n",
            "\tPredict:you re welcome\n",
            "Correct:the travelers_lodge is closest to you at 6_miles away in moderate_traffic would you like directions there\n",
            "\tPredict:i have a couple listings for the_westin which is\n",
            "Correct:i m setting the gps for 333_arbol_dr\n",
            "\tPredict:travelers_lodge is located at\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:we re 5_miles away from p_.f_._changs do you want to go there\n",
            "\tPredict:i have a listing for a chinese_restaurant that is 2_miles away\n",
            "R:0.0631,W:46.7011:  66% 67/101 [00:17<00:09,  3.51it/s]Correct:what city would you like weather information about\n",
            "\tPredict:what city do you want the rain forecast for\n",
            "Correct:the weather forecast for oakland does not show rain right now\n",
            "\tPredict:it is not raining right now in oakland now\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:taking you to p_.f_._changs\n",
            "\tPredict:p_.f_._changs is at 669_el_camino_real\n",
            "Correct:the addres of p_.f_._changs is 669_el_camino_real\n",
            "\tPredict:p_.f_._changs is at 669_el_camino_real\n",
            "Correct:navigation set we should arrive shortly\n",
            "\tPredict:navigation set to p_.f_._changs\n",
            "Correct:i will schedule a conference at 6pm on the tenth with sales_team\n",
            "\tPredict:okay setting a reminder for conference 6pm tenth 6pm tenth with the sales_team\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0656,W:47.1651:  67% 68/101 [00:18<00:09,  3.64it/s]Correct:chevron is 6_miles away\n",
            "\tPredict:the closest gas_station is chevron at\n",
            "Correct:i m sorry chevron is the only gas_station i am able to locate\n",
            "\tPredict:chevron is the closest gas_station at 783_arcadia_pl\n",
            "Correct:its address is 783_arcadia_pl i sent it on your screen\n",
            "\tPredict:i sent the\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:okay setting a reminder to take your medicine with ana at 7pm on tuesday\n",
            "\tPredict:okay setting a reminder medicine medicine tuesday 7pm tuesday 7pm with\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your reminder has been set for saturday at 2pm to take medicine with martha\n",
            "\tPredict:okay setting a reminder medicine medicine with with martha saturday saturday at 2pm\n",
            "R:0.0656,W:47.8414:  68% 69/101 [00:18<00:08,  3.68it/s]Correct:stanford_express_care is 7_miles away though there is a car_collision_nearby would you like directions there\n",
            "\tPredict:stanford_express_care is 7_miles away at 214_el_camino_real\n",
            "Correct:okay heading to 214_el_camino_real\n",
            "\tPredict:stanford_express_care is at 214_el_camino_real\n",
            "Correct:we re 3_miles away from chef_chu_s and 6_miles away from mandarin_roots\n",
            "\tPredict:there is chinese_restaurant p_.f_._changs 6_miles away\n",
            "Correct:chef_chu_s is closer\n",
            "\tPredict:chef_chu_s is is 3_miles away i sent its position on your screen\n",
            "Correct:chef_chu_s is located at 593_arrowhead_way\n",
            "\tPredict:chef_chu_s is is chef_chu_s\n",
            "Correct:chef_chu_s is 3_miles away through moderate_traffic\n",
            "\tPredict:chef_chu_s is the only one i sent on your screen the best route to reach there\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there are no starbucks around here but we re 4_miles away from cafe_venetia is it okay to go there\n",
            "\tPredict:the nearest is is 4_miles away away is away you\n",
            "R:0.0668,W:48.6362:  69% 70/101 [00:18<00:08,  3.55it/s]Correct:there is currently snow in san_francisco\n",
            "\tPredict:what city are you in in san_francisco\n",
            "Correct:i sent the address with the fastest route on screen drive carefully\n",
            "\tPredict:you re welcome\n",
            "Correct:your next meeting is with your boss at 10am in conference_room_102\n",
            "\tPredict:your have is meeting scheduled at one on at with the the at with the the\n",
            "Correct:you re welcome did you also want to know the agenda for the meeting\n",
            "\tPredict:you are welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:am or pm\n",
            "\tPredict:what time would you like the swimming swimming 4 at\n",
            "Correct:swimming reminder has been set for the_12th at 4 50pm\n",
            "\tPredict:okay i a pm\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0681,W:49.8000:  70% 71/101 [00:18<00:07,  3.80it/s]Correct:happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:which one i have two one with your father on the_3rd and one with tom on the_9th\n",
            "\tPredict:you have two doctor_appointment scheduled one on the_1st with and one on and one on at at with at\n",
            "Correct:the appointment on the_9th is at 3pm\n",
            "\tPredict:you are welcome\n",
            "Correct:your tennis_activity is set for 11am with jon in attendance\n",
            "\tPredict:your tennis_activity is tennis_activity on the_3rd at 11am with your and one on\n",
            "Correct:what day and time is the conference\n",
            "\tPredict:what time and time should i set the conference\n",
            "Correct:i have that conference set up for you to be reminded of is there anything else you need\n",
            "\tPredict:okay setting a reminder for your august 14th on august 14th at 4pm\n",
            "Correct:your reminder for the conference with the executive_team on august the_14th at 4pm is all set\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0681,W:50.4548:  71% 72/101 [00:19<00:07,  3.63it/s]Correct:what city would you like weather information about\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:in san_jose it is expected to be between 40f - 60f monday and tuesday 70f - 80f wednesday 90f - 100f thursday 70f - 90f friday 30f - 40f saturday and 40f - 60f on sunday\n",
            "\tPredict:what san_jose in san_jose in the\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:in san_mateo it will be stormy on saturday and hail on sunday\n",
            "\tPredict:what city are gonna be in san_mateo this week\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you interested in\n",
            "\tPredict:what city do you want the temperature for\n",
            "Correct:the temperature in los_angeles this week reaches a low of 40f and the highest will be 100f\n",
            "\tPredict:it los_angeles be be on in on on on\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "R:0.0693,W:51.0271:  72% 73/101 [00:19<00:07,  3.63it/s]Correct:you re welcome hope you have a great day\n",
            "\tPredict:you re welcome\n",
            "Correct:where are you interested in\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:in camarillo it will be stormy on tuesday\n",
            "\tPredict:it will not be stormy in camarillo camarillo camarillo\n",
            "Correct:happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:friday at 3pm\n",
            "\tPredict:your next conference is on friday at 3pm\n",
            "Correct:okay i am scheduling a conference with your boss for the_5th of this month at 1pm\n",
            "\tPredict:okay setting a reminder for the_5th the_5th the_5th at at\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:okay scheduling a tennis_activity with martha for friday at 4pm\n",
            "\tPredict:okay scheduling a tennis_activity with martha for friday at 4pm\n",
            "R:0.0705,W:51.9050:  73% 74/101 [00:19<00:07,  3.73it/s]Correct:it will not be windy in atherton in the next_few_days\n",
            "\tPredict:it will not be windy in atherton atherton on\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the travelers_lodge is located 4_miles away at 333_arbol_dr would you like directions there\n",
            "\tPredict:the rest_stop is is located at away away at is away\n",
            "Correct:ok i am setting gps for 333_arbol_dr\n",
            "\tPredict:travelers_lodge is located at 611_ames_ave\n",
            "Correct:you re welcome have a great day\n",
            "\tPredict:you re welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:okay scheduling a reminder for your tennis_activity at 5pm on the_10th with jon\n",
            "\tPredict:okay setting a reminder for tennis_activity with jon tenth tenth tenth at 5pm\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0743,W:52.2596:  74% 75/101 [00:19<00:06,  3.75it/s]Correct:jacks_house is at 864_almanor_ln\n",
            "\tPredict:the is is away away friend s house is\n",
            "Correct:taking you to jack s house\n",
            "\tPredict:i sent the away\n",
            "Correct:i sent all the info on your screen drive carefully\n",
            "\tPredict:gps set for jack s house\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:there are the ravenswood_shopping_center and the stanford_shopping_center both 3_miles away do you want me to flip a coin between them or will you pick where to go\n",
            "\tPredict:the nearest mall is stanford_shopping_center stanford_shopping_center is you\n",
            "Correct:navigating you to ravenswood_shopping_center\n",
            "\tPredict:setting navigation to stanford_shopping_center 113_arbol_dr\n",
            "Correct:okay which one i have one for the_12th at 9am and one for the_4th at 5pm\n",
            "\tPredict:your optometrist_appointment is optometrist_appointment on one on with with your with and one on at\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0743,W:53.1102:  75% 76/101 [00:20<00:07,  3.57it/s]Correct:what city are you wanting to know if it s going to rain\n",
            "\tPredict:what city are you want to know if it s going to rain\n",
            "Correct:it will rain on wednesday in redwood_city\n",
            "\tPredict:it is not to rain in redwood_city redwood_city\n",
            "Correct:no problem so long\n",
            "\tPredict:you are welcome\n",
            "Correct:trader_joes is 2_miles away at 408_university_ave there is currently heavy_traffic on the route\n",
            "\tPredict:there is a away\n",
            "Correct:setting navigation now there is heavy_traffic on the way so drive carefully\n",
            "\tPredict:setting navigation to trader_joes now\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:the address to ravenswood_shopping_center is 434_arastradero_rd\n",
            "\tPredict:you re welcome\n",
            "Correct:good day\n",
            "\tPredict:you re welcome\n",
            "R:0.0743,W:53.8017:  76% 77/101 [00:20<00:06,  3.74it/s]Correct:cafe_venetia is 2_miles away at 269_alger_dr\n",
            "\tPredict:there is coffee shop 2_miles away\n",
            "Correct:setting navigation now for the quickest route there is a road_block_nearby so drive carefully\n",
            "\tPredict:i sent the the route to your screen\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:stanford_express_care is nearby\n",
            "\tPredict:the nearest hospital is stanford_childrens_health\n",
            "Correct:does stanford_express_care have heavy_traffic nearby\n",
            "\tPredict:stanford_express_care is located at 214_el_camino_real\n",
            "Correct:the route to stanford_express_care does have heavy_traffic\n",
            "\tPredict:stanford_express_care is located at 214_el_camino_real\n",
            "Correct:yes stanford_childrens_health is 1_miles away with no_traffic do you want the address\n",
            "\tPredict:stanford_express_care is at 899_ames_ct\n",
            "Correct:i m sorry i did and i sent the address on your screen drive carefully\n",
            "\tPredict:you re welcome\n",
            "R:0.0743,W:54.7093:  77% 78/101 [00:20<00:06,  3.75it/s]Correct:there is a safeway 4_miles away\n",
            "\tPredict:there is 2_miles away away away\n",
            "Correct:safeway is located at 452_arcadia_pl with a road_block_nearby\n",
            "\tPredict:there is moderate_traffic on the route is to\n",
            "Correct:i sent the fastest route on your gps drive carefully\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the_westin is 6_miles away\n",
            "\tPredict:there is hotel hotel is the_westin it s 2_miles away\n",
            "Correct:navigating to the_westin\n",
            "\tPredict:the_westin is located at 329_el_camino_real\n",
            "Correct:the address is 329_el_camino_real\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome happy to be of assistance\n",
            "\tPredict:you re welcome\n",
            "R:0.0755,W:55.3684:  78% 79/101 [00:20<00:05,  4.02it/s]Correct:what city are you wanting to know the temperature for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:the lowest_temperature in fresno this week is predicted to be 60f the highest 100f\n",
            "\tPredict:the fresno it will be between 80f - 100f monday and on friday and - - on of - - - - - on of - - - - on of - - - on of - - - on of - - on sunday\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:which city are you wanting to know the weather for\n",
            "\tPredict:what city would you like the weather for\n",
            "Correct:it will be windy tomorrow in seattle\n",
            "\tPredict:it will not be windy in in\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:i am unable to locate starbucks there is a coffee_or_tea_place called coupa miles away\n",
            "\tPredict:there is a starbucks 1_miles away\n",
            "Correct:your next meeting is monday at 3pm\n",
            "\tPredict:your next meeting is at 3pm\n",
            "R:0.0755,W:55.9549:  79% 80/101 [00:21<00:05,  3.76it/s]Correct:what day would you like the forcast for\n",
            "\tPredict:it will not be dry in redwood_city on\n",
            "Correct:it will not be dry in redwood_city next_week\n",
            "\tPredict:it will be dry on saturday in redwood_city\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:monday in fresno will have the highest_temperature of 90f\n",
            "\tPredict:it highest_temperature in fresno on saturday in fresno\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:coupa is 1_miles away with heavy_traffic it shouldn t take us long it s located at 394_van_ness_ave\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:okay i am scheduling a swimming event with tom on the_6th at 2pm\n",
            "\tPredict:okay scheduling a swimming_activity with tom on the_6th of of this month\n",
            "R:0.0767,W:56.3915:  80% 81/101 [00:21<00:05,  3.72it/s]Correct:jills_house is located at 347_alta_mesa_ave\n",
            "\tPredict:the is 8_miles 347_alta_mesa_ave 8_miles away friend you is\n",
            "Correct:certainly setting navigation details now there s a car_collision_nearby so take care on the roads\n",
            "\tPredict:setting navigation to 56_cadwell_street now\n",
            "Correct:midtown_shopping_center is the closest shopping_center to you would you like the address\n",
            "\tPredict:the nearest mall is is midtown_shopping_center\n",
            "Correct:we re going to 338_alester_ave there is heavy_traffic but it s just 2_miles away\n",
            "\tPredict:midtown_shopping_center is at 338_alester_ave\n",
            "Correct:sending the address for the midtown_shopping_center at 338_alester_ave to your gps\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:not a problem\n",
            "\tPredict:you are welcome\n",
            "Correct:i will note down your eye doctor_appointment at 5pm the_12th\n",
            "\tPredict:okay setting eye doctor_appointment on eye eye doctor_appointment on the_12th at 5pm\n",
            "R:0.0780,W:57.0924:  81% 82/101 [00:21<00:05,  3.61it/s]Correct:in what city are you asking\n",
            "\tPredict:what city are you want the temperature for\n",
            "Correct:the temperature in durham will be lowest_temperature low of 70f high of 90f tuesday highest_temperature low of 40f high of 60f thursday\n",
            "\tPredict:what city are you in the the\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is cafe_venetia that s 4_miles away at 269_alger_dr\n",
            "\tPredict:the is a away from away away is you\n",
            "Correct:there is no_traffic on our route i ve set the navigation there\n",
            "\tPredict:setting navigation to cafe_venetia\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:the football game is wednesday at 10am\n",
            "\tPredict:your football game is at at at\n",
            "Correct:my pleasure\n",
            "\tPredict:you are welcome\n",
            "R:0.0804,W:57.6752:  82% 83/101 [00:22<00:04,  3.80it/s]Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:home is listed as 56 caldwell street\n",
            "\tPredict:i have a listing for a chinese_restaurant is the\n",
            "Correct:setting quickest route home now\n",
            "\tPredict:i sent to your screen the route to your screen\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:pizza_chicago is 2_miles away\n",
            "\tPredict:the is is away\n",
            "Correct:the address is 915_arbol_dr would you like directions there\n",
            "\tPredict:there is a road_block_nearby but i sent on your screen the best route\n",
            "Correct:okay heading to pizza_chicago\n",
            "\tPredict:gps set for quickest route to you\n",
            "Correct:the nearest one is midtown_shopping_center but there is a road_block_nearby there is also ravenswood_shopping_center 2_miles farther but with no_traffic on its direction\n",
            "\tPredict:the nearest mall is\n",
            "R:0.0804,W:58.5645:  83% 84/101 [00:22<00:04,  3.89it/s]Correct:setting directions to 434_arastradero_rd\n",
            "\tPredict:midtown_shopping_center is at\n",
            "Correct:you re welcome always happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:we re 5_miles away from starbucks do you like their tea\n",
            "\tPredict:there is tea away\n",
            "Correct:you will find starbucks at 792_bedoin_street\n",
            "\tPredict:the is is is\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:whole_foods is 4_miles away\n",
            "\tPredict:whole_foods is 4_miles away\n",
            "Correct:the quickest route to whole_foods at 819_alma_street does not have any traffic\n",
            "\tPredict:whole_foods is 4_miles away\n",
            "Correct:gps set and have a great day\n",
            "\tPredict:navigation set for navigation\n",
            "R:0.0829,W:59.1386:  84% 85/101 [00:22<00:03,  4.25it/s]Correct:the nearer cafe is cafe_venetia it s 2_miles away and there is a car_collision_nearby\n",
            "\tPredict:the nearest barista is 2_miles away at 145_amherst_st\n",
            "Correct:i m afraid that cafe_venetia is the best option in this moment i sent on your screen the directions to reach there through another route\n",
            "\tPredict:there is a car_collision_nearby but i sent on your screen another route on your screen\n",
            "Correct:you re welcome drive safely\n",
            "\tPredict:you re welcome\n",
            "Correct:coupa is nearby and serves tea\n",
            "\tPredict:there is a safeway 1_miles away\n",
            "Correct:moderate_traffic to coupa\n",
            "\tPredict:coupa is located at 394_van_ness_ave\n",
            "Correct:coupa s address is 394_van_ness_ave\n",
            "\tPredict:coupa is located at 394_van_ness_ave\n",
            "Correct:no but palo_alto_cafe is just 1_miles away and has moderate_traffic so we should be able to get there quicker how does that sound\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:ok initiating navigation\n",
            "\tPredict:you re welcome\n",
            "R:0.0829,W:59.8352:  85% 86/101 [00:22<00:03,  3.79it/s]Correct:what city are you asking about\n",
            "\tPredict:what city would you like the to know the day forecast for\n",
            "Correct:monday will be foggy with low s of 30f and high s of 40f similar heh\n",
            "\tPredict:it will be cloudy on monday in manhattan on monday\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:what city are you asking about\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:tomorrow will not be hot in atherton it will be dry low of 30f high of 50f\n",
            "\tPredict:it will not be hot in atherton\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:what location would you like the weather information for\n",
            "\tPredict:there will be clear_skies in in today today\n",
            "Correct:okay scheduling an appointment for yoga\n",
            "\tPredict:what time and time should i schedule\n",
            "R:0.0829,W:60.6658:  86% 87/101 [00:23<00:03,  3.85it/s]Correct:today in san_jose there will be an overcast co clear_skies\n",
            "\tPredict:there is no be in in in today\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:it will be warm in san_francisco tomorrow it should not be windy\n",
            "\tPredict:it will not be windy in san_francisco tomorrow\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:the closest coffee place is coupa but there is heavy_traffic would you like a different location\n",
            "\tPredict:there is coffee away\n",
            "Correct:starbucks at 792_bedoin_street is 3_miles away with no_traffic\n",
            "\tPredict:the address is coupa\n",
            "Correct:sending you direction now\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0842,W:61.4189:  87% 88/101 [00:23<00:03,  4.05it/s]Correct:what city are you interested in the forecast for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:it will snow in los_angeles tomorrow with a low of 70f and a high of 90f\n",
            "\tPredict:it will be raining on tuesday in los_angeles\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:you have a conference on the_13th at 9am with hr\n",
            "\tPredict:you have a conference on the_13th at 9am\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "Correct:your meeting is scheduled for 2pm tomorrow the agenda is to discuss_the_merger\n",
            "\tPredict:your meeting is at at\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0866,W:61.9217:  88% 89/101 [00:23<00:02,  4.12it/s]Correct:what city can i give you this weather information for\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:snow is predicted to fall in san_jose on thursday this week\n",
            "\tPredict:it will not snow in san_jose this week\n",
            "Correct:you are very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you have four football_activity scheduled one on monday at 6pm with alex one on the_13th at 9am with alex one on wednesday at 1pm with jeff and one on saturday at 5pm with ana\n",
            "\tPredict:you have two football_activity scheduled one on wednesday with jeff and one on on on with at with at\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "Correct:your next appointment with your optometrist is tuesday at 11am\n",
            "\tPredict:your eye eye eye doctor_appointment on tuesday at 10am\n",
            "Correct:reminder set for lab_appointment at 6pm on august 3 with alex\n",
            "\tPredict:okay setting a reminder for your lab_appointment on on august 6pm august with alex\n",
            "Correct:okay scheduling a football_activity with tom for friday at 10am\n",
            "\tPredict:okay setting a reminder for football_activity with tom for friday at\n",
            "R:0.0866,W:62.3356:  89% 90/101 [00:23<00:03,  3.44it/s]Correct:the only nearby hospital is stanford_express_care which is 5_miles away\n",
            "\tPredict:the hospital hospital is 5_miles away at 214_el_camino_real\n",
            "Correct:there is currently a road_block_nearby on the 5_miles route to stanford_express_care\n",
            "\tPredict:stanford_express_care is located at 214_el_camino_real\n",
            "Correct:the only route to stanford is reporting a road block but i am finding you the quickest route to 214_el_camino_real\n",
            "\tPredict:stanford_express_care is at 214_el_camino_real\n",
            "Correct:you re welcome drive safely and take care\n",
            "\tPredict:you re welcome\n",
            "Correct:we are 3_miles away from town and county shopping_center does it sound good to you\n",
            "\tPredict:the closest shopping_center is town_and_country\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "Correct:okay setting reminder for doctorappointment on the tenth would you like me to include a time\n",
            "\tPredict:okay setting a reminder for a doctorappointment on tenth tenth tenth\n",
            "Correct:great i ve set your doctorappointment for the_12th at 1pm\n",
            "\tPredict:you are welcome\n",
            "R:0.0866,W:63.0394:  90% 91/101 [00:24<00:03,  3.31it/s]Correct:what city do you need the 2 day temperature for\n",
            "\tPredict:what city do you want the weather for\n",
            "Correct:in grand_rapids it will be 50f - 70f today and 20f - 40f tomorrow\n",
            "\tPredict:it will be between of of in - - 60f - -\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:yes it is\n",
            "\tPredict:you re welcome\n",
            "Correct:town_and_country is located at 383_university_ave\n",
            "\tPredict:the address is at\n",
            "Correct:have a good day\n",
            "\tPredict:town_and_country is at\n",
            "Correct:coupa is 5_miles away\n",
            "\tPredict:there is a coffee philz 1_miles away\n",
            "Correct:i will pick the shortest route available for you don t worry do you want to get the info on your screen\n",
            "\tPredict:the is is at\n",
            "R:0.0879,W:63.7965:  91% 92/101 [00:24<00:02,  3.59it/s]Correct:what city do you wanna know if it will snow today\n",
            "\tPredict:what city would you like to know if it will snow\n",
            "Correct:today is gonna be warm in oakland\n",
            "\tPredict:it will not snow in oakland oakland today\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:i sent the route on your screen you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your lab_appointment is at 7pm with jon and another one at 2pm with alex\n",
            "\tPredict:you have two lab_appointment scheduled one on the_12th with with and one on at with at\n",
            "Correct:my pleasure\n",
            "\tPredict:you are welcome\n",
            "Correct:your swimming_activity is on the_5th with your father\n",
            "\tPredict:your swimming two swimming_activity scheduled one on the_5th with your father and one on on with at with\n",
            "Correct:happy to help\n",
            "\tPredict:you are welcome\n",
            "R:0.0879,W:64.6173:  92% 93/101 [00:24<00:02,  3.61it/s]Correct:what city do you want to know if it s going to rain or not\n",
            "\tPredict:what city are you want to know the forecast for\n",
            "Correct:i have a few chinese_restaurant listed which one would you like to know about\n",
            "\tPredict:there is a away\n",
            "Correct:p_.f_._changs is 3_miles away but there is currently heavy_traffic on the route panda_express is farther at 4_miles away but there isn t any traffic\n",
            "\tPredict:the is is away\n",
            "Correct:yes panda_express seems to be the fastest\n",
            "\tPredict:you re welcome\n",
            "Correct:all right navigating to panda_express at 842_arrowhead_way\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:your next meeting is with your boss on the_5th at 2pm\n",
            "\tPredict:you next meeting is on the_5th at 1pm\n",
            "Correct:you re welcome\n",
            "\tPredict:you are welcome\n",
            "R:0.0891,W:65.1479:  93% 94/101 [00:24<00:01,  3.88it/s]Correct:there is no rain in fresno today but it will be cloudy\n",
            "\tPredict:you are welcome\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:what city can i give you the weather forecast for\n",
            "\tPredict:what city are you want to know today search today search today\n",
            "Correct:today is gonna be dry on oakland\n",
            "\tPredict:it will not be humid in oakland today\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:i have two which one i have one for the_14th at 6pm and one for the_12th at 7pm\n",
            "\tPredict:you have two swimming_activity scheduled one on on with with and one on on with at with with\n",
            "Correct:anytime\n",
            "\tPredict:you are welcome\n",
            "Correct:okay scheduling a meeting with your boss to go_over_quarterly_report on the_5th at 6pm\n",
            "\tPredict:okay setting a reminder for the_5th the_5th the_5th at 6pm with this month\n",
            "R:0.0903,W:65.8310:  94% 95/101 [00:25<00:01,  3.61it/s]Correct:is there a specific day you would like me to check the weather in grand_rapids for\n",
            "\tPredict:what city are you in\n",
            "Correct:grand_rapids on friday will be windy between 60f - 70f\n",
            "\tPredict:there is no snow in the grand_rapids\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:did you want the current weather\n",
            "\tPredict:what city would you like the weather the new_york weather\n",
            "Correct:today s weather forecast is foggy the low temperature of 40f and high of 60f\n",
            "\tPredict:today current weather in new_york is predicted to be cloudy with low of 20f and high of 40f\n",
            "Correct:not a problem\n",
            "\tPredict:you re welcome\n",
            "Correct:what city would you like weather information about\n",
            "\tPredict:what city do you want the weather for\n",
            "R:0.0916,W:66.4148:  95% 96/101 [00:25<00:01,  3.61it/s]Correct:yes the forecast calls for rain in new_york today\n",
            "\tPredict:it is not raining in new_york today\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "Correct:it is cloudy in redwood_city today\n",
            "\tPredict:it now not snowing in redwood_city right now\n",
            "Correct:i m sorry the only route home has heavy_traffic it will be a 7_miles drive to 10_ames_street\n",
            "\tPredict:the is is away away\n",
            "Correct:you re very welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:what date is your appointment\n",
            "\tPredict:what time and time should i\n",
            "Correct:a reminder has been set for your doctorappointment at 7pm on monday\n",
            "\tPredict:okay i will set your doctorappointment on the_1st at 7pm\n",
            "Correct:no problem\n",
            "\tPredict:you are welcome\n",
            "R:0.0916,W:67.0914:  96% 97/101 [00:25<00:01,  3.83it/s]Correct:no it is not snowing in redwood_city today\n",
            "\tPredict:you re welcome\n",
            "Correct:happy to help\n",
            "\tPredict:you re welcome\n",
            "Correct:would you prefer stanford_shopping_center which is 7_miles away with no_traffic or ravenswood_shopping_center which is 1_miles away with a road_block_nearby\n",
            "\tPredict:the nearest 7_miles away from stanford_shopping_center would you like the address\n",
            "Correct:setting directions to 434_arastradero_rd now\n",
            "\tPredict:i sent the info on your screen drive carefully\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there is moderate_traffic on the 4_miles route home to 56_cadwell_street\n",
            "\tPredict:your home is at 56_cadwell_street\n",
            "Correct:you have two yoga_activity scheduled one on monday at 5pm and another on the_5th at 4pm\n",
            "\tPredict:you have two yoga_activity scheduled one on the_5th with jeff and one on with at with\n",
            "Correct:have a great day\n",
            "\tPredict:you are welcome\n",
            "R:0.0916,W:67.7267:  97% 98/101 [00:26<00:00,  3.53it/s]Correct:you re welcome drive carefully\n",
            "\tPredict:you re welcome\n",
            "Correct:anytime\n",
            "\tPredict:you re welcome\n",
            "Correct:here are directions to cafe_venetia\n",
            "\tPredict:the nearest cafe is trader_joes 6_miles away at 408_university_ave\n",
            "Correct:unfortunately there is heavy_traffic but it s just 2_miles away you should be able to reach it quickly anyway\n",
            "\tPredict:cafe_venetia is at 408_university_ave\n",
            "Correct:269_alger_dr is the location of cafe_venetia\n",
            "\tPredict:the address is cafe_venetia\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:okay a reminder is set for tennis with martha at 10am what is the date for the scheduled tennis event\n",
            "\tPredict:okay setting a tennis reminder for at end at march at at\n",
            "Correct:ok setting reminder for your tennis with martha at 10am for the last week of march\n",
            "\tPredict:you re welcome\n",
            "R:0.0928,W:68.5318:  98% 99/101 [00:26<00:00,  3.69it/s]Correct:for what city are you interested in\n",
            "\tPredict:what city are you want the weather for\n",
            "Correct:the closest is pizza_chicago 5_miles away at 915_arbol_dr would you like directions there\n",
            "\tPredict:the nearest pizza place is palo_alto_cafe and the name is the shell\n",
            "Correct:here is the shortest route to pizza_chicago\n",
            "\tPredict:the address is 53_university_av i sent it on your screen\n",
            "Correct:you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:there s a parking_garage 2_miles away would you like the name and address of it\n",
            "\tPredict:we re 3_miles away from mandarin_roots and the address is 880_ames_ct\n",
            "Correct:navigating to palo_alto_garage_r\n",
            "\tPredict:there is no_traffic on the route to you\n",
            "Correct:i did and i sent the info on your screen drive carefully\n",
            "\tPredict:you re to the webster_garage\n",
            "Correct:you are welcome\n",
            "\tPredict:you re welcome\n",
            "R:0.0941,W:69.1208:  99% 100/101 [00:26<00:00,  3.62it/s]Correct:it will not be windy in cleveland next_week\n",
            "\tPredict:it will not be windy cleveland cleveland cleveland\n",
            "Correct:no problem\n",
            "\tPredict:you re welcome\n",
            "Correct:the nearest grocery_store is located 6_miles away at 452_arcadia_pl\n",
            "\tPredict:the nearest grocery_store is from you\n",
            "Correct:safeway is located at 452_arcadia_pl and navigation is set\n",
            "\tPredict:setting navigation to to now\n",
            "Correct:i did i did you re welcome\n",
            "\tPredict:you re welcome\n",
            "Correct:okay scheduling a conference with the executive_team for the_15th at 9am\n",
            "\tPredict:okay setting a reminder for conference the_15th the_15th of this this month with executive_team\n",
            "Correct:you re welcome friend\n",
            "\tPredict:you are welcome\n",
            "R:0.0941,W:69.7256: 100% 101/101 [00:26<00:00,  3.75it/s]\n",
            "07-08 20:47 F1 SCORE:\t0.3747998529248531\n",
            "07-08 20:47 \tCAL F1:\t0.46128572641177656\n",
            "07-08 20:47 \tWET F1:\t0.49756415900483647\n",
            "07-08 20:47 \tNAV F1:\t0.2573685398329947\n",
            "07-08 20:47 BLEU SCORE:11.84\n",
            "11.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6YFhSqANOYa",
        "colab_type": "code",
        "outputId": "0d51fd47-126f-451e-9791-1e8d2740d71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# !python3 main_test.py -path=save/mem2seq-KVR/HDD128BSZ8DR0.2L1lr0.001Mem2Seq12.03 -dec=Mem2Seq -bsz=8 -ds=kvr\n",
        "ls save"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mHDD128BSZ8DR0.2L1lr0.001Mem2Seq11.92\u001b[0m/  \u001b[01;34mHDD128BSZ8DR0.2L1lr0.001Mem2Seq13.03\u001b[0m/\n",
            "\u001b[01;34mHDD128BSZ8DR0.2L1lr0.001Mem2Seq12.03\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rojx22Qb3epL",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "8691c434-f5db-46aa-a5e2-9fee76ced54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "#@title Wikification\n",
        "import urllib.parse, urllib.request, json\n",
        "\n",
        "def CallWikifier(text, lang=\"en\", threshold=0.8):\n",
        "    # Prepare the URL.\n",
        "    data = urllib.parse.urlencode([\n",
        "        (\"text\", text), (\"lang\", lang),\n",
        "        (\"userKey\", \"zqqxdqnjkbynbisnfsgkuzugboxuzg\"),\n",
        "        (\"pageRankSqThreshold\", \"%g\" % threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
        "        (\"nTopDfValuesToIgnore\", \"200\"), (\"nWordsToIgnoreFromList\", \"200\"),\n",
        "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
        "        (\"support\", \"true\"), (\"ranges\", \"false\"),\n",
        "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\"),\n",
        "        (\"partsOfSpeech\", \"true\")\n",
        "        ])\n",
        "    url = \"http://www.wikifier.org/annotate-article\"\n",
        "    # Call the Wikifier and read the response.\n",
        "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
        "    with urllib.request.urlopen(req, timeout = 60) as f:\n",
        "        response = f.read()\n",
        "        response = json.loads(response.decode(\"utf8\"))\n",
        "    # Output the annotations.\n",
        "    print(*response[\"nouns\"], sep='\\n')\n",
        "    for annotation in response[\"annotations\"]:\n",
        "        print(\"%s (%s)\" % (annotation[\"title\"], annotation[\"url\"]))\n",
        "\n",
        "CallWikifier(\"Syria's foreign minister has said Damascus is ready to offer a prisoner exchange with rebels.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'iFrom': 0, 'iTo': 4, 'normForm': 'syria', 'synsetIds': []}\n",
            "{'iFrom': 8, 'iTo': 23, 'normForm': 'foreign minister', 'synsetIds': ['110103794']}\n",
            "{'iFrom': 16, 'iTo': 23, 'normForm': 'minister', 'synsetIds': ['100585810', '109983572', '110320695', '110320863']}\n",
            "{'iFrom': 34, 'iTo': 41, 'normForm': 'damascus', 'synsetIds': []}\n",
            "{'iFrom': 63, 'iTo': 70, 'normForm': 'prisoner', 'synsetIds': ['110476086']}\n",
            "{'iFrom': 72, 'iTo': 79, 'normForm': 'exchange', 'synsetIds': ['100167063', '100167278', '100196485', '101093085', '101109467', '101166258', '101166517', '102994858', '103302487', '107134706', '111409538']}\n",
            "{'iFrom': 86, 'iTo': 91, 'normForm': 'rebel', 'synsetIds': ['110210137', '110303654']}\n",
            "Syria (http://en.wikipedia.org/wiki/Syria)\n",
            "Damascus (http://en.wikipedia.org/wiki/Damascus)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkFVTa2ivrev",
        "colab_type": "code",
        "outputId": "608557c5-e470-4062-c0ea-bc9bea57c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"7pm or 7pm\")\n",
        "print(doc.count_by, len(doc))\n",
        "for token in doc:\n",
        "  print(token, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in method count_by of spacy.tokens.doc.Doc object at 0x7f0f92fea270> 5\n",
            "7 NUM\n",
            "pm NOUN\n",
            "or CCONJ\n",
            "7 NUM\n",
            "pm NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbWSqdIrCC28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm OpenNMT-py.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Crxjh5MPUSu",
        "colab_type": "text"
      },
      "source": [
        "# OpenNMT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOL44TBTJ41Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../')\n",
        "!rm -rf OpenNMT-py\n",
        "# !git clone https://github.com/kolaSamuel/OpenNMT-py-with-BERT.git\n",
        "# os.chdir('OpenNMT-py')#-with-BERT')\n",
        "# !pip install -r requirements.txt\n",
        "# !git clone --branch OpenNMT https://github.com/kolaSamuel/alexa-dataset-contextual-query-rewrite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhsTP34_SBZ",
        "colab_type": "code",
        "outputId": "83bd0edc-f70d-4ea4-b4af-d3db81af3f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git\n",
        "# !git clone https://github.com/shakeel608/OpenNMT-py-with-BERT.git\n",
        "# !git clone https://github.com/nyu-dl/dl4ir-doc2query\n",
        "# !git clone --branch Glove https://github.com/kolaSamuel/OpenNMT-py.git\n",
        "!cd OpenNMT-py\n",
        "import os\n",
        "os.chdir('OpenNMT-py')\n",
        "# os.chdir('OpenNMT-py-with-BERT')\n",
        "!pip install -r requirements.txt\n",
        "!git clone --branch CQR https://github.com/kolaSamuel/alexa-dataset-contextual-query-rewrite\n",
        "# os.mkdir('opennmt_format')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 14461 (delta 1), reused 8 (delta 0), pack-reused 14442\u001b[K\n",
            "Receiving objects: 100% (14461/14461), 146.07 MiB | 13.42 MiB/s, done.\n",
            "Resolving deltas: 100% (10339/10339), done.\n",
            "Collecting git+https://github.com/pytorch/text.git@master#wheel=torchtext (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/pytorch/text.git (to revision master) to /tmp/pip-req-build-3k3xk21d\n",
            "  Running command git clone -q https://github.com/pytorch/text.git /tmp/pip-req-build-3k3xk21d\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Collecting tqdm==4.30.* (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/4c/103a4d3415dafc1ddfe6a6624333971756e2d3dd8c6dc0f520152855f040/tqdm-4.30.0-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.16.0)\n",
            "Collecting configargparse (from -r requirements.txt (line 6))\n",
            "  Downloading https://files.pythonhosted.org/packages/55/ea/f0ade52790bcd687127a302b26c1663bf2e0f23210d5281dbfcd1dfcda28/ConfigArgParse-0.14.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0->-r requirements.txt (line 4)) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0->-r requirements.txt (line 4)) (1.16.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->-r requirements.txt (line 4)) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->-r requirements.txt (line 4)) (2.8)\n",
            "Building wheels for collected packages: configargparse, torchtext\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-0.14.0-cp36-none-any.whl size=17503 sha256=5acb5a7b90bf70a7ff3443ed5dfc41a5ab4e422fd5c1e292180f26bd9b0dc457\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/9c/ce/7e904dddb8c7595ffbe3409d24455bc5005852850e36011bda\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtext: filename=torchtext-0.4.0-cp36-none-any.whl size=53405 sha256=1573c751cd6582d3e99946bc3e72741e0737ba6fd0e9e09fe447bed2dbc36225\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9qq2atq5/wheels/47/f9/8d/a9e397ec2629a3fd3219b2ebc3ec8b55396fd3cf55963a77a5\n",
            "Successfully built configargparse torchtext\n",
            "Installing collected packages: tqdm, configargparse, torchtext\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed configargparse-0.14.0 torchtext-0.4.0 tqdm-4.30.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'alexa-dataset-contextual-query-rewrite'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 216 (delta 15), reused 0 (delta 0), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (216/216), 5.75 MiB | 4.84 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1fuDbJVTutu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# os.chdir('../')\n",
        "# !mv *.pt OpenNMT-py\n",
        "os.chdir('OpenNMT-py')\n",
        "# os.chdir('OpenNMT-py-with-BERT')\n",
        "# !pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmOjo--2AOCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.chdir('OpenNMT-py')\n",
        "os.mkdir('./msmarco_data')\n",
        "!wget https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz -P ./msmarco_data\n",
        "!tar -xvf ./msmarco_data/collectionandqueries.tar.gz -C ./msmarco_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xolTcUOHL0At",
        "colab_type": "text"
      },
      "source": [
        "Go  <a href='https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md#how-do-i-use-the-transformer-model-do-you-support-multi-gpu'> Here </a>  for instructions on how to change models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyRdHhTwAYmX",
        "colab_type": "code",
        "outputId": "56544865-73a5-4e1a-bf88-fe244ab7e2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!python ./preprocess.py \\\n",
        "  -train_src ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TRAIN_input.txt \\\n",
        "  -train_tgt ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TRAIN_output.txt \\\n",
        "  -valid_src ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_input.txt \\\n",
        "  -valid_tgt ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_output.txt \\\n",
        "  -save_data ./preprocessed \\\n",
        "  -share_vocab \n",
        "# -dynamic_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2019-08-27 18:50:20,726 INFO] Extracting features...\n",
            "[2019-08-27 18:50:20,727 INFO]  * number of source features: 0.\n",
            "[2019-08-27 18:50:20,727 INFO]  * number of target features: 0.\n",
            "[2019-08-27 18:50:20,727 INFO] Building `Fields` object...\n",
            "[2019-08-27 18:50:20,727 INFO] Building & saving training data...\n",
            "[2019-08-27 18:50:20,727 INFO] Reading source and target files: ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TRAIN_input.txt ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TRAIN_output.txt.\n",
            "[2019-08-27 18:50:20,727 INFO] Building shard 0.\n",
            "[2019-08-27 18:50:20,731 INFO]  * saving 0th train data shard to ./preprocessed.train.0.pt.\n",
            "[2019-08-27 18:50:20,753 INFO]  * tgt vocab size: 148.\n",
            "[2019-08-27 18:50:20,753 INFO]  * src vocab size: 303.\n",
            "[2019-08-27 18:50:20,753 INFO]  * merging src and tgt vocab...\n",
            "[2019-08-27 18:50:20,754 INFO]  * merged vocab size: 314.\n",
            "[2019-08-27 18:50:20,755 INFO] Building & saving validation data...\n",
            "[2019-08-27 18:50:20,755 INFO] Reading source and target files: ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_input.txt ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_output.txt.\n",
            "[2019-08-27 18:50:20,755 INFO] Building shard 0.\n",
            "[2019-08-27 18:50:20,756 INFO]  * saving 0th valid data shard to ./preprocessed.valid.0.pt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_MqSe5obAHs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Glove Embeddings\n",
        "!mkdir \"glove_dir\"\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d \"glove_dir\"\n",
        "\n",
        "!tools/embeddings_to_torch.py -emb_file_both \"glove_dir/glove.6B.100d.txt\" \\\n",
        "-dict_file \"preprocessed.vocab.pt\" \\\n",
        "-output_file \"data/embeddings\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzlbIHINOEMJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title RNN model\n",
        "!python -u ./train.py  \\\n",
        "        -data preprocessed \\\n",
        "        -save_model CQR_valid_100\\\n",
        "        -train_steps 20000 \\\n",
        "        -rnn_size 128 \\\n",
        "        -word_vec_size 128 \\\n",
        "        -encoder_type rnn \\\n",
        "        -decoder_type rnn \\\n",
        "        -optim adagrad \\\n",
        "        -learning_rate 0.15 \\\n",
        "        -share_embeddings \\\n",
        "        -valid_steps 100 \\\n",
        "        -save_checkpoint_steps 1000\\\n",
        "        -log_file log.txt\n",
        "#         -pre_word_vecs_enc \"data/embeddings.enc.pt\" \\\n",
        "#         -pre_word_vecs_dec \"data/embeddings.dec.pt\" \\\n",
        "#         -copy_attn \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W6trjJzKmd9",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "d93612f8-21da-46ed-a710-51d2bc596bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Transformer model\n",
        "!python  train.py -data preprocessed -save_model CQR_TREC_split_60_40_lr_15 \\\n",
        "        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\\n",
        "        -encoder_type transformer -decoder_type transformer -position_encoding \\\n",
        "        -train_steps 10000  -max_generator_batches 2 -dropout 0.1 \\\n",
        "        -batch_size 256 -batch_type tokens -normalization tokens  -accum_count 2 \\\n",
        "        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 800 -learning_rate 0.15 \\\n",
        "        -max_grad_norm 0 -param_init 0  -param_init_glorot \\\n",
        "        -label_smoothing 0.1 -valid_steps 100 -save_checkpoint_steps 1000 -log_file log.txt \\\n",
        "        -train_from CQR_step_7000.pt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2019-08-27 18:50:27,531 INFO] Loading checkpoint from CQR_step_7000.pt\n",
            "[2019-08-27 18:50:27,910 INFO] Loading vocab from checkpoint at CQR_step_7000.pt.\n",
            "[2019-08-27 18:50:27,910 INFO]  * src vocab size = 2088\n",
            "[2019-08-27 18:50:27,910 INFO]  * tgt vocab size = 2088\n",
            "[2019-08-27 18:50:27,910 INFO] Building model...\n",
            "[2019-08-27 18:50:28,485 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(2088, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(2088, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax()\n",
            "          (dropout): Dropout(p=0.1)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2088, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[2019-08-27 18:50:28,544 INFO] encoder: 19984384\n",
            "[2019-08-27 18:50:28,544 INFO] decoder: 27365416\n",
            "[2019-08-27 18:50:28,545 INFO] * number of parameters: 47349800\n",
            "[2019-08-27 18:50:28,801 INFO] Starting training on CPU, could be very slow\n",
            "[2019-08-27 18:50:28,801 INFO] Start training loop and validate every 100 steps...\n",
            "[2019-08-27 18:50:28,801 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:50:28,802 INFO] number of examples: 59\n",
            "[2019-08-27 18:50:37,098 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:50:37,099 INFO] number of examples: 59\n",
            "[2019-08-27 18:50:45,315 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:50:45,316 INFO] number of examples: 59\n",
            "[2019-08-27 18:50:53,532 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:50:53,533 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:01,766 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:01,767 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:10,167 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:10,168 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:18,489 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:18,490 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:26,775 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:26,776 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:35,019 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:35,020 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:43,243 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:43,244 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:51,487 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:51,488 INFO] number of examples: 59\n",
            "[2019-08-27 18:51:59,745 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:51:59,746 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:08,040 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:08,041 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:12,325 INFO] Step 7050/10000; acc:  80.42; ppl:  2.61; xent: 0.96; lr: 0.00008; 181/ 48 tok/s;    104 sec\n",
            "[2019-08-27 18:52:16,307 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:16,308 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:24,566 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:24,567 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:32,763 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:32,764 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:40,987 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:40,988 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:49,236 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:49,237 INFO] number of examples: 59\n",
            "[2019-08-27 18:52:57,454 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:52:57,455 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:05,646 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:05,647 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:13,866 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:13,867 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:22,126 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:22,127 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:30,399 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:30,399 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:38,581 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:38,581 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:46,870 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:53:46,871 INFO] number of examples: 59\n",
            "[2019-08-27 18:53:55,152 INFO] Step 7100/10000; acc:  97.95; ppl:  1.15; xent: 0.14; lr: 0.00008; 180/ 48 tok/s;    206 sec\n",
            "[2019-08-27 18:53:55,153 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 18:53:55,154 INFO] number of examples: 66\n",
            "[2019-08-27 18:54:00,360 INFO] Validation perplexity: 12.7795\n",
            "[2019-08-27 18:54:00,361 INFO] Validation accuracy: 58.1651\n",
            "[2019-08-27 18:54:00,361 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:00,362 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:08,608 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:08,609 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:16,707 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:16,708 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:24,982 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:24,983 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:33,129 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:33,130 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:41,288 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:41,289 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:49,506 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:49,507 INFO] number of examples: 59\n",
            "[2019-08-27 18:54:57,634 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:54:57,635 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:05,858 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:05,859 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:14,028 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:14,029 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:22,219 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:22,219 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:30,355 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:30,356 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:38,502 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:38,503 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:42,795 INFO] Step 7150/10000; acc:  99.26; ppl:  1.09; xent: 0.09; lr: 0.00008; 174/ 47 tok/s;    314 sec\n",
            "[2019-08-27 18:55:46,690 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:46,691 INFO] number of examples: 59\n",
            "[2019-08-27 18:55:54,844 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:55:54,845 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:03,092 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:03,093 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:11,460 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:11,460 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:19,753 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:19,754 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:28,059 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:28,060 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:36,320 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:36,321 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:44,620 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:44,621 INFO] number of examples: 59\n",
            "[2019-08-27 18:56:52,869 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:56:52,870 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:01,114 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:01,115 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:09,430 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:09,431 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:17,759 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:17,760 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:26,180 INFO] Step 7200/10000; acc:  99.36; ppl:  1.08; xent: 0.08; lr: 0.00008; 179/ 48 tok/s;    417 sec\n",
            "[2019-08-27 18:57:26,181 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 18:57:26,182 INFO] number of examples: 66\n",
            "[2019-08-27 18:57:31,302 INFO] Validation perplexity: 14.8156\n",
            "[2019-08-27 18:57:31,302 INFO] Validation accuracy: 58.5321\n",
            "[2019-08-27 18:57:31,302 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:31,303 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:39,548 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:39,549 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:47,761 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:47,761 INFO] number of examples: 59\n",
            "[2019-08-27 18:57:56,022 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:57:56,023 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:04,382 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:04,382 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:12,838 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:12,839 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:21,257 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:21,258 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:29,604 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:29,605 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:37,898 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:37,899 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:46,266 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:46,267 INFO] number of examples: 59\n",
            "[2019-08-27 18:58:54,613 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:58:54,614 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:02,952 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:02,953 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:11,344 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:11,345 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:15,691 INFO] Step 7250/10000; acc:  99.56; ppl:  1.06; xent: 0.06; lr: 0.00008; 171/ 46 tok/s;    527 sec\n",
            "[2019-08-27 18:59:19,693 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:19,694 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:28,141 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:28,142 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:36,486 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:36,487 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:44,873 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:44,874 INFO] number of examples: 59\n",
            "[2019-08-27 18:59:53,234 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 18:59:53,235 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:01,581 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:01,582 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:10,016 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:10,017 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:18,368 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:18,369 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:26,769 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:26,770 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:35,176 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:35,176 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:43,536 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:43,537 INFO] number of examples: 59\n",
            "[2019-08-27 19:00:51,895 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:00:51,896 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:00,203 INFO] Step 7300/10000; acc:  99.50; ppl:  1.06; xent: 0.06; lr: 0.00008; 177/ 48 tok/s;    631 sec\n",
            "[2019-08-27 19:01:00,204 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:01:00,205 INFO] number of examples: 66\n",
            "[2019-08-27 19:01:05,340 INFO] Validation perplexity: 15.6716\n",
            "[2019-08-27 19:01:05,340 INFO] Validation accuracy: 57.6147\n",
            "[2019-08-27 19:01:05,340 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:05,341 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:13,583 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:13,584 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:21,841 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:21,842 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:30,070 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:30,070 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:38,234 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:38,235 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:46,451 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:46,452 INFO] number of examples: 59\n",
            "[2019-08-27 19:01:54,719 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:01:54,720 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:03,072 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:03,073 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:11,533 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:11,533 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:20,012 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:20,013 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:28,499 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:28,500 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:36,913 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:36,914 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:45,193 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:45,194 INFO] number of examples: 59\n",
            "[2019-08-27 19:02:49,543 INFO] Step 7350/10000; acc:  99.56; ppl:  1.06; xent: 0.05; lr: 0.00008; 171/ 46 tok/s;    741 sec\n",
            "[2019-08-27 19:02:53,540 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:02:53,541 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:01,858 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:01,859 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:10,207 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:10,207 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:18,519 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:18,520 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:26,974 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:26,975 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:35,467 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:35,468 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:43,932 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:43,932 INFO] number of examples: 59\n",
            "[2019-08-27 19:03:52,332 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:03:52,333 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:00,703 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:00,704 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:09,184 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:09,185 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:17,622 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:17,623 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:26,219 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:26,219 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:34,566 INFO] Step 7400/10000; acc:  99.50; ppl:  1.05; xent: 0.05; lr: 0.00008; 176/ 47 tok/s;    846 sec\n",
            "[2019-08-27 19:04:34,567 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:04:34,568 INFO] number of examples: 66\n",
            "[2019-08-27 19:04:39,680 INFO] Validation perplexity: 15.4488\n",
            "[2019-08-27 19:04:39,680 INFO] Validation accuracy: 58.8991\n",
            "[2019-08-27 19:04:39,680 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:39,681 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:48,025 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:48,026 INFO] number of examples: 59\n",
            "[2019-08-27 19:04:56,398 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:04:56,399 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:04,730 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:04,731 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:13,111 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:13,112 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:21,546 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:21,547 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:29,876 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:29,877 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:38,224 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:38,225 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:46,536 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:46,537 INFO] number of examples: 59\n",
            "[2019-08-27 19:05:54,919 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:05:54,919 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:03,283 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:03,284 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:11,825 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:11,826 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:20,317 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:20,318 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:24,771 INFO] Step 7450/10000; acc:  99.52; ppl:  1.05; xent: 0.05; lr: 0.00008; 170/ 45 tok/s;    956 sec\n",
            "[2019-08-27 19:06:28,776 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:28,776 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:37,237 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:37,238 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:45,729 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:45,730 INFO] number of examples: 59\n",
            "[2019-08-27 19:06:54,186 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:06:54,187 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:02,674 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:02,675 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:11,304 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:11,305 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:19,856 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:19,857 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:28,515 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:28,516 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:36,972 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:36,973 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:45,472 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:45,472 INFO] number of examples: 59\n",
            "[2019-08-27 19:07:53,980 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:07:53,981 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:02,477 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:02,478 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:11,054 INFO] Step 7500/10000; acc:  99.64; ppl:  1.04; xent: 0.04; lr: 0.00008; 174/ 47 tok/s;   1062 sec\n",
            "[2019-08-27 19:08:11,055 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:08:11,056 INFO] number of examples: 66\n",
            "[2019-08-27 19:08:16,168 INFO] Validation perplexity: 15.6511\n",
            "[2019-08-27 19:08:16,168 INFO] Validation accuracy: 58.8991\n",
            "[2019-08-27 19:08:16,168 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:16,169 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:24,607 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:24,608 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:33,025 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:33,026 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:41,485 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:41,485 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:49,938 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:49,939 INFO] number of examples: 59\n",
            "[2019-08-27 19:08:58,346 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:08:58,347 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:06,698 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:06,699 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:15,145 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:15,146 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:23,605 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:23,606 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:32,187 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:32,188 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:40,574 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:40,575 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:49,012 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:49,013 INFO] number of examples: 59\n",
            "[2019-08-27 19:09:57,502 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:09:57,503 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:01,929 INFO] Step 7550/10000; acc:  99.60; ppl:  1.05; xent: 0.04; lr: 0.00008; 169/ 45 tok/s;   1173 sec\n",
            "[2019-08-27 19:10:06,078 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:06,079 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:14,758 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:14,758 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:23,365 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:23,366 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:31,869 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:31,870 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:40,452 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:40,453 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:49,067 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:49,068 INFO] number of examples: 59\n",
            "[2019-08-27 19:10:57,638 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:10:57,638 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:06,241 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:06,242 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:14,873 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:14,874 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:23,573 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:23,574 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:32,134 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:32,135 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:40,746 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:40,746 INFO] number of examples: 59\n",
            "[2019-08-27 19:11:49,434 INFO] Step 7600/10000; acc:  99.66; ppl:  1.04; xent: 0.04; lr: 0.00008; 172/ 46 tok/s;   1281 sec\n",
            "[2019-08-27 19:11:49,435 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:11:49,436 INFO] number of examples: 66\n",
            "[2019-08-27 19:11:54,527 INFO] Validation perplexity: 15.9111\n",
            "[2019-08-27 19:11:54,527 INFO] Validation accuracy: 58.8991\n",
            "[2019-08-27 19:11:54,527 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:11:54,528 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:03,116 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:03,117 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:11,811 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:11,812 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:20,577 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:20,578 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:29,309 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:29,310 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:38,047 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:38,048 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:46,792 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:46,793 INFO] number of examples: 59\n",
            "[2019-08-27 19:12:55,542 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:12:55,543 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:04,233 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:04,234 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:13,082 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:13,083 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:21,944 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:21,945 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:30,700 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:30,701 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:39,483 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:39,484 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:44,047 INFO] Step 7650/10000; acc:  99.68; ppl:  1.04; xent: 0.04; lr: 0.00008; 163/ 44 tok/s;   1395 sec\n",
            "[2019-08-27 19:13:48,346 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:48,347 INFO] number of examples: 59\n",
            "[2019-08-27 19:13:57,227 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:13:57,228 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:06,172 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:06,173 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:15,094 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:15,095 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:24,095 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:24,096 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:33,172 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:33,174 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:42,382 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:42,383 INFO] number of examples: 59\n",
            "[2019-08-27 19:14:51,511 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:14:51,512 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:00,386 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:00,387 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:09,329 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:09,330 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:18,273 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:18,274 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:27,222 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:27,222 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:36,103 INFO] Step 7700/10000; acc:  99.38; ppl:  1.05; xent: 0.05; lr: 0.00008; 165/ 44 tok/s;   1507 sec\n",
            "[2019-08-27 19:15:36,104 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:15:36,105 INFO] number of examples: 66\n",
            "[2019-08-27 19:15:41,292 INFO] Validation perplexity: 14.8803\n",
            "[2019-08-27 19:15:41,292 INFO] Validation accuracy: 58.7156\n",
            "[2019-08-27 19:15:41,292 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:41,293 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:50,118 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:50,119 INFO] number of examples: 59\n",
            "[2019-08-27 19:15:58,924 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:15:58,925 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:07,836 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:07,837 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:16,732 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:16,733 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:25,726 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:25,726 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:34,622 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:34,623 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:43,535 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:43,536 INFO] number of examples: 59\n",
            "[2019-08-27 19:16:52,550 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:16:52,551 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:01,491 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:01,492 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:10,500 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:10,501 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:19,512 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:19,512 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:28,520 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:28,521 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:33,129 INFO] Step 7750/10000; acc:  99.64; ppl:  1.04; xent: 0.04; lr: 0.00008; 160/ 43 tok/s;   1624 sec\n",
            "[2019-08-27 19:17:37,369 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:37,370 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:46,142 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:46,143 INFO] number of examples: 59\n",
            "[2019-08-27 19:17:54,953 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:17:54,954 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:03,786 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:03,787 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:12,715 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:12,715 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:21,684 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:21,685 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:30,759 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:30,760 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:39,759 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:39,760 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:48,748 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:48,749 INFO] number of examples: 59\n",
            "[2019-08-27 19:18:57,712 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:18:57,713 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:06,604 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:06,605 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:15,592 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:15,593 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:24,703 INFO] Step 7800/10000; acc:  99.66; ppl:  1.04; xent: 0.03; lr: 0.00008; 166/ 45 tok/s;   1736 sec\n",
            "[2019-08-27 19:19:24,704 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:19:24,705 INFO] number of examples: 66\n",
            "[2019-08-27 19:19:29,966 INFO] Validation perplexity: 17.093\n",
            "[2019-08-27 19:19:29,966 INFO] Validation accuracy: 57.7982\n",
            "[2019-08-27 19:19:29,966 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:29,967 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:38,797 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:38,798 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:47,770 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:47,771 INFO] number of examples: 59\n",
            "[2019-08-27 19:19:56,935 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:19:56,936 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:06,108 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:06,109 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:15,110 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:15,111 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:24,175 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:24,176 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:33,150 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:33,151 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:42,077 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:42,078 INFO] number of examples: 59\n",
            "[2019-08-27 19:20:51,087 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:20:51,088 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:00,002 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:00,003 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:08,980 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:08,981 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:18,021 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:18,022 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:22,736 INFO] Step 7850/10000; acc:  99.80; ppl:  1.03; xent: 0.03; lr: 0.00007; 158/ 42 tok/s;   1854 sec\n",
            "[2019-08-27 19:21:27,063 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:27,064 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:35,989 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:35,990 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:44,925 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:44,926 INFO] number of examples: 59\n",
            "[2019-08-27 19:21:53,937 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:21:53,938 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:02,948 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:02,949 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:11,952 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:11,953 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:20,948 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:20,949 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:29,912 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:29,913 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:38,882 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:38,883 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:47,854 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:47,855 INFO] number of examples: 59\n",
            "[2019-08-27 19:22:56,822 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:22:56,823 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:05,826 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:05,827 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:14,793 INFO] Step 7900/10000; acc:  99.50; ppl:  1.03; xent: 0.03; lr: 0.00007; 165/ 44 tok/s;   1966 sec\n",
            "[2019-08-27 19:23:14,794 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:23:14,795 INFO] number of examples: 66\n",
            "[2019-08-27 19:23:20,009 INFO] Validation perplexity: 17.6544\n",
            "[2019-08-27 19:23:20,009 INFO] Validation accuracy: 58.3486\n",
            "[2019-08-27 19:23:20,009 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:20,010 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:28,939 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:28,940 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:37,841 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:37,842 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:46,676 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:46,677 INFO] number of examples: 59\n",
            "[2019-08-27 19:23:55,610 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:23:55,611 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:04,560 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:04,561 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:13,513 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:13,513 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:22,460 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:22,461 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:31,487 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:31,488 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:40,346 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:40,347 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:49,248 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:49,249 INFO] number of examples: 59\n",
            "[2019-08-27 19:24:58,248 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:24:58,249 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:07,288 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:07,289 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:11,916 INFO] Step 7950/10000; acc:  99.58; ppl:  1.03; xent: 0.03; lr: 0.00007; 160/ 43 tok/s;   2083 sec\n",
            "[2019-08-27 19:25:16,176 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:16,177 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:25,065 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:25,066 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:34,002 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:34,002 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:42,877 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:42,878 INFO] number of examples: 59\n",
            "[2019-08-27 19:25:51,714 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:25:51,715 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:00,612 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:00,612 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:09,590 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:09,591 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:18,436 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:18,437 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:27,252 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:27,253 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:36,033 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:36,034 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:44,866 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:44,867 INFO] number of examples: 59\n",
            "[2019-08-27 19:26:53,679 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:26:53,680 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:02,486 INFO] Step 8000/10000; acc:  99.70; ppl:  1.03; xent: 0.03; lr: 0.00007; 168/ 45 tok/s;   2194 sec\n",
            "[2019-08-27 19:27:02,487 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:27:02,488 INFO] number of examples: 66\n",
            "[2019-08-27 19:27:07,674 INFO] Validation perplexity: 16.8873\n",
            "[2019-08-27 19:27:07,674 INFO] Validation accuracy: 58.7156\n",
            "[2019-08-27 19:27:07,683 INFO] Saving checkpoint CQR_TREC_split_60_40_lr_15_step_8000.pt\n",
            "[2019-08-27 19:27:09,218 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:09,219 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:18,070 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:18,071 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:26,812 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:26,813 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:35,476 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:35,477 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:44,202 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:44,203 INFO] number of examples: 59\n",
            "[2019-08-27 19:27:52,962 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:27:52,963 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:01,767 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:01,767 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:10,569 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:10,570 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:19,397 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:19,398 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:28,287 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:28,288 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:37,102 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:37,103 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:45,936 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:45,937 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:54,749 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:28:54,750 INFO] number of examples: 59\n",
            "[2019-08-27 19:28:59,185 INFO] Step 8050/10000; acc:  99.66; ppl:  1.03; xent: 0.03; lr: 0.00007; 160/ 43 tok/s;   2310 sec\n",
            "[2019-08-27 19:29:03,304 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:03,304 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:11,730 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:11,731 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:20,045 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:20,046 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:28,603 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:28,604 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:37,015 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:37,016 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:45,315 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:45,316 INFO] number of examples: 59\n",
            "[2019-08-27 19:29:53,718 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:29:53,719 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:02,133 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:02,134 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:10,561 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:10,561 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:19,068 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:19,069 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:27,465 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:27,466 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:35,814 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:35,815 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:44,176 INFO] Step 8100/10000; acc:  99.70; ppl:  1.03; xent: 0.03; lr: 0.00007; 176/ 47 tok/s;   2415 sec\n",
            "[2019-08-27 19:30:44,177 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:30:44,178 INFO] number of examples: 66\n",
            "[2019-08-27 19:30:49,078 INFO] Validation perplexity: 17.4062\n",
            "[2019-08-27 19:30:49,079 INFO] Validation accuracy: 58.1651\n",
            "[2019-08-27 19:30:49,079 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:49,079 INFO] number of examples: 59\n",
            "[2019-08-27 19:30:57,357 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:30:57,358 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:05,611 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:05,612 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:13,878 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:13,879 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:22,231 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:22,232 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:30,521 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:30,522 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:38,749 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:38,750 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:46,977 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:46,978 INFO] number of examples: 59\n",
            "[2019-08-27 19:31:55,303 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:31:55,304 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:03,657 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:03,657 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:12,084 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:12,085 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:20,478 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:20,479 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:28,848 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:28,849 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:33,211 INFO] Step 8150/10000; acc:  99.58; ppl:  1.03; xent: 0.03; lr: 0.00007; 171/ 46 tok/s;   2524 sec\n",
            "[2019-08-27 19:32:37,246 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:37,247 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:45,535 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:45,535 INFO] number of examples: 59\n",
            "[2019-08-27 19:32:53,855 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:32:53,855 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:02,195 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:02,196 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:10,504 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:10,505 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:18,845 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:18,846 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:27,186 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:27,187 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:35,478 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:35,479 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:43,794 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:43,795 INFO] number of examples: 59\n",
            "[2019-08-27 19:33:52,110 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:33:52,111 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:00,429 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:00,429 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:08,832 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:08,833 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:17,215 INFO] Step 8200/10000; acc:  99.80; ppl:  1.02; xent: 0.02; lr: 0.00007; 178/ 48 tok/s;   2628 sec\n",
            "[2019-08-27 19:34:17,216 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:34:17,217 INFO] number of examples: 66\n",
            "[2019-08-27 19:34:22,137 INFO] Validation perplexity: 18.1478\n",
            "[2019-08-27 19:34:22,137 INFO] Validation accuracy: 57.7982\n",
            "[2019-08-27 19:34:22,138 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:22,138 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:30,415 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:30,416 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:38,609 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:38,610 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:46,830 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:46,831 INFO] number of examples: 59\n",
            "[2019-08-27 19:34:55,064 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:34:55,065 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:03,330 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:03,331 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:11,557 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:11,558 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:19,778 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:19,779 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:28,092 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:28,093 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:36,273 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:36,274 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:44,458 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:44,459 INFO] number of examples: 59\n",
            "[2019-08-27 19:35:52,668 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:35:52,669 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:00,961 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:00,962 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:05,304 INFO] Step 8250/10000; acc:  99.72; ppl:  1.02; xent: 0.02; lr: 0.00007; 173/ 46 tok/s;   2737 sec\n",
            "[2019-08-27 19:36:09,349 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:09,350 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:17,718 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:17,718 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:26,012 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:26,013 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:34,319 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:34,320 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:42,602 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:42,602 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:50,886 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:50,887 INFO] number of examples: 59\n",
            "[2019-08-27 19:36:59,165 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:36:59,166 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:07,472 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:07,473 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:15,756 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:15,757 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:24,129 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:24,130 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:32,424 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:32,425 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:40,734 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:40,735 INFO] number of examples: 59\n",
            "[2019-08-27 19:37:49,080 INFO] Step 8300/10000; acc:  99.72; ppl:  1.02; xent: 0.02; lr: 0.00007; 179/ 48 tok/s;   2840 sec\n",
            "[2019-08-27 19:37:49,081 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:37:49,082 INFO] number of examples: 66\n",
            "[2019-08-27 19:37:53,880 INFO] Validation perplexity: 18.2153\n",
            "[2019-08-27 19:37:53,880 INFO] Validation accuracy: 58.1651\n",
            "[2019-08-27 19:37:53,880 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:37:53,881 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:02,091 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:02,092 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:10,294 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:10,295 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:18,450 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:18,451 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:26,653 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:26,654 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:34,864 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:34,865 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:43,067 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:43,068 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:51,306 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:51,307 INFO] number of examples: 59\n",
            "[2019-08-27 19:38:59,508 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:38:59,509 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:07,716 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:07,716 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:15,928 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:15,929 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:24,178 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:24,179 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:32,378 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:32,379 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:36,598 INFO] Step 8350/10000; acc:  99.62; ppl:  1.02; xent: 0.02; lr: 0.00007; 174/ 47 tok/s;   2948 sec\n",
            "[2019-08-27 19:39:40,515 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:40,516 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:48,725 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:48,726 INFO] number of examples: 59\n",
            "[2019-08-27 19:39:56,999 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:39:57,000 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:05,318 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:05,319 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:13,655 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:13,656 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:22,067 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:22,068 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:30,353 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:30,353 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:38,703 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:38,704 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:47,053 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:47,054 INFO] number of examples: 59\n",
            "[2019-08-27 19:40:55,384 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:40:55,385 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:03,660 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:03,661 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:12,007 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:12,008 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:20,312 INFO] Step 8400/10000; acc:  99.66; ppl:  1.02; xent: 0.02; lr: 0.00007; 179/ 48 tok/s;   3052 sec\n",
            "[2019-08-27 19:41:20,313 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:41:20,314 INFO] number of examples: 66\n",
            "[2019-08-27 19:41:25,230 INFO] Validation perplexity: 19.8265\n",
            "[2019-08-27 19:41:25,230 INFO] Validation accuracy: 57.6147\n",
            "[2019-08-27 19:41:25,231 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:25,231 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:33,382 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:33,383 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:41,544 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:41,545 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:49,723 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:49,724 INFO] number of examples: 59\n",
            "[2019-08-27 19:41:58,021 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:41:58,022 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:06,367 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:06,368 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:14,730 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:14,731 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:23,081 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:23,082 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:31,312 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:31,313 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:39,558 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:39,559 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:47,832 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:47,833 INFO] number of examples: 59\n",
            "[2019-08-27 19:42:56,111 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:42:56,112 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:04,429 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:04,430 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:08,790 INFO] Step 8450/10000; acc:  99.56; ppl:  1.03; xent: 0.03; lr: 0.00007; 172/ 46 tok/s;   3160 sec\n",
            "[2019-08-27 19:43:12,752 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:12,753 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:21,059 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:21,060 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:29,300 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:29,301 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:37,526 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:37,527 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:45,962 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:45,963 INFO] number of examples: 59\n",
            "[2019-08-27 19:43:54,231 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:43:54,232 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:02,444 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:02,445 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:10,771 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:10,772 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:19,118 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:19,119 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:27,503 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:27,504 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:35,773 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:35,774 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:44,035 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:44,036 INFO] number of examples: 59\n",
            "[2019-08-27 19:44:52,330 INFO] Step 8500/10000; acc:  99.56; ppl:  1.03; xent: 0.03; lr: 0.00007; 179/ 48 tok/s;   3264 sec\n",
            "[2019-08-27 19:44:52,331 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:44:52,332 INFO] number of examples: 66\n",
            "[2019-08-27 19:44:57,105 INFO] Validation perplexity: 19.2742\n",
            "[2019-08-27 19:44:57,105 INFO] Validation accuracy: 56.8807\n",
            "[2019-08-27 19:44:57,105 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:44:57,106 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:05,272 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:05,273 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:13,500 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:13,501 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:21,772 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:21,773 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:29,955 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:29,956 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:38,152 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:38,153 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:46,433 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:46,434 INFO] number of examples: 59\n",
            "[2019-08-27 19:45:54,647 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:45:54,648 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:02,896 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:02,897 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:11,172 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:11,173 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:19,380 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:19,381 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:27,665 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:27,666 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:35,908 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:35,909 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:40,162 INFO] Step 8550/10000; acc:  99.56; ppl:  1.03; xent: 0.03; lr: 0.00007; 173/ 46 tok/s;   3371 sec\n",
            "[2019-08-27 19:46:44,145 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:44,146 INFO] number of examples: 59\n",
            "[2019-08-27 19:46:52,384 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:46:52,385 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:00,600 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:00,600 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:08,833 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:08,834 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:17,052 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:17,053 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:25,359 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:25,360 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:33,569 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:33,570 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:41,729 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:41,730 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:49,946 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:49,946 INFO] number of examples: 59\n",
            "[2019-08-27 19:47:58,208 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:47:58,208 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:06,401 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:06,402 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:14,661 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:14,662 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:22,944 INFO] Step 8600/10000; acc:  99.70; ppl:  1.02; xent: 0.02; lr: 0.00007; 180/ 48 tok/s;   3474 sec\n",
            "[2019-08-27 19:48:22,945 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:48:22,946 INFO] number of examples: 66\n",
            "[2019-08-27 19:48:27,696 INFO] Validation perplexity: 20.7415\n",
            "[2019-08-27 19:48:27,697 INFO] Validation accuracy: 56.8807\n",
            "[2019-08-27 19:48:27,697 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:27,697 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:35,842 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:35,843 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:43,935 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:43,936 INFO] number of examples: 59\n",
            "[2019-08-27 19:48:51,984 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:48:51,984 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:00,073 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:00,074 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:08,202 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:08,203 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:16,269 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:16,269 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:24,558 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:24,559 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:32,779 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:32,780 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:40,865 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:40,866 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:48,960 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:48,961 INFO] number of examples: 59\n",
            "[2019-08-27 19:49:57,168 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:49:57,169 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:05,345 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:05,346 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:09,601 INFO] Step 8650/10000; acc:  99.54; ppl:  1.03; xent: 0.02; lr: 0.00007; 175/ 47 tok/s;   3581 sec\n",
            "[2019-08-27 19:50:13,577 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:13,578 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:21,834 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:21,835 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:30,053 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:30,054 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:38,200 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:38,201 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:46,354 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:46,354 INFO] number of examples: 59\n",
            "[2019-08-27 19:50:54,661 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:50:54,662 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:02,921 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:02,921 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:11,131 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:11,132 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:19,276 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:19,277 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:27,501 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:27,502 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:35,718 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:35,719 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:43,859 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:43,860 INFO] number of examples: 59\n",
            "[2019-08-27 19:51:52,018 INFO] Step 8700/10000; acc:  99.46; ppl:  1.03; xent: 0.03; lr: 0.00007; 181/ 48 tok/s;   3683 sec\n",
            "[2019-08-27 19:51:52,020 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:51:52,020 INFO] number of examples: 66\n",
            "[2019-08-27 19:51:56,764 INFO] Validation perplexity: 19.8735\n",
            "[2019-08-27 19:51:56,764 INFO] Validation accuracy: 56.5138\n",
            "[2019-08-27 19:51:56,764 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:51:56,765 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:04,865 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:04,866 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:13,054 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:13,055 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:21,258 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:21,259 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:29,340 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:29,341 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:37,403 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:37,403 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:45,501 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:45,502 INFO] number of examples: 59\n",
            "[2019-08-27 19:52:53,655 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:52:53,656 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:01,779 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:01,780 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:09,849 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:09,850 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:17,935 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:17,936 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:26,056 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:26,057 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:34,138 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:34,139 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:38,328 INFO] Step 8750/10000; acc:  99.68; ppl:  1.02; xent: 0.02; lr: 0.00007; 176/ 47 tok/s;   3790 sec\n",
            "[2019-08-27 19:53:42,188 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:42,189 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:50,227 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:50,228 INFO] number of examples: 59\n",
            "[2019-08-27 19:53:58,385 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:53:58,386 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:06,567 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:06,568 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:14,821 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:14,822 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:23,096 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:23,097 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:31,313 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:31,314 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:39,441 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:39,442 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:47,629 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:47,629 INFO] number of examples: 59\n",
            "[2019-08-27 19:54:55,772 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:54:55,773 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:03,903 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:03,903 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:12,131 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:12,132 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:20,310 INFO] Step 8800/10000; acc:  99.64; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   3892 sec\n",
            "[2019-08-27 19:55:20,311 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:55:20,312 INFO] number of examples: 66\n",
            "[2019-08-27 19:55:25,130 INFO] Validation perplexity: 20.4871\n",
            "[2019-08-27 19:55:25,130 INFO] Validation accuracy: 57.2477\n",
            "[2019-08-27 19:55:25,130 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:25,131 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:33,216 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:33,216 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:41,263 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:41,264 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:49,376 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:49,377 INFO] number of examples: 59\n",
            "[2019-08-27 19:55:57,510 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:55:57,511 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:05,761 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:05,762 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:13,962 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:13,963 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:22,151 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:22,151 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:30,325 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:30,326 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:38,417 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:38,418 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:46,541 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:46,541 INFO] number of examples: 59\n",
            "[2019-08-27 19:56:54,734 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:56:54,735 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:02,858 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:02,859 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:07,120 INFO] Step 8850/10000; acc:  99.70; ppl:  1.02; xent: 0.02; lr: 0.00007; 175/ 47 tok/s;   3998 sec\n",
            "[2019-08-27 19:57:11,077 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:11,078 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:19,252 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:19,253 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:27,443 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:27,444 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:35,552 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:35,553 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:43,690 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:43,690 INFO] number of examples: 59\n",
            "[2019-08-27 19:57:51,861 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:57:51,861 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:00,001 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:00,001 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:08,189 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:08,190 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:16,348 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:16,349 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:24,545 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:24,546 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:32,725 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:32,726 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:40,965 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:40,966 INFO] number of examples: 59\n",
            "[2019-08-27 19:58:49,154 INFO] Step 8900/10000; acc:  99.70; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   4100 sec\n",
            "[2019-08-27 19:58:49,155 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 19:58:49,155 INFO] number of examples: 66\n",
            "[2019-08-27 19:58:53,910 INFO] Validation perplexity: 21.8024\n",
            "[2019-08-27 19:58:53,910 INFO] Validation accuracy: 57.0642\n",
            "[2019-08-27 19:58:53,910 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:58:53,911 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:01,981 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:01,982 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:10,029 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:10,030 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:18,110 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:18,110 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:26,336 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:26,337 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:34,477 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:34,478 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:42,533 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:42,534 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:50,603 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:50,604 INFO] number of examples: 59\n",
            "[2019-08-27 19:59:58,736 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 19:59:58,737 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:06,914 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:06,915 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:15,154 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:15,155 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:23,382 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:23,383 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:31,593 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:31,594 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:35,867 INFO] Step 8950/10000; acc:  99.78; ppl:  1.02; xent: 0.01; lr: 0.00007; 175/ 47 tok/s;   4207 sec\n",
            "[2019-08-27 20:00:39,814 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:39,815 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:47,970 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:47,971 INFO] number of examples: 59\n",
            "[2019-08-27 20:00:56,096 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:00:56,096 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:04,259 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:04,259 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:12,677 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:12,678 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:21,083 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:21,084 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:29,363 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:29,364 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:37,591 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:37,592 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:45,852 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:45,853 INFO] number of examples: 59\n",
            "[2019-08-27 20:01:54,081 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:01:54,082 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:02,200 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:02,200 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:10,329 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:10,330 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:18,537 INFO] Step 9000/10000; acc:  99.68; ppl:  1.02; xent: 0.02; lr: 0.00007; 180/ 48 tok/s;   4310 sec\n",
            "[2019-08-27 20:02:18,539 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:02:18,539 INFO] number of examples: 66\n",
            "[2019-08-27 20:02:23,357 INFO] Validation perplexity: 22.5587\n",
            "[2019-08-27 20:02:23,357 INFO] Validation accuracy: 55.9633\n",
            "[2019-08-27 20:02:23,365 INFO] Saving checkpoint CQR_TREC_split_60_40_lr_15_step_9000.pt\n",
            "[2019-08-27 20:02:24,983 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:24,984 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:33,146 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:33,146 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:41,169 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:41,170 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:49,200 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:49,201 INFO] number of examples: 59\n",
            "[2019-08-27 20:02:57,294 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:02:57,295 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:05,395 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:05,396 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:13,481 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:13,481 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:21,551 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:21,552 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:29,669 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:29,670 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:37,773 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:37,774 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:45,858 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:45,859 INFO] number of examples: 59\n",
            "[2019-08-27 20:03:53,966 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:03:53,967 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:02,062 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:02,062 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:06,318 INFO] Step 9050/10000; acc:  99.78; ppl:  1.02; xent: 0.02; lr: 0.00007; 173/ 46 tok/s;   4418 sec\n",
            "[2019-08-27 20:04:10,301 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:10,302 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:18,498 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:18,499 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:26,842 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:26,843 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:35,021 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:35,022 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:43,268 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:43,269 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:51,468 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:51,469 INFO] number of examples: 59\n",
            "[2019-08-27 20:04:59,685 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:04:59,686 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:07,918 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:07,919 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:16,072 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:16,073 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:24,293 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:24,294 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:32,401 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:32,402 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:40,508 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:40,509 INFO] number of examples: 59\n",
            "[2019-08-27 20:05:48,630 INFO] Step 9100/10000; acc:  99.72; ppl:  1.01; xent: 0.01; lr: 0.00007; 181/ 49 tok/s;   4520 sec\n",
            "[2019-08-27 20:05:48,632 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:05:48,632 INFO] number of examples: 66\n",
            "[2019-08-27 20:05:53,375 INFO] Validation perplexity: 23.3724\n",
            "[2019-08-27 20:05:53,375 INFO] Validation accuracy: 56.8807\n",
            "[2019-08-27 20:05:53,375 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:05:53,376 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:01,396 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:01,397 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:09,500 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:09,501 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:17,609 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:17,610 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:25,832 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:25,832 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:33,956 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:33,957 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:42,072 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:42,073 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:50,116 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:50,116 INFO] number of examples: 59\n",
            "[2019-08-27 20:06:58,138 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:06:58,139 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:06,238 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:06,239 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:14,382 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:14,383 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:22,512 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:22,513 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:30,624 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:30,625 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:34,835 INFO] Step 9150/10000; acc:  99.80; ppl:  1.01; xent: 0.01; lr: 0.00007; 176/ 47 tok/s;   4626 sec\n",
            "[2019-08-27 20:07:38,721 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:38,722 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:46,800 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:46,800 INFO] number of examples: 59\n",
            "[2019-08-27 20:07:54,931 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:07:54,932 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:03,062 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:03,062 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:11,230 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:11,231 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:19,444 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:19,445 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:27,666 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:27,667 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:35,851 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:35,852 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:44,012 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:44,013 INFO] number of examples: 59\n",
            "[2019-08-27 20:08:52,207 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:08:52,208 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:00,394 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:00,395 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:08,582 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:08,583 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:16,777 INFO] Step 9200/10000; acc:  99.62; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   4728 sec\n",
            "[2019-08-27 20:09:16,778 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:09:16,779 INFO] number of examples: 66\n",
            "[2019-08-27 20:09:21,515 INFO] Validation perplexity: 23.8096\n",
            "[2019-08-27 20:09:21,515 INFO] Validation accuracy: 56.3303\n",
            "[2019-08-27 20:09:21,516 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:21,516 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:29,734 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:29,735 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:37,804 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:37,804 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:45,948 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:45,949 INFO] number of examples: 59\n",
            "[2019-08-27 20:09:54,125 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:09:54,126 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:02,303 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:02,304 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:10,515 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:10,515 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:18,677 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:18,677 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:26,832 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:26,833 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:35,006 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:35,007 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:43,122 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:43,122 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:51,309 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:51,310 INFO] number of examples: 59\n",
            "[2019-08-27 20:10:59,482 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:10:59,483 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:03,745 INFO] Step 9250/10000; acc:  99.74; ppl:  1.02; xent: 0.02; lr: 0.00007; 175/ 47 tok/s;   4835 sec\n",
            "[2019-08-27 20:11:07,707 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:07,707 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:15,895 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:15,895 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:24,105 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:24,106 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:32,309 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:32,310 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:40,450 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:40,451 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:48,622 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:48,623 INFO] number of examples: 59\n",
            "[2019-08-27 20:11:56,806 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:11:56,807 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:05,008 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:05,008 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:13,220 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:13,221 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:21,409 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:21,410 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:29,550 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:29,551 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:37,738 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:37,739 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:45,910 INFO] Step 9300/10000; acc:  99.44; ppl:  1.02; xent: 0.02; lr: 0.00007; 181/ 49 tok/s;   4937 sec\n",
            "[2019-08-27 20:12:45,911 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:12:45,912 INFO] number of examples: 66\n",
            "[2019-08-27 20:12:50,650 INFO] Validation perplexity: 23.5884\n",
            "[2019-08-27 20:12:50,650 INFO] Validation accuracy: 56.3303\n",
            "[2019-08-27 20:12:50,650 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:50,651 INFO] number of examples: 59\n",
            "[2019-08-27 20:12:58,726 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:12:58,727 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:06,814 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:06,815 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:14,861 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:14,862 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:22,941 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:22,942 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:31,022 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:31,023 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:39,106 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:39,106 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:47,165 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:47,166 INFO] number of examples: 59\n",
            "[2019-08-27 20:13:55,257 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:13:55,258 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:03,420 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:03,421 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:11,624 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:11,625 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:19,828 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:19,829 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:28,218 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:28,219 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:32,469 INFO] Step 9350/10000; acc:  99.62; ppl:  1.02; xent: 0.02; lr: 0.00007; 175/ 47 tok/s;   5044 sec\n",
            "[2019-08-27 20:14:36,387 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:36,388 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:44,516 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:44,517 INFO] number of examples: 59\n",
            "[2019-08-27 20:14:52,785 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:14:52,785 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:00,902 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:00,903 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:09,089 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:09,090 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:17,255 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:17,256 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:25,502 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:25,503 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:33,658 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:33,659 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:41,795 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:41,796 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:49,894 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:49,895 INFO] number of examples: 59\n",
            "[2019-08-27 20:15:58,020 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:15:58,021 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:06,208 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:06,209 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:14,417 INFO] Step 9400/10000; acc:  99.62; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   5146 sec\n",
            "[2019-08-27 20:16:14,418 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:16:14,419 INFO] number of examples: 66\n",
            "[2019-08-27 20:16:19,165 INFO] Validation perplexity: 23.288\n",
            "[2019-08-27 20:16:19,165 INFO] Validation accuracy: 55.7798\n",
            "[2019-08-27 20:16:19,165 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:19,166 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:27,303 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:27,304 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:35,411 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:35,411 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:43,522 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:43,523 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:51,666 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:51,667 INFO] number of examples: 59\n",
            "[2019-08-27 20:16:59,726 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:16:59,727 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:07,847 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:07,848 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:15,932 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:15,933 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:24,085 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:24,086 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:32,179 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:32,180 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:40,254 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:40,255 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:48,324 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:48,325 INFO] number of examples: 59\n",
            "[2019-08-27 20:17:56,465 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:17:56,466 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:00,697 INFO] Step 9450/10000; acc:  99.80; ppl:  1.01; xent: 0.01; lr: 0.00007; 176/ 47 tok/s;   5252 sec\n",
            "[2019-08-27 20:18:04,637 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:04,638 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:12,833 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:12,834 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:21,093 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:21,094 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:29,288 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:29,289 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:37,441 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:37,441 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:45,535 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:45,536 INFO] number of examples: 59\n",
            "[2019-08-27 20:18:53,698 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:18:53,699 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:01,847 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:01,848 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:10,020 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:10,021 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:18,198 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:18,199 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:26,434 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:26,435 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:34,584 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:34,585 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:42,777 INFO] Step 9500/10000; acc:  99.50; ppl:  1.02; xent: 0.02; lr: 0.00007; 181/ 49 tok/s;   5354 sec\n",
            "[2019-08-27 20:19:42,779 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:19:42,780 INFO] number of examples: 66\n",
            "[2019-08-27 20:19:47,493 INFO] Validation perplexity: 21.083\n",
            "[2019-08-27 20:19:47,493 INFO] Validation accuracy: 55.4128\n",
            "[2019-08-27 20:19:47,494 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:47,494 INFO] number of examples: 59\n",
            "[2019-08-27 20:19:55,693 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:19:55,693 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:03,850 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:03,850 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:12,075 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:12,076 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:20,243 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:20,243 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:28,458 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:28,459 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:36,631 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:36,632 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:44,774 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:44,774 INFO] number of examples: 59\n",
            "[2019-08-27 20:20:52,968 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:20:52,969 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:01,092 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:01,093 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:09,276 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:09,277 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:17,415 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:17,416 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:25,632 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:25,633 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:29,887 INFO] Step 9550/10000; acc:  99.60; ppl:  1.02; xent: 0.02; lr: 0.00007; 175/ 47 tok/s;   5461 sec\n",
            "[2019-08-27 20:21:33,795 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:33,796 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:41,969 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:41,970 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:50,108 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:50,109 INFO] number of examples: 59\n",
            "[2019-08-27 20:21:58,351 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:21:58,352 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:06,540 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:06,541 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:14,756 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:14,757 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:22,987 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:22,988 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:31,168 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:31,169 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:39,321 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:39,321 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:47,488 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:47,489 INFO] number of examples: 59\n",
            "[2019-08-27 20:22:55,700 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:22:55,701 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:03,836 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:03,837 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:12,058 INFO] Step 9600/10000; acc:  99.68; ppl:  1.02; xent: 0.02; lr: 0.00007; 181/ 49 tok/s;   5563 sec\n",
            "[2019-08-27 20:23:12,059 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:23:12,060 INFO] number of examples: 66\n",
            "[2019-08-27 20:23:16,765 INFO] Validation perplexity: 22.5589\n",
            "[2019-08-27 20:23:16,765 INFO] Validation accuracy: 56.6972\n",
            "[2019-08-27 20:23:16,765 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:16,766 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:24,893 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:24,894 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:32,970 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:32,971 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:41,061 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:41,062 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:49,132 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:49,133 INFO] number of examples: 59\n",
            "[2019-08-27 20:23:57,259 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:23:57,260 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:05,391 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:05,392 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:13,591 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:13,592 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:21,754 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:21,755 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:29,914 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:29,915 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:38,080 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:38,081 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:46,242 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:46,243 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:54,425 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:24:54,426 INFO] number of examples: 59\n",
            "[2019-08-27 20:24:58,661 INFO] Step 9650/10000; acc:  99.76; ppl:  1.01; xent: 0.01; lr: 0.00007; 175/ 47 tok/s;   5670 sec\n",
            "[2019-08-27 20:25:02,627 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:02,628 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:10,807 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:10,808 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:18,947 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:18,947 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:27,154 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:27,155 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:35,325 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:35,326 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:43,491 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:43,491 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:51,682 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:51,683 INFO] number of examples: 59\n",
            "[2019-08-27 20:25:59,808 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:25:59,808 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:07,937 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:07,938 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:16,130 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:16,131 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:24,343 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:24,344 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:32,511 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:32,512 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:40,689 INFO] Step 9700/10000; acc:  99.68; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   5772 sec\n",
            "[2019-08-27 20:26:40,690 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:26:40,691 INFO] number of examples: 66\n",
            "[2019-08-27 20:26:45,433 INFO] Validation perplexity: 23.0776\n",
            "[2019-08-27 20:26:45,434 INFO] Validation accuracy: 56.6972\n",
            "[2019-08-27 20:26:45,434 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:45,435 INFO] number of examples: 59\n",
            "[2019-08-27 20:26:53,524 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:26:53,525 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:01,623 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:01,623 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:09,809 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:09,810 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:17,969 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:17,971 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:26,096 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:26,097 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:34,154 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:34,155 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:42,160 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:42,161 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:50,206 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:50,207 INFO] number of examples: 59\n",
            "[2019-08-27 20:27:58,324 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:27:58,325 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:06,466 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:06,467 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:14,644 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:14,645 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:22,832 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:22,833 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:27,060 INFO] Step 9750/10000; acc:  99.74; ppl:  1.01; xent: 0.01; lr: 0.00007; 176/ 47 tok/s;   5878 sec\n",
            "[2019-08-27 20:28:30,964 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:30,965 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:39,081 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:39,081 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:47,243 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:47,244 INFO] number of examples: 59\n",
            "[2019-08-27 20:28:55,409 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:28:55,410 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:03,561 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:03,562 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:11,733 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:11,733 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:19,914 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:19,915 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:28,174 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:28,175 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:36,368 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:36,369 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:44,540 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:44,541 INFO] number of examples: 59\n",
            "[2019-08-27 20:29:52,715 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:29:52,716 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:00,852 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:00,853 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:09,076 INFO] Step 9800/10000; acc:  99.76; ppl:  1.01; xent: 0.01; lr: 0.00007; 182/ 49 tok/s;   5980 sec\n",
            "[2019-08-27 20:30:09,077 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:30:09,078 INFO] number of examples: 66\n",
            "[2019-08-27 20:30:13,812 INFO] Validation perplexity: 24.0783\n",
            "[2019-08-27 20:30:13,813 INFO] Validation accuracy: 57.2477\n",
            "[2019-08-27 20:30:13,813 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:13,813 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:21,940 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:21,941 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:29,978 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:29,979 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:38,063 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:38,064 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:46,101 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:46,102 INFO] number of examples: 59\n",
            "[2019-08-27 20:30:54,172 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:30:54,173 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:02,214 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:02,215 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:10,291 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:10,292 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:18,360 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:18,361 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:26,482 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:26,483 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:34,569 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:34,570 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:42,634 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:42,635 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:50,713 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:50,714 INFO] number of examples: 59\n",
            "[2019-08-27 20:31:54,928 INFO] Step 9850/10000; acc:  99.74; ppl:  1.01; xent: 0.01; lr: 0.00007; 177/ 47 tok/s;   6086 sec\n",
            "[2019-08-27 20:31:58,847 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:31:58,848 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:07,035 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:07,036 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:15,290 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:15,291 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:23,555 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:23,555 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:31,733 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:31,734 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:39,887 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:39,888 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:48,084 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:48,085 INFO] number of examples: 59\n",
            "[2019-08-27 20:32:56,247 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:32:56,248 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:04,391 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:04,392 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:12,600 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:12,601 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:20,763 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:20,764 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:28,928 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:28,929 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:37,088 INFO] Step 9900/10000; acc:  99.78; ppl:  1.01; xent: 0.01; lr: 0.00007; 181/ 49 tok/s;   6188 sec\n",
            "[2019-08-27 20:33:37,089 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:33:37,089 INFO] number of examples: 66\n",
            "[2019-08-27 20:33:41,843 INFO] Validation perplexity: 24.6146\n",
            "[2019-08-27 20:33:41,843 INFO] Validation accuracy: 57.4312\n",
            "[2019-08-27 20:33:41,843 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:41,844 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:49,939 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:49,940 INFO] number of examples: 59\n",
            "[2019-08-27 20:33:58,100 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:33:58,101 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:06,269 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:06,270 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:14,479 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:14,480 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:22,628 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:22,629 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:30,808 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:30,808 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:38,924 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:38,925 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:47,058 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:47,059 INFO] number of examples: 59\n",
            "[2019-08-27 20:34:55,253 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:34:55,254 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:03,397 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:03,398 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:11,604 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:11,605 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:19,784 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:19,785 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:24,094 INFO] Step 9950/10000; acc:  99.74; ppl:  1.01; xent: 0.01; lr: 0.00007; 175/ 47 tok/s;   6295 sec\n",
            "[2019-08-27 20:35:28,025 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:28,026 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:36,118 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:36,118 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:44,239 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:44,240 INFO] number of examples: 59\n",
            "[2019-08-27 20:35:52,406 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:35:52,407 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:00,557 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:00,558 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:08,764 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:08,765 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:16,904 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:16,905 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:25,133 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:25,134 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:33,298 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:33,299 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:41,430 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:41,430 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:49,621 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:49,621 INFO] number of examples: 59\n",
            "[2019-08-27 20:36:57,713 INFO] Loading dataset from preprocessed.train.0.pt\n",
            "[2019-08-27 20:36:57,714 INFO] number of examples: 59\n",
            "[2019-08-27 20:37:05,845 INFO] Step 10000/10000; acc:  99.64; ppl:  1.02; xent: 0.02; lr: 0.00007; 182/ 49 tok/s;   6397 sec\n",
            "[2019-08-27 20:37:05,846 INFO] Loading dataset from preprocessed.valid.0.pt\n",
            "[2019-08-27 20:37:05,847 INFO] number of examples: 66\n",
            "[2019-08-27 20:37:10,592 INFO] Validation perplexity: 23.7733\n",
            "[2019-08-27 20:37:10,593 INFO] Validation accuracy: 56.6972\n",
            "[2019-08-27 20:37:10,600 INFO] Saving checkpoint CQR_TREC_split_60_40_lr_15_step_10000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Wn0UZNOQML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # # !zip -r /content/OpenNMT-py.zip /content/OpenNMT-py\n",
        "# # from google.colab import files\n",
        "# # files.download(\"OpenNMT-py.zip\")\n",
        "# import os\n",
        "# os.chdir('OpenNMT-py')\n",
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4GBX0uFf7ZZ",
        "colab_type": "code",
        "outputId": "183a0b20-cbbe-4b3a-d50d-5320a856590f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./translate.py \\\n",
        "                     -gpu 0 \\\n",
        "                     -model ./onmt/CQR_TREC_split_60_40_lr_15_step_8000.pt \\\n",
        "                     -src ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_input.txt \\\n",
        "                     -tgt ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_output.txt \\\n",
        "                     -output ./pred-Entity-8000-TEST.txt \\\n",
        "                     -replace_unk \\\n",
        "                     -verbose \\\n",
        "                     -report_time \\\n",
        "                     -beam_size 1\n",
        "\n",
        "# !python ./translate.py \\\n",
        "#                      -gpu 0 \\\n",
        "#                      -model ./CQR_TREC_step_7500.pt \\\n",
        "#                      -src ./alexa-dataset-contextual-query-rewrite/TREC_TEST_input.txt \\\n",
        "#                      -replace_unk \\\n",
        "#                      -verbose \\\n",
        "#                      -report_time \\\n",
        "#                      -beam_size 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2019-08-29 00:23:41,407 INFO] Translating shard 0.\n",
            "\n",
            "SENT 1: ['<user>', 'tell', 'me', 'about', 'the', 'types', 'of', 'irregular', 'heart', 'beat.']\n",
            "PRED 1: tell me about the types types of of\n",
            "PRED SCORE: -0.1024\n",
            "GOLD 1: tell me about the types of <unk> <unk> <unk>\n",
            "GOLD SCORE: -17.3571\n",
            "\n",
            "SENT 2: ['<user>', 'what', 'are', 'entity_1?', '<system>', 'what', 'are', 'some', 'examples?', '<user>', 'describe', 'the', 'characteristics', 'of', 'entity_2', 'topology.', '<system>', 'what', 'routes', 'can', 'be', 'taken', 'by', 'an', 'upstream', 'packet?', '<user>', 'describe', 'core,', 'aggregator', 'and', 'edge', 'switches.']\n",
            "PRED 2: describe and switches. switches. by entity_1?\n",
            "PRED SCORE: -0.0855\n",
            "GOLD 2: <unk> entity_2 <unk> <unk> and <unk> <unk>\n",
            "GOLD SCORE: -21.7179\n",
            "\n",
            "SENT 3: ['<user>', 'what', 'defines', 'a', 'entity_1?', '<system>', 'how', 'are', 'they', 'related', 'to', 'the', 'periodic', 'table?', '<user>', 'how', 'is', 'it', 'organised?', '<system>', 'how', 'does', 'a', 'chemical', 'compound', 'differ', 'from', 'an', 'element?', '<user>', 'what', 'are', 'different', 'types', 'of', 'chemical', 'compoundss?', '<system>', 'how', 'are', 'they', 'formed?', '<user>', 'which', 'ones', 'are', 'important', 'for', 'the', 'human', 'body?']\n",
            "PRED 3: which are important the ones of for entity_2?\n",
            "PRED SCORE: -0.0744\n",
            "GOLD 3: which entity_1 are <unk> for the <unk> <unk>\n",
            "GOLD SCORE: -18.5410\n",
            "\n",
            "SENT 4: ['<user>', 'what', 'are', 'the', 'best', 'ways', 'to', 'cook', 'a', 'entity_2?', '<system>', 'should', 'i', 'entity_1', 'a', 'entity_2', 'before', 'smoking', 'it?', '<user>', 'how', 'much', 'salt', 'do', 'i', 'use', 'to', 'entity_1', 'it?', '<system>', 'what', 'kind', 'of', 'wood', 'should', 'i', 'use?', '<user>', 'how', 'long', 'should', 'it', 'be', 'smoked', 'for?']\n",
            "PRED 4: how long should entity_1 for?\n",
            "PRED SCORE: -0.0903\n",
            "GOLD 4: how long should a entity_1 entity_2 be <unk> for?\n",
            "GOLD SCORE: -49.2852\n",
            "\n",
            "SENT 5: ['<user>', 'what', 'is', 'entity_1', 'exactly?', '<system>', 'how', 'is', 'it', 'made?', '<user>', 'what', 'are', 'its', 'origins?', '<system>', 'what', 'is', 'the', 'best', 'entity_1?', '<user>', 'is', 'it', 'a', 'kind', 'of', 'cheese?', '<system>', 'can', 'i', 'eat', 'it', 'raw?', '<user>', 'what', 'are', 'common', 'ways', 'to', 'cook', 'it?', '<system>', 'in', 'what', 'cuisines', 'is', 'it', 'popular?', '<user>', 'is', 'entity_1', 'good', 'for', 'you?']\n",
            "PRED 5: what is entity_1 entity_1 in\n",
            "PRED SCORE: -0.1069\n",
            "GOLD 5: is entity_1 good for you?\n",
            "GOLD SCORE: -25.6404\n",
            "\n",
            "SENT 6: ['<user>', 'tell', 'me', 'about', 'entity_1.']\n",
            "PRED 6: tell me about entity_1.\n",
            "PRED SCORE: -0.0902\n",
            "GOLD 6: tell me about entity_1.\n",
            "GOLD SCORE: -0.3510\n",
            "\n",
            "SENT 7: ['<user>', 'what', 'is', 'a', 'normal', 'entity_1?', '<system>', 'what', 'does', 'it', 'mean', 'if', \"it's\", 'higher', 'than', 'this?', '<user>', 'what', 'is', 'a', 'dangerous', 'level?']\n",
            "PRED 7: what this? is a dangerous it's\n",
            "PRED SCORE: -0.1843\n",
            "GOLD 7: what is a <unk> entity_1?\n",
            "GOLD SCORE: -12.0332\n",
            "\n",
            "SENT 8: ['<user>', 'tell', 'me', 'about', 'the', 'international', 'entity_1', 'olympiad.']\n",
            "PRED 8: tell me about the entity_1 entity_1 entity_1\n",
            "PRED SCORE: -0.1213\n",
            "GOLD 8: tell me about the <unk> entity_1 <unk>\n",
            "GOLD SCORE: -6.9536\n",
            "\n",
            "SENT 9: ['<user>', 'what', 'is', 'chemical', 'energy?', '<system>', 'describe', 'a', 'entity_1.', '<user>', 'how', 'is', 'it', 'represented', 'entity_2?', '<system>', 'give', 'me', 'an', 'example.', '<user>', 'what', 'are', 'the', 'different', 'kinds', 'of', 'entity_1?']\n",
            "PRED 9: what are the different of entity_1?\n",
            "PRED SCORE: -0.0895\n",
            "GOLD 9: what are the different <unk> of <unk> <unk>\n",
            "GOLD SCORE: -24.0900\n",
            "\n",
            "SENT 10: ['<user>', 'why', 'is', 'entity_2', 'important?', '<system>', 'why', 'should', 'one', 'study', 'it?', '<user>', 'tell', 'me', 'about', 'its', 'career', 'options.', '<system>', 'are', 'entity_4', 'in', 'high', 'demand?', '<user>', 'how', 'much', 'do', 'they', 'make', 'on', 'average', 'in', 'a', 'year?']\n",
            "PRED 10: how much do entity_1 average in a average\n",
            "PRED SCORE: -0.0851\n",
            "GOLD 10: how much do entity_4 make on average in a <unk>\n",
            "GOLD SCORE: -39.5236\n",
            "\n",
            "SENT 11: ['<user>', 'what', 'comprises', 'a', 'entity_1?', '<system>', 'is', 'it', 'the', 'same', 'for', 'men', 'as', 'well', 'as', 'women?', '<user>', 'what', 'are', 'some', 'specific', 'recommendations', 'for', 'women?']\n",
            "PRED 11: what are the comprises some of entity_1 a\n",
            "PRED SCORE: -0.0936\n",
            "GOLD 11: what are some specific entity_1 <unk> for <unk>\n",
            "GOLD SCORE: -27.9205\n",
            "\n",
            "SENT 12: ['<user>', 'what', 'is', 'the', 'entity_1?', '<system>', 'what', 'era', 'did', 'it', 'happen?', '<user>', 'what', 'genre', 'did', 'it', 'involve?', '<system>', 'what', 'is', 'so', 'special', 'about', 'it?', '<user>', 'which', 'bands', 'were', 'a', 'key', 'part?', '<system>', 'what', 'was', 'the', 'leading', 'band', 'and', 'what', 'was', 'its', 'role?', '<user>', 'is', 'it', 'related', 'to', 'any', 'other', 'musical', 'movements?', '<system>', 'what', 'was', 'its', 'impact', 'on', 'american', 'music?', '<user>', 'what', 'other', 'music', 'movement', 'succeeded', 'it?']\n",
            "PRED 12: what is entity_3 to to to entity_1?\n",
            "PRED SCORE: -0.0957\n",
            "GOLD 12: what other <unk> <unk> <unk> entity_1?\n",
            "GOLD SCORE: -18.1287\n",
            "\n",
            "SENT 13: ['<user>', 'what', 'are', 'the', 'most', 'important', 'us', 'supreme', 'court', 'cases?', '<system>', 'what', 'did', 'plessy', 'v.', 'ferguson', 'establish?', '<user>', 'how', 'about', 'marbury', 'vs', 'madison?', '<system>', 'was', 'it', 'unanimous?', '<user>', 'what', 'was', 'the', 'implication', 'of', 'roe', 'vs', 'wade?']\n",
            "PRED 13: what was the vs of <user>\n",
            "PRED SCORE: -0.1436\n",
            "GOLD 13: what was the <unk> of <unk> <unk> <unk> entity_1?\n",
            "GOLD SCORE: -22.5510\n",
            "\n",
            "SENT 14: ['<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'entity_1?']\n",
            "PRED 14: what are the types of entity_1?\n",
            "PRED SCORE: -0.0945\n",
            "GOLD 14: what are the different types of entity_1?\n",
            "GOLD SCORE: -6.1333\n",
            "\n",
            "SENT 15: ['<user>', 'tell', 'me', 'about', 'entity_1.', '<system>', 'are', 'they', 'really', 'whales?', '<user>', 'how', 'do', 'they', 'hunt?', '<system>', 'what', 'do', 'they', 'eat?', '<user>', 'where', 'did', 'they', 'get', 'their', 'name?']\n",
            "PRED 15: where do entity_1 get\n",
            "PRED SCORE: -0.6536\n",
            "GOLD 15: where did entity_1 get their name?\n",
            "GOLD SCORE: -30.2170\n",
            "\n",
            "SENT 16: ['<user>', 'what', 'are', 'the', 'main', 'breeds', 'of', 'entity_1?', '<system>', 'tell', 'me', 'about', 'boer', 'goats.', '<user>', 'what', 'breed', 'is', 'entity_2?', '<system>', 'are', 'entity_3', 'good', 'for', 'it?', '<user>', 'what', 'about', 'boer', 'goats?', '<system>', 'what', 'are', 'pygmies', 'used', 'for?', '<user>', 'what', 'is', 'the', 'best', 'for', 'fiber', 'production?']\n",
            "PRED 16: what is the best best for is\n",
            "PRED SCORE: -0.1101\n",
            "GOLD 16: what is the best entity_1 for <unk> <unk>\n",
            "GOLD SCORE: -29.1016\n",
            "\n",
            "SENT 17: ['<user>', 'how', 'does', 'entity_3', 'entity_1?', '<system>', 'what', 'happens', 'to', 'its', 'molecules?', '<user>', 'why', 'is', \"n't\", 'the', 'bottom', 'of', 'the', 'ocean', 'frozen?', '<system>', 'can', 'the', 'bottom', 'of', 'the', 'ocean', 'entity_1?', '<user>', 'where', 'is', 'the', 'coldest', 'place', 'on', 'earth?', '<system>', 'does', 'dome', 'fuji', 'have', 'inhabitants?', '<user>', 'why', 'is', 'entity_2', 'so', 'cold?', '<system>', 'how', 'cold', 'is', 'the', 'entity_3', 'underneath', 'the', 'ice?', '<user>', 'what', 'is', 'the', 'biggest', 'town?', '<user>', 'what', 'are', 'entity_2?', '<system>', 'how', 'did', 'this', 'become', 'a', 'new', 'trend?', '<user>', 'why', 'are', 'most', 'of', 'the', 'constructions', 'inspired', 'by', 'these', 'entity_2?']\n",
            "PRED 17: how is the most of entity_2 how\n",
            "PRED SCORE: -0.0919\n",
            "GOLD 17: what is the <unk> <unk> <unk> are most of the <unk> <unk> by entity_2?\n",
            "GOLD SCORE: -70.1752\n",
            "\n",
            "SENT 18: ['<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'entity_1?', '<system>', 'tell', 'me', 'about', 'the', 'characteristics', 'of', 'entity_3.', '<user>', 'what', 'are', 'they', 'composed', 'of?']\n",
            "PRED 18: what are the composed of entity_1?\n",
            "PRED SCORE: -0.0973\n",
            "GOLD 18: what are entity_5 <unk> of?\n",
            "GOLD SCORE: -15.0163\n",
            "\n",
            "SENT 19: ['<user>', 'what', 'comprises', 'a', 'entity_1?', '<system>', 'is', 'it', 'the', 'same', 'for', 'men', 'as', 'well', 'as', 'women?', '<user>', 'what', 'are', 'some', 'specific', 'recommendations', 'for', 'women?', '<system>', 'are', 'entity_2', 'necessary', 'for', 'a', 'good', 'health?', '<user>', 'can', 'it', 'harm', 'you', 'instead?']\n",
            "PRED 19: can you instead? good entity_1\n",
            "PRED SCORE: -0.6458\n",
            "GOLD 19: can entity_2 <unk> you instead?\n",
            "GOLD SCORE: -19.5720\n",
            "\n",
            "SENT 20: ['<user>', 'what', 'is', 'a', 'good', 'age', 'to', 'get', 'entity_1?', '<system>', 'give', 'me', 'a', 'short', 'description', 'of', 'the', 'procedure.', '<user>', 'what', 'are', 'its', 'benefits?', '<system>', 'what', 'are', 'the', 'risks?', '<user>', 'describe', 'experiences', 'of', 'some', 'people', 'who', 'had', 'it', 'done.']\n",
            "PRED 20: describe some different people of entity_1\n",
            "PRED SCORE: -0.5145\n",
            "GOLD 20: <unk> <unk> of some people who <unk> entity_1 done.\n",
            "GOLD SCORE: -55.3479\n",
            "\n",
            "SENT 21: ['<user>', 'tell', 'me', 'about', 'the', 'international', 'entity_1', 'olympiad.', '<system>', 'how', 'do', 'i', 'prepare', 'for', 'it?', '<user>', 'how', 'tough', 'is', 'the', 'exam?', '<system>', 'what', 'kind', 'of', 'problems', 'can', 'i', 'expect?', '<user>', 'tell', 'me', 'about', 'the', 'history', 'of', 'entity_1', 'as', 'a', 'field.', '<system>', 'who', 'was', 'panini', 'and', 'what', 'were', 'his', 'contributions?', '<user>', 'what', 'is', 'the', 'link', 'between', 'entity_1', 'and', 'computer', 'science?']\n",
            "PRED 21: what is the entity_1 of entity_1 me\n",
            "PRED SCORE: -0.0775\n",
            "GOLD 21: what is the <unk> between entity_1 and <unk> <unk>\n",
            "GOLD SCORE: -28.8239\n",
            "\n",
            "SENT 22: ['<user>', 'what', 'are', 'some', 'advantages', 'of', 'using', 'entity_1?', '<system>', 'can', 'i', 'run', 'entity_2', 'software', 'in', 'it?', '<user>', 'how', 'does', 'it', 'compare', 'to', 'entity_2?']\n",
            "PRED 22: how does entity_2 to to to entity_2?\n",
            "PRED SCORE: -0.0968\n",
            "GOLD 22: how does entity_1 <unk> to entity_2?\n",
            "GOLD SCORE: -6.7832\n",
            "\n",
            "SENT 23: ['<user>', 'what', 'are', 'entity_1?', '<system>', 'what', 'are', 'some', 'examples?', '<user>', 'describe', 'the', 'characteristics', 'of', 'entity_2', 'topology.', '<system>', 'what', 'routes', 'can', 'be', 'taken', 'by', 'an', 'upstream', 'packet?', '<user>', 'describe', 'core,', 'aggregator', 'and', 'edge', 'switches.', '<system>', 'are', 'flows', 'bidirectional?', '<user>', 'how', 'many', 'forwarding', 'rules', 'are', 'there', 'in', 'each', 'layer', 'of', 'the', 'entity_2?']\n",
            "PRED 23: how are there any other each in entity_2?\n",
            "PRED SCORE: -0.0887\n",
            "GOLD 23: how many <unk> <unk> are there in each <unk> of the entity_2?\n",
            "GOLD SCORE: -65.2754\n",
            "\n",
            "SENT 24: ['<user>', 'what', 'are', 'the', 'different', 'forms', 'of', 'entity_2?']\n",
            "PRED 24: what are the different types of entity_2?\n",
            "PRED SCORE: -0.0828\n",
            "GOLD 24: what are the different <unk> of entity_2?\n",
            "GOLD SCORE: -7.9708\n",
            "\n",
            "SENT 25: ['<user>', 'what', 'is', 'a', 'normal', 'entity_1?']\n",
            "PRED 25: what is a normal entity_1?\n",
            "PRED SCORE: -0.0843\n",
            "GOLD 25: what is a <unk> entity_1?\n",
            "GOLD SCORE: -2.3075\n",
            "\n",
            "SENT 26: ['<user>', 'what', 'is', 'a', 'entity_5?', '<system>', 'what', 'are', 'entity_4', 'required', 'to', 'become', 'one?', '<user>', 'what', 'does', 'it', 'cost?', '<system>', \"what's\", 'the', 'average', 'entity_1', 'in', 'the', 'uk?', '<user>', 'what', 'about', 'in', 'the', 'us?']\n",
            "PRED 26: what about the us? about in entity_1\n",
            "PRED SCORE: -0.1067\n",
            "GOLD 26: what about the average entity_6 entity_1 the <unk>\n",
            "GOLD SCORE: -37.2246\n",
            "\n",
            "SENT 27: ['<user>', 'what', 'is', 'a', 'entity_5?', '<system>', 'what', 'are', 'entity_4', 'required', 'to', 'become', 'one?', '<user>', 'what', 'does', 'it', 'cost?', '<system>', \"what's\", 'the', 'average', 'entity_1', 'in', 'the', 'uk?', '<user>', 'what', 'about', 'in', 'the', 'us?', '<system>', 'what', 'school', 'subjects', 'are', 'needed', 'to', 'become', 'a', 'entity_3?', '<user>', 'what', 'is', 'the', 'pa', 'average', 'salary', 'vs', 'an', 'entity_3?', '<system>', 'what', 'the', 'difference', 'between', 'a', 'entity_5', 'and', 'a', 'entity_2?', '<user>', 'do', 'nurse', 'practitioners', 'or', 'pas', 'make', 'more?', '<system>', 'is', 'a', 'pa', 'above', 'a', 'entity_2?', '<user>', 'what', 'is', 'the', 'fastest', 'way', 'to', 'become', 'a', 'entity_2?']\n",
            "PRED 27: what is the average average entity_1 an\n",
            "PRED SCORE: -0.1040\n",
            "GOLD 27: what is the fastest way to <unk> a entity_3?\n",
            "GOLD SCORE: -23.4523\n",
            "\n",
            "SENT 28: ['<user>', 'what', 'was', 'the', 'entity_1', 'of', '1933?', '<system>', 'what', 'is', 'exempt', 'from', 'it?', '<user>', 'why', 'was', 'it', 'needed?']\n",
            "PRED 28: why did the entity_1 was of entity_1\n",
            "PRED SCORE: -0.0954\n",
            "GOLD 28: <unk> was the entity_1 of <unk> <unk>\n",
            "GOLD SCORE: -14.6736\n",
            "\n",
            "SENT 29: ['<user>', 'what', 'are', 'entity_2?', '<system>', 'how', 'did', 'this', 'become', 'a', 'new', 'trend?', '<user>', 'why', 'are', 'most', 'of', 'the', 'constructions', 'inspired', 'by', 'these', 'entity_2?', '<system>', 'what', 'are', 'the', 'different', 'domains', 'where', 'such', 'entity_2', 'have', 'been', 'adapted?', '<user>', 'how', 'has', 'it', 'affected', 'technology?', '<system>', 'how', 'have', 'the', 'different', 'giants', 'in', 'the', 'entity_1', 'responded', 'to', 'this?', '<user>', 'has', 'its', 'introduction', 'led', 'to', 'increase', 'in', 'entity_1?']\n",
            "PRED 29: how has entity_2 different the entity_1 <user>\n",
            "PRED SCORE: -1.2192\n",
            "GOLD 29: has entity_2 <unk> <unk> to <unk> in entity_1?\n",
            "GOLD SCORE: -22.8756\n",
            "\n",
            "SENT 30: ['<user>', 'what', 'is', 'entity_1', 'exactly?']\n",
            "PRED 30: what is entity_1 entity_1\n",
            "PRED SCORE: -0.0687\n",
            "GOLD 30: what is entity_1 exactly?\n",
            "GOLD SCORE: -10.2953\n",
            "\n",
            "SENT 31: ['<user>', 'what', 'are', 'the', 'most', 'common', 'entity_1', 'used', 'in', 'cooking?', '<system>', 'how', 'are', 'they', 'different', 'from', 'herbs?', '<user>', 'why', 'do', 'entity_1', 'taste', 'good?', '<system>', 'where', 'do', 'most', 'of', 'them', 'come', 'from?', '<user>', 'what', 'cuisines', 'use', 'them', 'heavily?', '<system>', 'what', 'are', 'the', 'most', 'popular', 'indian', 'ones?', '<user>', 'tell', 'me', 'about', 'turmeric.']\n",
            "PRED 31: tell me about the cold turmeric. from? for entity_2.\n",
            "PRED SCORE: -0.0915\n",
            "GOLD 31: tell me about <unk>\n",
            "GOLD SCORE: -11.0353\n",
            "\n",
            "SENT 32: ['<user>', 'what', 'are', 'the', 'most', 'common', 'entity_1', 'used', 'in', 'cooking?']\n",
            "PRED 32: what are the most most entity_1 most\n",
            "PRED SCORE: -0.1043\n",
            "GOLD 32: what are the most <unk> entity_1 <unk> in <unk>\n",
            "GOLD SCORE: -17.0281\n",
            "\n",
            "SENT 33: ['<user>', 'what', 'is', 'entity_3?', '<system>', 'what', 'are', 'its', 'major', 'types?', '<user>', 'tell', 'me', 'about', 'entity_2.', '<system>', 'how', 'is', 'it', 'different', 'from', 'biological', 'entity_3?', '<user>', 'does', 'it', 'change', 'the', 'composition', 'of', 'rocks?', '<system>', 'what', 'is', 'the', 'entity_1', 'of', 'entity_2?', '<user>', 'give', 'me', 'an', 'example.']\n",
            "PRED 33: give me an example. of entity_1\n",
            "PRED SCORE: -0.3959\n",
            "GOLD 33: give me an <unk> the entity_1 of entity_2?\n",
            "GOLD SCORE: -15.9629\n",
            "\n",
            "SENT 34: ['<user>', 'what', 'are', 'some', 'advantages', 'of', 'using', 'entity_1?']\n",
            "PRED 34: what are some advantages of entity_1?\n",
            "PRED SCORE: -0.0882\n",
            "GOLD 34: what are some <unk> of using entity_1?\n",
            "GOLD SCORE: -10.6423\n",
            "\n",
            "SENT 35: ['<user>', 'describe', 'entity_1.', '<system>', 'what', 'are', 'its', 'important', 'functions?', '<user>', 'tell', 'me', 'about', 'the', 'phospholipid', 'bilayer.', '<system>', 'what', 'are', 'its', 'functions?', '<user>', 'what', 'if', 'the', 'entity_1', 'gets', 'damaged?', '<system>', 'do', 'they', 'repair', 'themselves?', '<user>', 'which', 'nutrients', 'may', 'help', 'in', 'the', 'repair', 'process?']\n",
            "PRED 35: which nutrients the the may may in entity_1?\n",
            "PRED SCORE: -0.0934\n",
            "GOLD 35: which <unk> may help in the <unk> <unk> of entity_1?\n",
            "GOLD SCORE: -32.9674\n",
            "\n",
            "SENT 36: ['<user>', 'what', 'are', 'the', 'best', 'ways', 'to', 'cook', 'a', 'entity_2?']\n",
            "PRED 36: what are the best best to a best\n",
            "PRED SCORE: -0.2612\n",
            "GOLD 36: what are the best <unk> to <unk> a entity_2?\n",
            "GOLD SCORE: -10.9399\n",
            "\n",
            "SENT 37: ['<user>', 'what', 'are', 'the', 'main', 'types', 'of', 'entity_1', 'farming?', '<system>', 'what', 'breeds', 'produce', 'the', 'most', 'milk?', '<user>', 'how', 'much', 'do', 'holsteins', 'produce?', '<system>', 'what', 'is', 'special', 'about', 'jersey', 'milk?', '<user>', 'do', 'we', 'eat', 'dairy', 'cattles?', '<system>', 'what', 'are', 'the', 'most', 'common', 'breeds', 'for', 'meat?', '<user>', 'where', 'are', 'entity_2', 'entity_1', 'from?']\n",
            "PRED 37: where are entity_1 from?\n",
            "PRED SCORE: -0.1235\n",
            "GOLD 37: where are entity_2 entity_1 from?\n",
            "GOLD SCORE: -19.8323\n",
            "\n",
            "SENT 38: ['<user>', 'tell', 'me', 'about', 'the', 'types', 'of', 'irregular', 'heart', 'beat.', '<system>', 'how', 'serious', 'is', 'an', 'irregular', 'heart', 'beat?', '<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'tachycardia?']\n",
            "PRED 38: what are the different types of entity_1?\n",
            "PRED SCORE: -0.0877\n",
            "GOLD 38: what are the different types of <unk>\n",
            "GOLD SCORE: -6.4322\n",
            "\n",
            "SENT 39: ['<user>', 'describe', 'entity_1.', '<system>', 'what', 'are', 'its', 'important', 'functions?', '<user>', 'tell', 'me', 'about', 'the', 'phospholipid', 'bilayer.']\n",
            "PRED 39: tell me about the <system> entity_1. entity_1.\n",
            "PRED SCORE: -0.1249\n",
            "GOLD 39: tell me about the <unk> <unk> in entity_1.\n",
            "GOLD SCORE: -13.5811\n",
            "\n",
            "SENT 40: ['<user>', 'what', 'is', 'a', 'entity_5?', '<system>', 'what', 'are', 'entity_4', 'required', 'to', 'become', 'one?', '<user>', 'what', 'does', 'it', 'cost?', '<system>', \"what's\", 'the', 'average', 'entity_1', 'in', 'the', 'uk?', '<user>', 'what', 'about', 'in', 'the', 'us?', '<system>', 'what', 'school', 'subjects', 'are', 'needed', 'to', 'become', 'a', 'entity_3?', '<user>', 'what', 'is', 'the', 'pa', 'average', 'salary', 'vs', 'an', 'entity_3?']\n",
            "PRED 40: what is the average average entity_1 an\n",
            "PRED SCORE: -0.0895\n",
            "GOLD 40: what is the entity_6 average <unk> <unk> a entity_4?\n",
            "GOLD SCORE: -41.1641\n",
            "\n",
            "SENT 41: ['<user>', 'what', 'is', 'a', 'entity_5?', '<system>', 'what', 'are', 'entity_4', 'required', 'to', 'become', 'one?', '<user>', 'what', 'does', 'it', 'cost?', '<system>', \"what's\", 'the', 'average', 'entity_1', 'in', 'the', 'uk?', '<user>', 'what', 'about', 'in', 'the', 'us?', '<system>', 'what', 'school', 'subjects', 'are', 'needed', 'to', 'become', 'a', 'entity_3?', '<user>', 'what', 'is', 'the', 'pa', 'average', 'salary', 'vs', 'an', 'entity_3?', '<system>', 'what', 'the', 'difference', 'between', 'a', 'entity_5', 'and', 'a', 'entity_2?', '<user>', 'do', 'nurse', 'practitioners', 'or', 'pas', 'make', 'more?']\n",
            "PRED 41: what is the average of entity_1 the\n",
            "PRED SCORE: -0.1011\n",
            "GOLD 41: do <unk> <unk> or <unk> <unk> make more entity_1?\n",
            "GOLD SCORE: -61.7562\n",
            "\n",
            "SENT 42: ['<user>', 'what', 'are', 'the', 'most', 'important', 'us', 'supreme', 'court', 'cases?', '<system>', 'what', 'did', 'plessy', 'v.', 'ferguson', 'establish?', '<user>', 'how', 'about', 'marbury', 'vs', 'madison?']\n",
            "PRED 42: how about most most entity_2?\n",
            "PRED SCORE: -0.0993\n",
            "GOLD 42: how about <unk> <unk> <unk> entity_1?\n",
            "GOLD SCORE: -12.8316\n",
            "\n",
            "SENT 43: ['<user>', 'what', 'is', 'chemical', 'energy?']\n",
            "PRED 43: what is chemical chemical\n",
            "PRED SCORE: -0.1043\n",
            "GOLD 43: what is <unk> <unk>\n",
            "GOLD SCORE: -0.9860\n",
            "\n",
            "SENT 44: ['<user>', 'describe', 'entity_2.', '<system>', 'what', 'makes', 'it', 'so', 'unusual?', '<user>', 'tell', 'me', 'about', 'its', 'entity_3.', '<system>', 'why', 'is', 'it', 'tilted?', '<user>', 'how', 'is', 'its', 'rotation', 'different', 'from', 'other', 'planets?', '<system>', 'what', 'is', 'peculiar', 'about', 'its', 'seasons?', '<user>', 'are', 'there', 'any', 'other', 'planets', 'similar', 'to', 'it?', '<system>', 'describe', 'the', 'characteristics', 'of', 'entity_1.', '<user>', 'why', 'is', 'it', 'important', 'to', 'our', 'solar', 'system?', '<system>', 'how', 'are', 'these', 'two', 'planets', 'similar', 'to', 'each', 'other?', '<user>', 'can', 'life', 'exist', 'on', 'either', 'of', 'them?']\n",
            "PRED 44: are there any other either either to entity_2?\n",
            "PRED SCORE: -0.0888\n",
            "GOLD 44: can <unk> <unk> on entity_2 or entity_1 of them?\n",
            "GOLD SCORE: -62.2671\n",
            "\n",
            "SENT 45: ['<user>', 'what', 'are', 'the', 'best', 'ways', 'to', 'cook', 'a', 'entity_2?', '<system>', 'should', 'i', 'entity_1', 'a', 'entity_2', 'before', 'smoking', 'it?', '<user>', 'how', 'much', 'salt', 'do', 'i', 'use', 'to', 'entity_1', 'it?']\n",
            "PRED 45: how much do entity_1 to entity_1 entity_1\n",
            "PRED SCORE: -0.1428\n",
            "GOLD 45: how much <unk> do i <unk> to entity_1 a entity_2?\n",
            "GOLD SCORE: -30.6507\n",
            "\n",
            "SENT 46: ['<user>', 'what', 'is', 'the', 'entity_1?', '<system>', 'what', 'era', 'did', 'it', 'happen?', '<user>', 'what', 'genre', 'did', 'it', 'involve?']\n",
            "PRED 46: what genre is entity_1 the\n",
            "PRED SCORE: -0.0827\n",
            "GOLD 46: what <unk> did entity_1 <unk>\n",
            "GOLD SCORE: -5.9954\n",
            "\n",
            "SENT 47: ['<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'entity_1?', '<system>', 'tell', 'me', 'about', 'the', 'characteristics', 'of', 'entity_3.', '<user>', 'what', 'are', 'they', 'composed', 'of?', '<system>', 'what', 'is', 'their', 'basic', 'structure?', '<user>', 'what', 'are', 'the', 'main', 'types', 'with', 'examples', 'of', 'each?', '<system>', 'tell', 'me', 'about', 'entity_2.', '<user>', 'what', 'is', 'their', 'function?', '<system>', 'what', 'are', 'the', 'types', 'of', 'entity_2?', '<user>', 'what', 'is', 'the', 'most', 'common?', '<system>', 'what', 'is', 'the', 'difference', 'between', 'them', 'and', 'entity_3?', '<user>', 'why', 'are', 'entity_3', 'better?']\n",
            "PRED 47: what is entity_3 entity_3\n",
            "PRED SCORE: -0.6204\n",
            "GOLD 47: <unk> are entity_5 better than entity_4?\n",
            "GOLD SCORE: -48.5778\n",
            "\n",
            "SENT 48: ['<user>', 'what', 'was', 'the', 'entity_1', 'of', '1933?', '<system>', 'what', 'is', 'exempt', 'from', 'it?', '<user>', 'why', 'was', 'it', 'needed?', '<system>', 'what', 'was', 'the', 'reason', 'for', 'creating', 'the', '1934', 'entity_1?', '<user>', 'what', 'is', 'the', 'entity_2?', '<system>', 'how', 'and', 'when', 'when', 'was', 'it', 'created?', '<user>', 'what', 'was', 'the', 'first', 'stock', 'market?']\n",
            "PRED 48: what was the stock was of entity_1?\n",
            "PRED SCORE: -0.0824\n",
            "GOLD 48: what was the first <unk> <unk>\n",
            "GOLD SCORE: -16.4700\n",
            "\n",
            "SENT 49: ['<user>', 'what', 'is', 'a', 'good', 'age', 'to', 'get', 'entity_1?']\n",
            "PRED 49: what is a good good to entity_1?\n",
            "PRED SCORE: -0.0899\n",
            "GOLD 49: what is a good <unk> to get entity_1?\n",
            "GOLD SCORE: -8.6406\n",
            "\n",
            "SENT 50: ['<user>', 'what', 'are', 'entity_1?']\n",
            "PRED 50: what are entity_1?\n",
            "PRED SCORE: -0.1039\n",
            "GOLD 50: what are entity_1?\n",
            "GOLD SCORE: -0.4010\n",
            "\n",
            "SENT 51: ['<user>', 'what', 'are', 'some', 'advantages', 'of', 'using', 'entity_1?', '<system>', 'can', 'i', 'run', 'entity_2', 'software', 'in', 'it?', '<user>', 'how', 'does', 'it', 'compare', 'to', 'entity_2?', '<system>', 'which', 'of', 'these', 'is', 'more', 'popular?', '<user>', 'how', 'do', 'i', 'install', 'software', 'on', 'it?']\n",
            "PRED 51: how do some entity_1 on\n",
            "PRED SCORE: -0.1200\n",
            "GOLD 51: how do i <unk> <unk> on entity_1?\n",
            "GOLD SCORE: -14.0243\n",
            "\n",
            "SENT 52: ['<user>', 'tell', 'me', 'about', 'the', 'types', 'of', 'irregular', 'heart', 'beat.', '<system>', 'how', 'serious', 'is', 'an', 'irregular', 'heart', 'beat?', '<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'tachycardia?', '<system>', 'what', 'is', 'entity_1?', '<user>', 'what', 'causes', 'it?', '<system>', \"what's\", 'the', 'difference', 'between', 'entity_1', 'and', 'fibrillation?', '<user>', 'how', 'does', 'entity_1', 'affect', 'an', 'ecg?']\n",
            "PRED 52: how does entity_1 affect\n",
            "PRED SCORE: -1.2183\n",
            "GOLD 52: how does entity_2 <unk> an <unk>\n",
            "GOLD SCORE: -10.4594\n",
            "\n",
            "SENT 53: ['<user>', 'what', 'is', 'entity_1', 'exactly?', '<system>', 'how', 'is', 'it', 'made?', '<user>', 'what', 'are', 'its', 'origins?']\n",
            "PRED 53: what are entity_1 entity_1\n",
            "PRED SCORE: -0.0741\n",
            "GOLD 53: what are <unk> <unk>\n",
            "GOLD SCORE: -11.5934\n",
            "\n",
            "SENT 54: ['<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'entity_1?', '<system>', 'tell', 'me', 'about', 'the', 'characteristics', 'of', 'entity_3.', '<user>', 'what', 'are', 'they', 'composed', 'of?', '<system>', 'what', 'is', 'their', 'basic', 'structure?', '<user>', 'what', 'are', 'the', 'main', 'types', 'with', 'examples', 'of', 'each?', '<system>', 'tell', 'me', 'about', 'entity_2.', '<user>', 'what', 'is', 'their', 'function?', '<system>', 'what', 'are', 'the', 'types', 'of', 'entity_2?', '<user>', 'what', 'is', 'the', 'most', 'common?']\n",
            "PRED 54: what is entity_3 function?\n",
            "PRED SCORE: -0.5467\n",
            "GOLD 54: what is the most <unk> entity_2 of entity_3?\n",
            "GOLD SCORE: -38.3372\n",
            "\n",
            "SENT 55: ['<user>', 'what', 'are', 'the', 'most', 'important', 'us', 'supreme', 'court', 'cases?', '<system>', 'what', 'did', 'plessy', 'v.', 'ferguson', 'establish?', '<user>', 'how', 'about', 'marbury', 'vs', 'madison?', '<system>', 'was', 'it', 'unanimous?', '<user>', 'what', 'was', 'the', 'implication', 'of', 'roe', 'vs', 'wade?', '<system>', 'what', 'were', 'the', 'main', 'arguments?', '<user>', 'what', 'was', 'the', 'point', 'of', 'brown', 'v', 'board', 'of', 'ed?']\n",
            "PRED 55: what was the point of entity_1?\n",
            "PRED SCORE: -0.0791\n",
            "GOLD 55: what was the point of <unk> <unk> <unk> of <unk> entity_1?\n",
            "GOLD SCORE: -50.3967\n",
            "\n",
            "SENT 56: ['<user>', 'tell', 'me', 'about', 'the', 'international', 'entity_1', 'olympiad.', '<system>', 'how', 'do', 'i', 'prepare', 'for', 'it?', '<user>', 'how', 'tough', 'is', 'the', 'exam?']\n",
            "PRED 56: how is the entity_2?\n",
            "PRED SCORE: -0.0894\n",
            "GOLD 56: how <unk> is the <unk> entity_1 <unk> <unk>\n",
            "GOLD SCORE: -18.4014\n",
            "\n",
            "SENT 57: ['<user>', 'what', 'is', 'entity_3?']\n",
            "PRED 57: what is entity_3?\n",
            "PRED SCORE: -0.0929\n",
            "GOLD 57: what is entity_3?\n",
            "GOLD SCORE: -0.8833\n",
            "\n",
            "SENT 58: ['<user>', 'what', 'is', 'the', 'entity_1?', '<system>', 'what', 'era', 'did', 'it', 'happen?', '<user>', 'what', 'genre', 'did', 'it', 'involve?', '<system>', 'what', 'is', 'so', 'special', 'about', 'it?', '<user>', 'which', 'bands', 'were', 'a', 'key', 'part?', '<system>', 'what', 'was', 'the', 'leading', 'band', 'and', 'what', 'was', 'its', 'role?', '<user>', 'is', 'it', 'related', 'to', 'any', 'other', 'musical', 'movements?']\n",
            "PRED 58: is entity_2 was some any to to entity_3 entity_1.\n",
            "PRED SCORE: -0.1034\n",
            "GOLD 58: is entity_1 <unk> to any other <unk> <unk>\n",
            "GOLD SCORE: -27.1101\n",
            "\n",
            "SENT 59: ['<user>', 'what', 'are', 'the', 'different', 'types', 'of', 'entity_1?', '<system>', 'tell', 'me', 'about', 'the', 'characteristics', 'of', 'entity_3.', '<user>', 'what', 'are', 'they', 'composed', 'of?', '<system>', 'what', 'is', 'their', 'basic', 'structure?', '<user>', 'what', 'are', 'the', 'main', 'types', 'with', 'examples', 'of', 'each?']\n",
            "PRED 59: what are the main types of entity_1?\n",
            "PRED SCORE: -0.0912\n",
            "GOLD 59: what are the <unk> types of entity_1 entity_5 with <unk> of <unk>\n",
            "GOLD SCORE: -41.1394\n",
            "\n",
            "SENT 60: ['<user>', 'describe', 'entity_2.', '<system>', 'what', 'are', 'its', 'dangers?', '<user>', 'are', 'there', 'any', 'methods', 'to', 'prevent', 'it', 'getting', 'worse?', '<system>', 'what', 'can', 'i', 'do', 'at', 'an', 'individual', 'level?', '<user>', 'how', 'do', 'i', 'reduce', 'my', 'entity_1?']\n",
            "PRED 60: how do entity_1 reduce\n",
            "PRED SCORE: -1.3066\n",
            "GOLD 60: how do i <unk> my entity_1?\n",
            "GOLD SCORE: -9.3197\n",
            "Traceback (most recent call last):\n",
            "  File \"./translate.py\", line 49, in <module>\n",
            "    main(opt)\n",
            "  File \"./translate.py\", line 33, in main\n",
            "    attn_debug=opt.attn_debug\n",
            "  File \"/content/OpenNMT-py/onmt/translate/translator.py\", line 353, in translate\n",
            "    translations = xlation_builder.from_batch(batch_data)\n",
            "  File \"/content/OpenNMT-py/onmt/translate/translation.py\", line 95, in from_batch\n",
            "    for n in range(self.n_best)]\n",
            "  File \"/content/OpenNMT-py/onmt/translate/translation.py\", line 95, in <listcomp>\n",
            "    for n in range(self.n_best)]\n",
            "  File \"/content/OpenNMT-py/onmt/translate/translation.py\", line 50, in _build_target_tokens\n",
            "    _, max_index = attn[i][:len(src_raw)].max(0)\n",
            "RuntimeError: cannot perform reduction function max on tensor with no elements because the operation does not have an identity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-GZXSW-Ohqa",
        "colab_type": "code",
        "outputId": "d6e382b7-7d88-49d5-f9b8-2bef4c19801b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!perl ./tools/multi-bleu.perl \\\n",
        "    ./alexa-dataset-contextual-query-rewrite/TREC++_Entity_index_TEST_output.txt < pred-simple-8000-TEST.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU = 13.44, 34.6/16.8/10.3/5.4 (BP=1.000, ratio=1.289, hyp_len=566, ref_len=439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqIIEmJYm2m0",
        "colab_type": "code",
        "outputId": "8fb83089-42bd-4c2a-8c50-c221aeae0777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "from string import ascii_letters\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.random.random((10,10)));\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAD8CAYAAAA11GIZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE2BJREFUeJzt3XuQnXV9x/H3hw0hEO4Gb0mUVKI2\nYlvoiggdb4AN6sAMthQsTmUYqW1BbpXBXrDSf+ql1MvES4roDKCURkZTmhKUy3glJlxqTQI1BkrC\nRRIukXuyu5/+cTZ2ScmeZ83z7PmdfT6vmWfmnLPPfs+XJfvd3+/7/J7fkW0iIkqzW68TiIh4ISlO\nEVGkFKeIKFKKU0QUKcUpIoqU4hQRRUpxiogipThFRJFSnCKiSNOaCDp92l7ec/r+tcfdus9A7TEB\nfuMlv6g95gDNrLzfNLx3I3GfHW7knwIzp21tJO5jD+1bf9ADh+qP2ZDnfrGFoS1Pa1di/P7bZvqR\nR4crnXvbT55bbnvhrrzfRDXyL3LP6ftz5CFn1B73gWMPrD0mwNXnfqr2mPvsNlJ7TIDPP3JUI3HX\nPXVQI3HfsP+9jcRd8vF31B5Tp26qPWZTVn/oq7sc45FHh/nx8ldUOnfgZT+btctvOEHN/LmMiOIZ\nGKGZP6J1SHGKaCljtrnatK4XUpwiWiwjp4gojjHDBW+ZlOIU0WIjDV1VrkOKU0RLGRguuDhVWoQp\naaGkuyWtk3RR00lFxOQYwZWOXug6cpI0ACwCjgM2AislLbW9punkIqI5BrYV3HOqMnI6Alhne73t\nrcDVwInNphURTTNmuOLRC1V6TrOBDWOebwTeuONJks4EzgSYsXsDtxZERL0Mw+UOnOq78df2YtuD\ntgenT5tZV9iIaEhnhXi1oxeqjJzuB+aOeT5n9LWI6GtimF26d7hRVYrTSmC+pHl0itIpwHsbzSoi\nGtdpiPdxcbI9JOksYDkwAFxue3XjmUVEozrrnPq4OAHYXgYsaziXiJhkI/08coqIqWlKjJwiYuox\nYrjgnbpTnCJaLNO6iCiOEVvdzL78dUhximipziLMlk3rXjX/Eb55/RW1x33b+WfXHhPguidfX3vM\nG09v5oMIrv9W/T9XgHe8508aiXvukhsaiXvN7vV/wMGMy15Ue0yAZw6ovwD4sXp+ddMQj4ji2GLY\nLRs5RUR/GMnIKSJK02mIl1sCys0sIhrVyoZ4RPSH4axziojSZIV4RBRrJFfrIqI0nRt/U5wiojBG\nbMvtKxFRGpsswoyIEimLMCOiPCYjp4goVBriEVEco2w2FxHl6Xw0VLkloNzMIqJh/f+hmhExBZmy\nV4iXm1lENG54dPTU7ehG0kJJd0taJ+miF/j6KyTdLOkOST+R9M5uMTNyimgpW7WMnCQNAIuA44CN\nwEpJS22vGXPa3wDX2P6CpAV0PqT34PHipjhFtFSnIV7L7StHAOtsrweQdDVwIjC2OBnYd/TxfsAD\n3YKmOEW01oT2EJ8ladWY54ttLx59PBvYMOZrG4E37vD9fwfcIOlsYCZwbLc3bKQ4bR7eg8WPH1J7\n3IfetbX2mACf//4xtcf886/cWHtMgKPO+2AjcfdddXsjcf/o5wsbiTu0Z/1XmT73t5+tPSbAD56p\n/3fhkz/asssxOg3xyj/HzbYHd+HtTgW+avsfJb0JuELSobZHdvYNGTlFtFhNK8TvB+aOeT5n9LWx\nzgAWAtj+kaQZwCzg4Z0FzdW6iJbavkK8ytHFSmC+pHmSpgOnAEt3OOc+4BgASb8JzAA2jRc0I6eI\nFqvjAw5sD0k6C1gODACX214t6RJgle2lwAXAP0s6j86M8v22PV7cFKeIlrJh20g9kyfby+gsDxj7\n2sVjHq8Bjp5IzBSniJbqTOvK7eykOEW0WMn31nUtm5Lmji47XyNptaRzJiOxiGjW9qUENTTEG1Fl\n5DQEXGD7dkn7ALdJ+vYOS9Mjou/0+bTO9oPAg6OPn5C0ls6K0BSniD43ZfYQl3QwcBiwoolkImLy\ndK7WTYGPhpK0N/AN4Fzbv3yBr58JnAlwwMtm1JZgRDSj9G16K004Je1OpzBdZfvaFzrH9mLbg7YH\n9z5w9zpzjIiGjIx+PFS3oxe6jpwkCfgysNb2pc2nFBGTYYI3/k66KiOno4H3AW+XdOfo0XUXu4go\n34h3q3T0QpWrdd+Hglv6EfFrscVQPy8liIipq+RpXYpTREuV3nNKcYposRSniChO6eucUpwiWmzK\n3L5S1RNDe/C9x+rf1P3Hb/tc7TEBjrjp7Npj3nzy79YeE+CH3/liI3E/cP6E9gGr7IGTmlmQO3jt\nf9Ye8z3fambDjdf+wz21x9yy+ae7HMOGoZo2m2tCRk4RLZZpXUQUJz2niCiWU5wiokSta4hHRPns\n9JwiokhiOFfrIqJE6TlFRHFyb11ElMmdvlOpUpwiWixX6yKiOE5DPCJKlWldRBQpV+siojh2ilNE\nFCpLCSKiSOk5RURxjBgp+GpduZlFRONc8ehG0kJJd0taJ+minZxzsqQ1klZL+lq3mBk5RbRVTQ1x\nSQPAIuA4YCOwUtJS22vGnDMf+AhwtO3HJL24W9yMnCLarJ6h0xHAOtvrbW8FrgZO3OGcDwCLbD8G\nYPvhbkFTnCJazFalo4vZwIYxzzeOvjbWq4FXS/qBpFslLewWtJFp3dbhady75cDa4775ix+uPSbA\nwSueqz3mXRfuU3tMgDfcfnIjcYeGm/k7teWi/RqJu/bWV9Qec8amZn4G957+qtpjbr18j12OYWBk\npPK0bpakVWOeL7a9eAJvNw2YD7wVmAN8V9LrbT8+3jdERBsZqN5z2mx7cCdfux+YO+b5nNHXxtoI\nrLC9DbhH0n/TKVYrd/aGmdZFtJhd7ehiJTBf0jxJ04FTgKU7nPNNOqMmJM2iM81bP17QFKeINquh\nIW57CDgLWA6sBa6xvVrSJZJOGD1tOfCIpDXAzcCHbT8yXtxM6yJaq1KzuxLby4BlO7x28ZjHBs4f\nPSpJcYpos9y+EhHFMbj61bpJl+IU0WrlFqfKDXFJA5LukHRdkwlFxCSq6+a6Bkzkat05dDrxETFV\n9HtxkjQHeBdwWbPpRMSk2b4Is8rRA1V7Tp8GLgR2ek+GpDOBMwGmv3jfXc8sIhpX8mZzXUdOkt4N\nPGz7tvHOs73Y9qDtwWn77lVbghHRoBFVO3qgysjpaOAESe8EZgD7SrrS9mnNphYRTVM/j5xsf8T2\nHNsH07ln5qYUpogpoGozvEcFLOucIlqrd83uKiZUnGzfAtzSSCYRMfkKntZl5BTRZiO9TmDnUpwi\n2mpim81NuhSniBYr+WpdilNEmxVcnLITZkQUqZGR026/GGDvf6r/FpaD1t5be0yAf1lxbe0xTz7u\nfbXHBHjy00ONxD30JQ82EveHNx7eSNyRtz9We8xnn96/9pgAL/9e/f/PNj5Tz5An07qIKI/p2a0p\nVaQ4RbRZRk4RUaJM6yKiTClOEVGkFKeIKI2caV1ElCpX6yKiRBk5RUSZUpwiojjpOUVEsVKcIqJE\nKnizuexKEBFFysgpos0yrYuI4qQhHhHFSnGKiCIVXJzSEI9oKdG5Wlfl6BpLWijpbknrJF00znnv\nkWRJg91ipjhFtJX/7+bfbsd4JA0Ai4DjgQXAqZIWvMB5+wDnACuqpJfiFNFmrniM7whgne31trcC\nVwMnvsB5fw98HHi2SmopThFtVk9xmg1sGPN84+hrvyLpcGCu7X+vmlojDXE98TS7f+e22uPedeVh\ntccEGG6gK/jUIQfUHhNgw4ZmlvSeNOeORuK+7oP/1kjcp0em1x7zxo8dWXtMgMdft0/tMUcG6tnq\nZAJLCWZJWjXm+WLbiyu9h7QbcCnw/onklqt1EW1WvThttr2zJvb9wNwxz+eMvrbdPsChwC2SAF4K\nLJV0gu2xBe95Upwi2sq13Vu3EpgvaR6donQK8N5fvY29BZi1/bmkW4C/HK8wQXpOEe1WQ8/J9hBw\nFrAcWAtcY3u1pEsknfDrppaRU0SL1XX7iu1lwLIdXrt4J+e+tUrMFKeINit4hXiKU0RbVVsm0DOV\nek6S9pe0RNJdktZKelPTiUVEs0Q9K8SbUnXk9Bngett/IGk6sFeDOUXEJOnrLVMk7Qe8mdEFVKPL\n07c2m1ZETIqCi1OVad08YBPwFUl3SLpM0syG84qIyVDP7SuNqFKcpgGHA1+wfRjwFPD/tkSQdKak\nVZJWbeO5mtOMiNrVtCtBU6oUp43ARtvbtzlYQqdYPY/txbYHbQ/uzh515hgRTennkZPth4ANkl4z\n+tIxwJpGs4qISVHXZnNNqHq17mzgqtErdeuB05tLKSImS19frQOwfSfQdVvNiOgjhS/CzArxiDZL\ncYqI0mxfIV6qFKeIFtNIudUpxSmirdJziohSZVoXEWVqW3Ga8/on+eR1t9Ye95XTflR7TIDfW3lG\n7TGfe28zt/DokRmNxL3is8c3EnfLIY2E5bKTvlR7zJv8xtpjAnziki/WHvNP79xUS5yMnCKiTClO\nEVGc+j59pREpThEtlXVOEVEul1udUpwiWiwjp4goTxZhRkSp0hCPiCKlOEVEeUwa4hFRpjTEI6JM\nKU4RUZoswoyIMtnZbC4iClVubUpximizTOsiojwGMq2LiCKVW5u6fxx5RExdcrWjaxxpoaS7Ja2T\ndNELfP18SWsk/UTSjZJe2S1milNEi2nElY5xY0gDwCLgeGABcKqkBTucdgcwaPu3gCXAJ7rlluIU\n0VaewDG+I4B1ttfb3gpcDZz4vLeyb7b99OjTW4E53YL2Vc/pd5ae00jcfdbV/2O48uzP1B4T4GNv\nPamRuHtc8Wwjcfeb3kzcP7v9j2uPec7Xbqo9JsDpPzi99pgPPLlol2N0FmFWbjrNkrRqzPPFtheP\nPp4NbBjztY3AeJ8WcQbwH93esK+KU0TUrPquBJttD+7q20k6DRgE3tLt3BSniBabwMhpPPcDc8c8\nnzP62vPfSzoW+GvgLba7fnZaek4RbVVfz2klMF/SPEnTgVOApWNPkHQY8CXgBNsPV0kvI6eI1qrn\n3jrbQ5LOApYDA8DltldLugRYZXsp8Elgb+BfJQHcZ/uE8eKmOEW0WU2bzdleBizb4bWLxzw+dqIx\nU5wi2iofqhkRxSp4m95KDXFJ50laLemnkr4uaUbTiUXEJKinId6IrsVJ0mzgQ3SWnh9Kp+F1StOJ\nRUTzNDJS6eiFqtO6acCekrYBewEPNJdSREwKM5FFmJOu68jJ9v3Ap4D7gAeBLbZv2PE8SWdKWiVp\n1eOPFvxfHBEACCNXO3qhyrTuADo38c0DXg7MHF2C/jy2F9setD24/4FZ2xnRF+xqRw9UqSLHAvfY\n3mR7G3AtcFSzaUXEpCi4OFXpOd0HHClpL+AZ4Bhg1fjfEhHFK7zn1LU42V4haQlwOzBEZ9OoxeN/\nV0T0g15diaui0tU62x8FPtpwLhExqXo3ZasiK8Qj2sqkOEVEocqd1aU4RbRZr9YwVZHiFNFmKU4R\nURwbhsud1zVSnDbecxAXnPbB2uPOOKaZWvrci+r/63Hhz/6w9pgAzx390kbiPvpdNRL3pSuGG4n7\n7UWX1h7zjJ81cz/79J/vWXtMPVfTXRgZOUVEkVKcIqI4BmrYQ7wpKU4RrWVwy3pOEdEHTPsa4hHR\nJ9JziogipThFRHly429ElMhAv2+ZEhFTVEZOEVGeFt6+EhF9wOCsc4qIImWFeEQUKT2niCiOnat1\nEVGojJwiojzGw83st1WHFKeItsqWKRFRrCwliIjSGHBGThFRHGezuYgoVMkNcbmBS4mSNgH/U+HU\nWcDm2hNoTj/l20+5Qn/lW0Kur7R90K4EkHQ9nf+WKjbbXrgr7zdRjRSnym8urbI92LMEJqif8u2n\nXKG/8u2nXPtZTR9+FRFRrxSniChSr4vT4h6//0T1U779lCv0V779lGvf6mnPKSJiZ3o9coqIeEE9\nK06SFkq6W9I6SRf1Ko9uJM2VdLOkNZJWSzqn1zlVIWlA0h2Srut1LuORtL+kJZLukrRW0pt6ndN4\nJJ03+u/gp5K+LmlGr3OaqnpSnCQNAIuA44EFwKmSFvQilwqGgAtsLwCOBP6i4FzHOgdY2+skKvgM\ncL3t1wK/TcE5S5oNfAgYtH0oMACc0tuspq5ejZyOANbZXm97K3A1cGKPchmX7Qdt3z76+Ak6vzyz\ne5vV+CTNAd4FXNbrXMYjaT/gzcCXAWxvtf14b7Pqahqwp6RpwF7AAz3OZ8rqVXGaDWwY83wjhf/C\nA0g6GDgMWNHbTLr6NHAhUO6NUx3zgE3AV0anoJdJmtnrpHbG9v3Ap4D7gAeBLbZv6G1WU1ca4hVJ\n2hv4BnCu7V/2Op+dkfRu4GHbt/U6lwqmAYcDX7B9GPAUUHL/8QA6I/x5wMuBmZJO621WU1evitP9\nwNwxz+eMvlYkSbvTKUxX2b621/l0cTRwgqR76UyX3y7pyt6mtFMbgY22t49El9ApVqU6FrjH9ibb\n24BrgaN6nNOU1avitBKYL2mepOl0mopLe5TLuCSJTk9kre1Le51PN7Y/YnuO7YPp/Fxvsl3kX3fb\nDwEbJL1m9KVjgDU9TKmb+4AjJe01+u/iGApu4Pe7nmyZYntI0lnAcjpXPC63vboXuVRwNPA+4L8k\n3Tn62l/ZXtbDnKaSs4GrRv9IrQdO73E+O2V7haQlwO10ruLeQVaLNyYrxCOiSGmIR0SRUpwiokgp\nThFRpBSniChSilNEFCnFKSKKlOIUEUVKcYqIIv0vHFqkfsEGWHsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}